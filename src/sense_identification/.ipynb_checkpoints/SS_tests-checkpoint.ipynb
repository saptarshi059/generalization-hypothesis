{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4fca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "logging.set_verbosity(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a2191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab['party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ca7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath('../../data/sense_data/sense_data-cuad.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2215d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d96e30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['Product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6cbc2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#With this code, we can verify whether each example has only 1 occurrence of the target word.\n",
    "#This helps to ensure that we don't have multiple instances of contextualization in the same sentence.\n",
    "for row in df.itertuples():\n",
    "    word = row.word.lower()\n",
    "    sentence = row.example\n",
    "    tokenized_idx = tokenizer(sentence)['input_ids']\n",
    "    if tokenized_idx.count(tokenizer.vocab[word]) != 1:\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424d4b1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Using pooler output\n",
    "\n",
    "cos = torch.nn.CosineSimilarity()\n",
    "sim_scores = defaultdict(list)\n",
    "\n",
    "for word in tqdm(df['word'].unique()):\n",
    "    word_indices = df[df['word'] == word].index\n",
    "    for comb in list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2)):\n",
    "        indexA = comb[0]\n",
    "        indexB = comb[1]\n",
    "\n",
    "        tokenized_inputA = tokenizer(df.iloc[indexA].example, return_tensors='pt') \n",
    "        pooler_outputA = model(**tokenized_inputA).pooler_output\n",
    "\n",
    "        tokenized_inputB = tokenizer(df.iloc[indexB].example, return_tensors='pt') \n",
    "        pooler_outputB = model(**tokenized_inputB).pooler_output\n",
    "\n",
    "        sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "            cos(pooler_outputA, pooler_outputB).item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d2d988",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81abe91ee94c475ebfb9e08bd8026074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11d57c3e27947a38b601d37970fd9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Product Improvements by the Company.\n",
      "For purposes of this Agreement, a Product Year shall be the twelve (12) month period following the commencement of the initial Term of this Agreement and each twelve (12) months thereafter.\n",
      "2 Product Improvements by the Distributor.\n",
      "6 Product Returns.\n",
      "If the Distributor shall fail to purchase the minimum number of units in any year, the Distributor's exclusive rights to sell and distribute the Product in the Market, may at Company's sole option, be reevaluated.\n",
      "In the event that Company is unable or unwilling to promptly perform any warranty work without reasonable cause and following full and fair opportunity to do so, or in the event of the necessity for emergency repairs of a defective Product for which there is no reasonable possibility of performance by Company, Distributor may perform such warranty work or hire a third party to perform such warranty work and the reasonable cost thereof shall be paid by Company.\n",
      "The Distributor agrees that the Company shall not be liable for its failure to perform due to any occurrence beyond the Company's reasonable control, including, but not limited to, acts of God, fires, Page -4- floods, wars, sabotage, accidents in shipping beyond the Company's control, labor strikes other than strikes against the Company itself, weather conditions or foreign or domestic government regulation or authority which directly affects Company's ability to deliver Product.\n",
      "Distributor must give Google a reasonable time to fix the problem and (if necessary) to supply Distributor with a corrected or replacement version of the Distribution Product or a way to work-around the problem that is not materially detrimental to Distributor, or to re-perform any relevant services.\n",
      "EULA means the end user license agreement applicable to a Product, which end user license agreement may be updated or modified by Google in its sole discretion from time to time.\n",
      "She was, in her own way, a product of the spirit of American self-construction.\n",
      "The “product,” as he calls it, was not as good as theater.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35042/2165007045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mentity_embeddingA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextualized_embeddingsA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_inputA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordA_vocab_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mentity_embeddingB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextualized_embeddingsB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_inputB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordB_vocab_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
      "\u001b[0;31mValueError\u001b[0m: None is not in list"
     ]
    }
   ],
   "source": [
    "#Using contextualized entity output (BERTs/RoBERTa)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "sim_scores = defaultdict(list)\n",
    "\n",
    "def find_vocab_idx(word, tokenization):\n",
    "    if 'roberta' in model_checkpoint:\n",
    "        if word in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word]\n",
    "        \n",
    "        if ('Ġ'+ word.lower()) in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab['Ġ'+ word.lower()] in tokenization['input_ids'].tolist()[0]:\n",
    "                word = 'Ġ' + word.lower()\n",
    "                return tokenizer.vocab[word]\n",
    "        \n",
    "        if ('Ġ'+ word) in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab['Ġ'+ word] in tokenization['input_ids'].tolist()[0]:\n",
    "                word = 'Ġ' + word\n",
    "                return tokenizer.vocab[word]\n",
    "    else:\n",
    "        if word in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word]\n",
    "        \n",
    "        if word.lower() in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word.lower()] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word.lower()]\n",
    "\n",
    "for word in tqdm(df['word'].unique()):\n",
    "    word_indices = df[df['word'] == word].index\n",
    "    for comb in tqdm(list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2))):\n",
    "        indexA = comb[0]\n",
    "        indexB = comb[1]        \n",
    "        \n",
    "        tokenized_inputA = tokenizer(df.iloc[indexA].example, return_tensors='pt') \n",
    "        contextualized_embeddingsA = model(**tokenized_inputA).last_hidden_state\n",
    "\n",
    "        tokenized_inputB = tokenizer(df.iloc[indexB].example, return_tensors='pt') \n",
    "        contextualized_embeddingsB = model(**tokenized_inputB).last_hidden_state\n",
    "\n",
    "        wordA_vocab_idx = find_vocab_idx(df.iloc[indexA].word, tokenized_inputA)\n",
    "        wordB_vocab_idx = find_vocab_idx(df.iloc[indexB].word, tokenized_inputB)\n",
    "        \n",
    "        entity_embeddingA = contextualized_embeddingsA[0][tokenized_inputA['input_ids'].tolist()[0].index(wordA_vocab_idx)]       \n",
    "        entity_embeddingB = contextualized_embeddingsB[0][tokenized_inputB['input_ids'].tolist()[0].index(wordB_vocab_idx)]\n",
    "        \n",
    "        sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "            cos(entity_embeddingA, entity_embeddingB).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94319e3f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Using contextualized entity ouptut (SenseBERT)\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append('/content/sense-bert')\n",
    "from sensebert import SenseBert\n",
    "\n",
    "with tf.Session() as session:\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    sim_scores = defaultdict(list)\n",
    "    model = SenseBert(\"sensebert-base-uncased\", session=session)  # or sensebert-large-uncased\n",
    "    tokenizer = model.tokenizer\n",
    "\n",
    "    def find_vocab_idx(word, input_ids):\n",
    "        if word in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word] in input_ids[0]:\n",
    "                return tokenizer.vocab[word]\n",
    "\n",
    "        if word.lower() in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word.lower()] in input_ids[0]:\n",
    "                return tokenizer.vocab[word.lower()]\n",
    "\n",
    "    for word in tqdm(df['word'].unique()):\n",
    "        word_indices = df[df['word'] == word].index\n",
    "        for comb in tqdm(list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2))):\n",
    "            indexA = comb[0]\n",
    "            indexB = comb[1]        \n",
    "\n",
    "            input_idsA, input_maskA = model.tokenize(df.iloc[indexA].example) \n",
    "            contextualized_embeddingsA, _, _ = model.run(input_idsA, input_maskA)\n",
    "\n",
    "            input_idsB, input_maskB = model.tokenize(df.iloc[indexB].example) \n",
    "            contextualized_embeddingsB, _, _ = model.run(input_idsB, input_maskB)\n",
    "\n",
    "            wordA_vocab_idx = find_vocab_idx(df.iloc[indexA].word, input_idsA)\n",
    "            wordB_vocab_idx = find_vocab_idx(df.iloc[indexB].word, input_idsB)\n",
    "\n",
    "            entity_embeddingA = contextualized_embeddingsA[0][input_idsA[0].index(wordA_vocab_idx)]       \n",
    "            entity_embeddingB = contextualized_embeddingsB[0][input_idsB[0].index(wordB_vocab_idx)]\n",
    "\n",
    "            sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "              cos(torch.FloatTensor(entity_embeddingA), torch.FloatTensor(entity_embeddingB)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for key, val in sim_scores.items():\n",
    "    print(key, np.round(torch.mean(torch.Tensor(val)).item(), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
