{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "logging.set_verbosity(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath('../../data/sense_data/sense_data-techqa.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6cbc2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#With this code, we can verify whether each example has only 1 occurrence of the target word.\n",
    "#This helps to ensure that we don't have multiple instances of contextualization in the same sentence.\n",
    "for row in df.itertuples():\n",
    "    word = row.word.lower()\n",
    "    sentence = row.example\n",
    "    tokenized_idx = tokenizer(sentence)['input_ids']\n",
    "    if tokenized_idx.count(tokenizer.vocab[word]) != 1:\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424d4b1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Using pooler output\n",
    "\n",
    "cos = torch.nn.CosineSimilarity()\n",
    "sim_scores = defaultdict(list)\n",
    "\n",
    "for word in tqdm(df['word'].unique()):\n",
    "    word_indices = df[df['word'] == word].index\n",
    "    for comb in list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2)):\n",
    "        indexA = comb[0]\n",
    "        indexB = comb[1]\n",
    "\n",
    "        tokenized_inputA = tokenizer(df.iloc[indexA].example, return_tensors='pt') \n",
    "        pooler_outputA = model(**tokenized_inputA).pooler_output\n",
    "\n",
    "        tokenized_inputB = tokenizer(df.iloc[indexB].example, return_tensors='pt') \n",
    "        pooler_outputB = model(**tokenized_inputB).pooler_output\n",
    "\n",
    "        sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "            cos(pooler_outputA, pooler_outputB).item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2d988",
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "#Using contextualized entity output\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "sim_scores = defaultdict(list)\n",
    "\n",
    "def find_vocab_idx(word, tokenization):\n",
    "    if 'roberta' in model_checkpoint:\n",
    "        if tokenizer.vocab[word] in tokenization['input_ids'].tolist()[0]:\n",
    "            return tokenizer.vocab[word]\n",
    "        elif tokenizer.vocab['Ġ'+ word.lower()] in tokenization['input_ids'].tolist()[0]:\n",
    "            word = 'Ġ' + word.lower()\n",
    "            return tokenizer.vocab[word]\n",
    "        else:\n",
    "            word = 'Ġ' + word\n",
    "            return tokenizer.vocab[word]\n",
    "    else:\n",
    "        try:            \n",
    "            if tokenizer.vocab[word] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word]   \n",
    "        except:\n",
    "            return tokenizer.vocab[word.lower()]\n",
    "\n",
    "for word in tqdm(df['word'].unique()):\n",
    "    word_indices = df[df['word'] == word].index\n",
    "    for comb in list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2)):\n",
    "        indexA = comb[0]\n",
    "        indexB = comb[1]        \n",
    "        \n",
    "        tokenized_inputA = tokenizer(df.iloc[indexA].example, return_tensors='pt') \n",
    "        contextualized_embeddingsA = model(**tokenized_inputA).last_hidden_state\n",
    "\n",
    "        tokenized_inputB = tokenizer(df.iloc[indexB].example, return_tensors='pt') \n",
    "        contextualized_embeddingsB = model(**tokenized_inputB).last_hidden_state\n",
    "\n",
    "        wordA_vocab_idx = find_vocab_idx(df.iloc[indexA].word, tokenized_inputA)\n",
    "        wordB_vocab_idx = find_vocab_idx(df.iloc[indexB].word, tokenized_inputB)\n",
    "        \n",
    "        entity_embeddingA = contextualized_embeddingsA[0][tokenized_inputA['input_ids'].tolist()[0].index(wordA_vocab_idx)]       \n",
    "        entity_embeddingB = contextualized_embeddingsB[0][tokenized_inputB['input_ids'].tolist()[0].index(wordB_vocab_idx)]\n",
    "        \n",
    "        sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "            cos(entity_embeddingA, entity_embeddingB).item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for key, val in sim_scores.items():\n",
    "    print(key, np.round(torch.mean(torch.Tensor(val)).item(), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
