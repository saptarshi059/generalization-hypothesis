{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4fca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "logging.set_verbosity(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a2191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74e4c932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6493"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ca7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath('../../data/sense_data/sense_data-cuad.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6cbc2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#With this code, we can verify whether each example has only 1 occurrence of the target word.\n",
    "#This helps to ensure that we don't have multiple instances of contextualization in the same sentence.\n",
    "for row in df.itertuples():\n",
    "    word = row.word.lower()\n",
    "    sentence = row.example\n",
    "    tokenized_idx = tokenizer(sentence)['input_ids']\n",
    "    if tokenized_idx.count(tokenizer.vocab[word]) != 1:\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424d4b1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Using pooler output\n",
    "\n",
    "cos = torch.nn.CosineSimilarity()\n",
    "sim_scores = defaultdict(list)\n",
    "\n",
    "for word in tqdm(df['word'].unique()):\n",
    "    word_indices = df[df['word'] == word].index\n",
    "    for comb in list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2)):\n",
    "        indexA = comb[0]\n",
    "        indexB = comb[1]\n",
    "\n",
    "        tokenized_inputA = tokenizer(df.iloc[indexA].example, return_tensors='pt') \n",
    "        pooler_outputA = model(**tokenized_inputA).pooler_output\n",
    "\n",
    "        tokenized_inputB = tokenizer(df.iloc[indexB].example, return_tensors='pt') \n",
    "        pooler_outputB = model(**tokenized_inputB).pooler_output\n",
    "\n",
    "        sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "            cos(pooler_outputA, pooler_outputB).item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8d2d988",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435bdd5b826640f080576235b50ece81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d47067094ac4d9e8dab09e18ad2e43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "party here\n",
      "party here\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11275/2962055898.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mwordB_vocab_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_vocab_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_inputB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mentity_embeddingA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextualized_embeddingsA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_inputA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordA_vocab_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mentity_embeddingB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextualized_embeddingsB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_inputB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordB_vocab_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None is not in list"
     ]
    }
   ],
   "source": [
    "#Using contextualized entity output\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "sim_scores = defaultdict(list)\n",
    "\n",
    "def find_vocab_idx(word, tokenization):\n",
    "    if 'roberta' in model_checkpoint:\n",
    "        if word in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word]\n",
    "            elif ('Ġ'+ word.lower()) in tokenizer.vocab.keys():\n",
    "                if tokenizer.vocab['Ġ'+ word.lower()] in tokenization['input_ids'].tolist()[0]:\n",
    "                    word = 'Ġ' + word.lower()\n",
    "                    return tokenizer.vocab[word]\n",
    "            else:\n",
    "                word = 'Ġ' + word\n",
    "                return tokenizer.vocab[word]\n",
    "    else:\n",
    "        if word in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word]\n",
    "        else:\n",
    "            return tokenizer.vocab[word.lower()]\n",
    "\n",
    "for word in tqdm(df['word'].unique()):\n",
    "    word_indices = df[df['word'] == word].index\n",
    "    for comb in tqdm(list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2))):\n",
    "        indexA = comb[0]\n",
    "        indexB = comb[1]        \n",
    "        \n",
    "        tokenized_inputA = tokenizer(df.iloc[indexA].example, return_tensors='pt') \n",
    "        contextualized_embeddingsA = model(**tokenized_inputA).last_hidden_state\n",
    "\n",
    "        tokenized_inputB = tokenizer(df.iloc[indexB].example, return_tensors='pt') \n",
    "        contextualized_embeddingsB = model(**tokenized_inputB).last_hidden_state\n",
    "\n",
    "        wordA_vocab_idx = find_vocab_idx(df.iloc[indexA].word, tokenized_inputA)\n",
    "        wordB_vocab_idx = find_vocab_idx(df.iloc[indexB].word, tokenized_inputB)\n",
    "        \n",
    "        entity_embeddingA = contextualized_embeddingsA[0][tokenized_inputA['input_ids'].tolist()[0].index(wordA_vocab_idx)]       \n",
    "        entity_embeddingB = contextualized_embeddingsB[0][tokenized_inputB['input_ids'].tolist()[0].index(wordB_vocab_idx)]\n",
    "        \n",
    "        sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "            cos(entity_embeddingA, entity_embeddingB).item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for key, val in sim_scores.items():\n",
    "    print(key, np.round(torch.mean(torch.Tensor(val)).item(), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
