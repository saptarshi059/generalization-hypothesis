{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "logging.set_verbosity(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab['party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath('../../data/sense_data/sense_data-cuad.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab['Product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6cbc2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#With this code, we can verify whether each example has only 1 occurrence of the target word.\n",
    "#This helps to ensure that we don't have multiple instances of contextualization in the same sentence.\n",
    "for row in df.itertuples():\n",
    "    word = row.word.lower()\n",
    "    sentence = row.example\n",
    "    tokenized_idx = tokenizer(sentence)['input_ids']\n",
    "    if tokenized_idx.count(tokenizer.vocab[word]) != 1:\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424d4b1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Using pooler output\n",
    "\n",
    "cos = torch.nn.CosineSimilarity()\n",
    "sim_scores = defaultdict(list)\n",
    "\n",
    "for word in tqdm(df['word'].unique()):\n",
    "    word_indices = df[df['word'] == word].index\n",
    "    for comb in list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2)):\n",
    "        indexA = comb[0]\n",
    "        indexB = comb[1]\n",
    "\n",
    "        tokenized_inputA = tokenizer(df.iloc[indexA].example, return_tensors='pt') \n",
    "        pooler_outputA = model(**tokenized_inputA).pooler_output\n",
    "\n",
    "        tokenized_inputB = tokenizer(df.iloc[indexB].example, return_tensors='pt') \n",
    "        pooler_outputB = model(**tokenized_inputB).pooler_output\n",
    "\n",
    "        sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "            cos(pooler_outputA, pooler_outputB).item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2d988",
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "#Using contextualized entity output (BERTs/RoBERTa)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "sim_scores = defaultdict(list)\n",
    "\n",
    "def find_vocab_idx(word, tokenization):\n",
    "    if 'roberta' in model_checkpoint:\n",
    "        if word in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word]\n",
    "        \n",
    "        if ('Ġ'+ word.lower()) in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab['Ġ'+ word.lower()] in tokenization['input_ids'].tolist()[0]:\n",
    "                word = 'Ġ' + word.lower()\n",
    "                return tokenizer.vocab[word]\n",
    "        \n",
    "        if ('Ġ'+ word) in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab['Ġ'+ word] in tokenization['input_ids'].tolist()[0]:\n",
    "                word = 'Ġ' + word\n",
    "                return tokenizer.vocab[word]\n",
    "    else:\n",
    "        if word in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word]\n",
    "        \n",
    "        if word.lower() in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word.lower()] in tokenization['input_ids'].tolist()[0]:\n",
    "                return tokenizer.vocab[word.lower()]\n",
    "\n",
    "for word in tqdm(df['word'].unique()):\n",
    "    word_indices = df[df['word'] == word].index\n",
    "    for comb in tqdm(list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2))):\n",
    "        indexA = comb[0]\n",
    "        indexB = comb[1]        \n",
    "        \n",
    "        tokenized_inputA = tokenizer(df.iloc[indexA].example, return_tensors='pt') \n",
    "        contextualized_embeddingsA = model(**tokenized_inputA).last_hidden_state\n",
    "\n",
    "        tokenized_inputB = tokenizer(df.iloc[indexB].example, return_tensors='pt') \n",
    "        contextualized_embeddingsB = model(**tokenized_inputB).last_hidden_state\n",
    "\n",
    "        wordA_vocab_idx = find_vocab_idx(df.iloc[indexA].word, tokenized_inputA)\n",
    "        wordB_vocab_idx = find_vocab_idx(df.iloc[indexB].word, tokenized_inputB)\n",
    "        \n",
    "        entity_embeddingA = contextualized_embeddingsA[0][tokenized_inputA['input_ids'].tolist()[0].index(wordA_vocab_idx)]       \n",
    "        entity_embeddingB = contextualized_embeddingsB[0][tokenized_inputB['input_ids'].tolist()[0].index(wordB_vocab_idx)]\n",
    "        \n",
    "        sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "            cos(entity_embeddingA, entity_embeddingB).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036a32f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Using contextualized entity ouptut (SenseBERT)\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append('/content/sense-bert')\n",
    "from sensebert import SenseBert\n",
    "\n",
    "with tf.Session() as session:\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    sim_scores = defaultdict(list)\n",
    "    model = SenseBert(\"sensebert-base-uncased\", session=session)  # or sensebert-large-uncased\n",
    "    tokenizer = model.tokenizer\n",
    "\n",
    "    def find_vocab_idx(word, input_ids):\n",
    "        if word in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word] in input_ids[0]:\n",
    "                return tokenizer.vocab[word]\n",
    "\n",
    "        if word.lower() in tokenizer.vocab.keys():\n",
    "            if tokenizer.vocab[word.lower()] in input_ids[0]:\n",
    "                return tokenizer.vocab[word.lower()]\n",
    "\n",
    "    for word in tqdm(df['word'].unique()):\n",
    "        word_indices = df[df['word'] == word].index\n",
    "        for comb in tqdm(list(combinations(list(range(word_indices[0], word_indices[-1]+1)), 2))):\n",
    "            indexA = comb[0]\n",
    "            indexB = comb[1]        \n",
    "\n",
    "            input_idsA, input_maskA = model.tokenize(df.iloc[indexA].example) \n",
    "            contextualized_embeddingsA, _, _ = model.run(input_idsA, input_maskA)\n",
    "\n",
    "            input_idsB, input_maskB = model.tokenize(df.iloc[indexB].example) \n",
    "            contextualized_embeddingsB, _, _ = model.run(input_idsB, input_maskB)\n",
    "\n",
    "            wordA_vocab_idx = find_vocab_idx(df.iloc[indexA].word, input_idsA)\n",
    "            wordB_vocab_idx = find_vocab_idx(df.iloc[indexB].word, input_idsB)\n",
    "\n",
    "            entity_embeddingA = contextualized_embeddingsA[0][input_idsA[0].index(wordA_vocab_idx)]       \n",
    "            entity_embeddingB = contextualized_embeddingsB[0][input_idsB[0].index(wordB_vocab_idx)]\n",
    "\n",
    "            sim_scores[(word, df.iloc[indexA].sense_def, df.iloc[indexB].sense_def)].append(\\\n",
    "              cos(torch.FloatTensor(entity_embeddingA), torch.FloatTensor(entity_embeddingB)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for key, val in sim_scores.items():\n",
    "    print(key, np.round(torch.mean(torch.Tensor(val)).item(), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
