{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, DatasetDict, load_from_disk, concatenate_datasets\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "with open(os.path.abspath('../data/TechQA/training_and_dev/training_Q_A.json')) as file:\n",
    "    train_json = json.load(file)\n",
    "\n",
    "with open(os.path.abspath('../data/TechQA/training_and_dev/dev_Q_A.json')) as file:\n",
    "    validation_json = json.load(file)\n",
    "    \n",
    "technotes = pd.read_json(\"../data/TechQA/training_and_dev/training_dev_technotes.json\", orient='index')\n",
    "\n",
    "print('All files loaded...')\n",
    "\n",
    "#Collecting all documents for a given record\n",
    "def add_context(json_file):\n",
    "    for row in tqdm(json_file):\n",
    "        if row['ANSWERABLE'] == 'Y':\n",
    "            row['context'] = technotes[technotes['id'] == row['DOCUMENT']].text[0]\n",
    "        else:\n",
    "            #Combine all documents to create a single document for a given unanswerable records.\n",
    "            text = ''\n",
    "            for doc_id in row['DOC_IDS']:\n",
    "                text += technotes[technotes['id'] == doc_id].text[0]\n",
    "            row['context'] = text\n",
    "\n",
    "add_context(train_json)\n",
    "add_context(validation_json)\n",
    "\n",
    "with open('training_Q_A_context.json', 'w') as fout:\n",
    "    json.dump(train_json , fout)\n",
    "\n",
    "with open('dev_Q_A_context.json', 'w') as f:\n",
    "    json.dump(validation_json, f)\n",
    "\n",
    "print('Files saved with context...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ca74ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QUESTION_ID': 'TRAIN_Q002',\n",
       " 'QUESTION_TITLE': 'Why are replies going to the DLQ with reason 2189 MQRC_CLUSTER_RESOLUTION_ERROR',\n",
       " 'QUESTION_TEXT': 'I have changed my cluster receiver to match the IP address used and now I am seeing replies in my DLQ with 2189 MQRC_CLUSTER_RESOLUTION_ERROR. The reply queue is on a different cluster which this qmgr is a member of. The reply message uses replytoqmgr and queue name. The qmgr should see the replytoqmgr in the cluster and get the message over to the queue, but instead goes to dead letter queue. This queue manager is a repository to one cluster and a member of another. ',\n",
       " 'DOCUMENT': '-',\n",
       " 'ANSWER': '-',\n",
       " 'START_OFFSET': '-',\n",
       " 'END_OFFSET': '-',\n",
       " 'ANSWERABLE': 'N',\n",
       " 'DOC_IDS': ['swg1PK33560',\n",
       "  'swg1IT24269',\n",
       "  'swg1PK98245',\n",
       "  'swg21612063',\n",
       "  'swg1PI56444',\n",
       "  'swg21638919',\n",
       "  'swg1PK40370',\n",
       "  'swg21650290',\n",
       "  'swg1PM18426',\n",
       "  'swg1PI98289',\n",
       "  'swg1PM49925',\n",
       "  'swg1PM22372',\n",
       "  'swg1PK26947',\n",
       "  'swg27039756',\n",
       "  'swg21622347',\n",
       "  'swg21571049',\n",
       "  'swg1IZ70611',\n",
       "  'swg1IT24999',\n",
       "  'swg21166048',\n",
       "  'swg1PI42870',\n",
       "  'swg1SA96306',\n",
       "  'swg27020934',\n",
       "  'swg21576423',\n",
       "  'swg21229905',\n",
       "  'swg1PK66962',\n",
       "  'swg1PM27593',\n",
       "  'swg1PI76942',\n",
       "  'swg1IY66462',\n",
       "  'swg1SA89637',\n",
       "  'swg1IT19598',\n",
       "  'swg1PK17023',\n",
       "  'swg21660802',\n",
       "  'swg1IC68561',\n",
       "  'swg1PM22462',\n",
       "  'swg1PM44820',\n",
       "  'swg1PM52988',\n",
       "  'swg1PK36884',\n",
       "  'swg1PI96848',\n",
       "  'swg1PI30429',\n",
       "  'swg1PK86875',\n",
       "  'swg21620882',\n",
       "  'swg1PK32886',\n",
       "  'swg1PK76183',\n",
       "  'swg1PI48932',\n",
       "  'swg1PI24496',\n",
       "  'swg1IT20323',\n",
       "  'swg1PI79259',\n",
       "  'swg1PI31166',\n",
       "  'swg1PM11753',\n",
       "  'swg22011290'],\n",
       " 'context': 'z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  If there are multiple reply messages from a request message, the\\n   first reply is sent to the queue specified in the MQMD.ReplyToQ,\\n   but subsequent replies are sent to the bridge request queue.\\n   This problem occurs at WebSphere MQ for z/OS V6.0 but not at MQ\\n   V5.3.1.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of Websphere MQ for z/OS Version 6 *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: A CICS transaction run through the      *\\n   *                      bridge issues EXEC CICS SYNCPOINT,      *\\n   *                      causing a reply message to be built and *\\n   *                      put to the reply queue. After this any  *\\n   *                      subsequent reply messages, including    *\\n   *                      the end of task message, are sent to    *\\n   *                      the request queue instead of the reply  *\\n   *                      queue.                                  *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   When building the first reply message the ReplyToQ and\\n   ReplyToQmgr in the message header are changed to the request\\n   queue. This value is then incorrectly used when opening the\\n   reply to queue for subsequent reply messages.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  cbpSendReplyMessage was changed to save the values of ReplyToQ\\n   and ReplyToQmgr from the message header prior to building the\\n   reply message. After the reply message has been sent, the saved\\n   values are restored for use in subsequent messages.\\n   000Y\\n   CSQCBLK0\\n   CSQCBLK1\\n   CSQCBLK2\\n   CSQCBP00\\n   CSQCBP10\\n   CSQCBP20\\n   CSQCBR00\\n   CSQCBR20\\n   CSQCBR30\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK33560\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2006-10-25\\n   \\n   \\n * CLOSED DATE\\n   2006-11-16\\n   \\n   \\n * LAST MODIFIED DATE\\n   2006-12-01\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK19745\\n   \\n   \\n\\nMODULES/MACROS\\n *     CSQCBLK0 CSQCBLK1 CSQCBLK2 CSQCBP00 CSQCBP10\\n   CSQCBP20 CSQCBR00 CSQCBR20 CSQCBR30\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655L8200\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UK19745 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK19745]\\n   UP06/12/01 P F611\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES. SUBSCRIBE TO THIS APAR\\nBy subscribing, you receive periodic emails alerting you to the status of the APAR, along with a link to the fix after it becomes available. You can track this item individually or track all items by product.\\n\\nNotify me when this APAR changes.\\n\\nNotify me when an APAR for this component changes.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  In this scenario there were local logger filesystem availability\\n   problems.  Because of this, there were 1000s of application\\n   messages on the SYSTEM.CLUSTER.TRANSMIT.QUEUE, and one or more\\n   channels held indoubt batches of messages.  At this time the\\n   administrator ran REFRESH CLUSTER on the local queue manager.\\n   The indoubt batches of messages became rolled back (probably\\n   automatically due to log space shortage, though the same effect\\n   could be seen if using RESOLVE CHANNEL to roll back the batch),\\n   so the messages appeared on the transmission queue again.\\n   \\n   After this the local queue manager\\'s repository manager program\\n   tried to reallocate messages from the rolled-back indoubt\\n   batches.  The reallocation routine found that the cluster cache\\n   no longer knew anything about the queues for which they were\\n   destined.  The repository manager did not break from its\\n   reallocation routine to ask the full repositories for details of\\n   the queues, so it suffered repeated\\n   MQRC_CLUSTER_RESOLUTION_ERROR errors over a period of many\\n   minutes, which were visible in the MQ trace file for the\\n   amqrrmfa process.\\n   \\n   Application calls to MQOPEN for queues not known locally will\\n   fail with 2189 MQRC_CLUSTER_RESOLUTION_ERROR.\\n   Other symptoms include:\\n   -- a failure to recognize other queue managers in the\\n      cluster including the QMGR hosting the cluster Q.\\n   -- message buildup on the SCCQ and the SCTQ.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   USERS AFFECTED:\\n   A system that suffers multiple problems including log space\\n   shortage, at a time when there are inflight batches and many\\n   other application messages sitting on the\\n   SYSTEM.CLUSTER.TRANSMIT.QUEUE.  This problem only occurs if the\\n   REFRESH CLUSTER command is issued while the queue manager is in\\n   this situation.\\n   \\n   \\n   Platforms affected:\\n   MultiPlatform\\n   \\n   ****************************************************************\\n   PROBLEM DESCRIPTION:\\n   The reallocation routine within the repository manager was\\n   suffering a MQRC_CLUSTER_RESOLUTION_ERROR condition repeatedly,\\n   for the same message each time.  There were 1000s of messages on\\n   the cluster transmission queue, and because of a flaw in the\\n   logic flow it would re-read the same message 1000s of times,\\n   with a 1 second sleep between each time.  It did not break from\\n   this loop to send the query to the full repositories that was\\n   necessary to relieve this situation.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  The correct behaviour for the reallocation routine is to\\n   schedule a re-run of itself in 60 seconds time, and break from\\n   its work to allow the repository manager to request information\\n   from the full repositories.  This is the behaviour that has now\\n   been coded in the MQ queue manager.\\n   \\n   ---------------------------------------------------------------\\n   The fix is targeted for delivery in the following PTFs:\\n   \\n   Version    Maintenance Level\\n   v8.0       8.0.0.10\\n   v9.0 LTS   9.0.0.4\\n   \\n   The latest available maintenance can be obtained from\\n   \\'WebSphere MQ Recommended Fixes\\'\\n   http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006037 [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006037]\\n   \\n   If the maintenance level is not yet available information on\\n   its planned availability can be found in \\'WebSphere MQ\\n   Planned Maintenance Release Dates\\'\\n   http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006309 [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006309]\\n   ---------------------------------------------------------------\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   IT24269\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   IBM MQ BASE MP\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5724H7251\\n   \\n   \\n * REPORTED RELEASE\\n   800\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   YesHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2018-03-02\\n   \\n   \\n * CLOSED DATE\\n   2018-03-19\\n   \\n   \\n * LAST MODIFIED DATE\\n   2018-03-19\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PI95380\\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   IBM MQ BASE MP\\n   \\n   \\n * FIXED COMPONENT ID\\n   5724H7251\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELSz/os SUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS FIXED IF NEXT.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  COD (Confirm On Delivery) messages appear on the DLQ (Dead\\n   Letter Queue) with MQ reason code 2049 (x\\'00000801\\')\\n   MQRC_PRIORITY_EXCEEDS_MAXIMUM.  Yet the application receives the\\n   COD message as well. The report message should not go both\\n   places.\\n   .\\n   This problem happens with WebSphere MQ V7 but not V6.  V7 was\\n   changed to match this statement in the Application Programming\\n   Reference\\n   \"If a message is put with a priority greater than the maximum\\n    supported by the local queue manager (this maximum is given by\\n    the MaxPriority queue-manager attribute), the message is\\n    accepted by the queue manager, but placed on the queue at the\\n    queue manager\\'s maximum priority; the MQPUT or MQPUT1 call\\n    completes with MQCC_WARNING and reason code\\n    MQRC_PRIORITY_EXCEEDS_MAXIMUM. However, the Priority field\\n    retains the value specified by the application that put the\\n    message.\"\\n   At V6, the priority in the MQMD was being changed.\\n   .\\n   A trace shows that MQMD.Priority=x\\'00FFFFFF\\' was bad on the\\n   request message.  The RC2049 occurred for the request message,\\n   but it was processed without going to the DLQ. Then, the report\\n   message got the RC2049 because the Priority from the original\\n   message was copied to it.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  Correct the application so that it sets MQMD.Priority within\\n   the valid range of 0 to 9.  The field is 4 bytes.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 0 Modification 0 and Modification 1. *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Report messages are unexpectedly put to *\\n   *                      the dead-letter queue with MQDLH.Reason *\\n   *                      set to MQRC_PRIORITY_EXCEEDS_MAXIMUM.   *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   An MQPUT is issued with MQMD.Priority specifying a priority\\n   greater than the maximum allowed by the queue-manager, and\\n   MQMD.Report specifying MQRO_COD. The message is put to the\\n   queue, and the MQPUT returns MQCC_WARNING,\\n   MQRC_PRIORITY_EXCEEDS_MAXIMUM.\\n   \\n   When an MQGET retrieves the above message, a COD report message\\n   is put to the reply queue as expected. However, a copy of the\\n   COD report message is also put to the dead-letter queue with\\n   a reason code of 2049, MQRC_PRIORITY_EXCEEDS_MAXIMUM.\\n   \\n   The same problem may also be seen when using other report\\n   options, for example MQRO_COA or MQRO_EXPIRTATION.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n *  This APAR is being closed FIN with agreement from the submitting\\n   customer.\\n   \\n   \\n    \\n   \\n   \\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK98245\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED FIN\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2009-10-08\\n   \\n   \\n * CLOSED DATE\\n   2009-10-19\\n   \\n   \\n * LAST MODIFIED DATE\\n   2009-10-19\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n\\nFIX INFORMATION\\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSN\\n   UPMQRC reason code 2035 MQFB feedback 298 replyto replytoq TECHNOTE (TROUBLESHOOTING)\\n\\nPROBLEM(ABSTRACT)\\n An application puts a message to an IMS Bridge queue with an MQMD.UserIdentifier or MQIIH.Authenticator that is invalid, or does not exist. \\n\\nThat message goes to the Dead Letter Queue ( DLQ ) as expected, but the exception report also goes to the DLQ rather than to the application. You would like for the application to receive the reply with the exception report \\n\\nCAUSE\\n298 = MQFB_NOT_AUTHORIZED_FOR_IMS means:\\nThe user ID contained in the message descriptor MQMD, or the password contained in the Authenticator field in the MQIIH structure, failed the validation performed by the IMS bridge. As a result the message was not passed to IMS.\\n\\nENVIRONMENT\\nIn the reported case, the application flow is: a message starts from a UNIX server. It goes to the Mainframe queue manager and into IMS via OTMA. The response goes back to mainframe queue manager and then to the UNIX server.\\n\\nIn this error scenario, the userid passed in the message header is INVALID, i.e. not defined to the security product (ACF2 in this case).\\n\\nAs expected, this results in the following:\\nACF01004 LOGONID ABC123 NOT FOUND\\nCSQ2005I CSQ1 CSQ2QCP0 ERROR PROCESSING MESSAGE, FEEDBACK=298, XCFGNAME=MQGRP XCFMNAME=IMN1OTMA TPIPE=QCT801\\n\\nThe message is put to the DLQ.\\n\\nFollowing that error, the bridge then attempts to put a response (indicating a security violation) on the the REPLY-TO queue, which results in the following:\\nACF01004 LOGONID ABC123 NOT FOUND\\nCSQ2004E CSQ1 CSQ2QCP0 ERROR USING QUEUE <queue> MQRC=2035\\n\\nThis REPLY message is then also put to the DLQ.\\n\\nFinal Result:\\n1) two messages are on the DLQ\\n2) the application on the UNIX never receives any response message and times out. ( 2033 MQRC_NO_MSG_AVAILABLE).\\n\\nOn another queue manager, with QUEUE security disabled (the NO.QUEUE.CHECKS profile switch defined), the first error still occurs as expected, but the second error (the 2035 on the REPLY-TO queue) does NOT happen and the UNIX application receives a response indicating the OTMA transaction failed due to a security violation.\\n\\nRESOLVING THE PROBLEM\\nIBM MQ is working as designed regarding the reply as indicated in the topics\\nIf the message cannot be put to the IMS queue [https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q023220_.html] \\nand \\nSecurity checking done by the IMS bridge [https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.0.0/com.ibm.mq.sec.doc/q012260_.html]. \\n\\nDefine the userid to the external security manager to allow both the request and report message to be delivered.\\n\\n\\n\\nPRODUCT ALIAS/SYNONYM\\n IBM MQ WebSphere MQ WMQz/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  Cics acts as webservice provider over MQ transport.\\n   A client put a message on the receive queue, that is triggered.\\n   Trigger monitor, then, writes on the INITQ and CPIL is attached\\n   because it is a SOAP message.\\n   CPIL  performs a MQGET on the receive queue. In the MQ message\\n   there is a replyDestination.\\n   CPIL then, starts CPIQ and it performs a MQPUT1 on the\\n   replyToQueue.\\n   But this queue is put-inhibited that cause to be returned:\\n   DFHPI0114 CPIQ The pipeline MQ transport mechanism failed\\n   because a call to WebSphere MQ function MQPUT1 returned with\\n   reason code 2051 MQRC_PUT_INHIBITED) x\\'803\\'\\n   .\\n   At this point CICS should write the message on Dead Letter\\n   Queue as described in the CICS manuals.\\n   .\\n   CPIQ issues a MQPUT1 on DLQ with PASS_ALL_CONTEXT but it fails\\n   with MQRC_CONTEXT_HANDLE_ERROR.\\n   No errors messages are sent for this error.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  n/a\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED:                                              *\\n   * All CICS Users.                                              *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION:                                         *\\n   * CICS webservice request with MQ transport fails with 2097 on *\\n   * DLQ when attempting to write to the DLQ.                     *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   * .                                                            *\\n   ****************************************************************\\n   A CICS webservice request with MQ transport fails because the\\n   specified reply-to queue is put disabled, (MQRC=2051) meaning\\n   the reply message cannot be written. The reply message should\\n   then be placed on the dead letter queue, but this also fails\\n   with a context handle error (MQRC=2097).\\n   \\n   Keyword(s): MQRC_CONTEXT_HANDLE_ERROR MQRC_PUT_INHIBITED\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  DFHPITQ1 has been altered so that the MQPUT1 which writes to the\\n   dead letter queue uses the mq_pmo.Option of\\n   MQPMO_SET_ALL_CONTEXT instead of MQPMO_PASS_ALL_CONTEXT.\\n   Additionally, the module is also changed so that whenever an\\n   MQPUT1 fails the failing message is written to the dead letter\\n   queue immediately.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI56444\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   CICS TS Z/OS V5\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655Y0400\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2016-02-02\\n   \\n   \\n * CLOSED DATE\\n   2016-04-05\\n   \\n   \\n * LAST MODIFIED DATE\\n   2016-05-04\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PI42870 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI42870]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UI36772\\n   \\n   \\n\\nMODULES/MACROS\\n *  DFHPITQ1\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   CICS TS Z/OS V5\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655Y0400\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UI36772 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI36772]\\n   UP16/04/22 P F604\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.DLQ dead letter queue FTE WMQFTE file transfer 2085 MQRC_UNKNOWN_OBJECT_NAME fteCreateTransfer -w TECHNOTE (TROUBLESHOOTING)\\n\\nPROBLEM(ABSTRACT)\\n You are using WebSphere MQ File Transfer Edition (WMQFTE/WMQ FTE) and notice a large buildup of MQDEAD messages on the dead-letter queue (DLQ) with reason code 2085: MQRC_UNKNOWN_OBJECT_NAME. \\n\\nCAUSE\\nWhen a blocking synchronous file transfer request is issued (-w parameter), the fteCreateTransfer command waits for the transfer to complete so that it can receive a reply message with the request outcome. Such a request can be issued using the fteCreateTransfer command with the -w parameter specified.\\n\\nWhen the -w parameter is specified with a wait period, a <reply> element of the file transfer request is created and populated with the name of the command queue manager where a temporary dynamic queue has been created to receive reply about the successful (or otherwise) completion of the transfer.\\n\\nThe temporary dynamic queue created by the fteCreateTransfer command is deleted when this command returns. As such, in many cases the command returns because the wait period has expired, causing the temporary queue to be deleted (as per normal processing). Therefore, when the transfer has completed, the WebSphere MQ FTE source agent attempts to put a message to this temporary queue, which now no longer exists. This causes it to be routed to the dead-letter queue (DLQ).\\n\\nRESOLVING THE PROBLEM\\nThere are three solutions to consider: \\n\\n 1. Specify a large enough wait period on the fteCreateTransfer command that ensures there is enough time for file transfer to complete and send the reply message before the fteCreateTransfer command returns. \\n 2. Do not specify the -w parameter on the fteCreateTransfer command so the request is asynchronous and non-blocking. \\n 3. Define a DLQ handler to delete messages that had been destined for the temporary dynamic queues if they are not needed or forward them to another queue for manual intervention should you wish to use these messages to investigate further. \\n 4. Do not specify a dead-letter queue for the WebSphere MQ FTE command queue manager.\\n\\n \\n\\nHISTORICAL NUMBER\\n 12528 082 000 \\n\\nPRODUCT ALIAS/SYNONYM\\n WebSphere MQ File Transfer Edition WMQFTE FTEz/os SUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS DUPLICATE OF ANOTHER APAR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  When MQQueueAgentThread puts a message to the Dead Letter Queue\\n   (DLQ), the information in the Dead Letter Header (DLH) is not\\n   always enough to easily determine the cause of the message going\\n   to the DLQ.  Such messages have\\n     PutApplName = \\'MQ JMS ConnectionConsumer   \\'.\\n   MQQueueAgentThread monitors a queue for an MDB listener.\\n   .\\n   For example:\\n   Intermittently, a message put by a JMS program goes to the DLQ\\n   with Reason Code 2030 MQRC_MSG_TOO_BIG_FOR_Q.  The MAXMSGL for\\n   the queue shown in the message is large enough for the message.\\n   The Dead Letter Handler (CSQUDLQH) is able to put the message to\\n   that queue.\\n   .\\n   RC2030 is related to the putting of a message.  The\\n   circumstances where MQQueueAgentThread would put a message to\\n   the dead letter queue following a failed put is as follows:\\n   1) The message is successfully PUT to a queue by an application.\\n   2) The message is retrieved by an MDB listener, but fails to be\\n   processed and is backed out 1 or more times.\\n   3) When the backout count reaches the backout threshold\\n   (BOTHRESH) for the queue, the MQQueueAgentThread attempts to\\n   requeue the message to the queue identified in the backout\\n   requeue name (BOQNAME) of the original queue.\\n   4) If the backout queue has a max message length smaller than\\n   the size of the message, this PUT will fail with a 2030 reason\\n   code.\\n   5) Dead letter processing is driven, and the message is put to\\n   the dead letter queue with a reason of 2030.\\n   .\\n   This situation is not obvious because the dead letter processing\\n   in MQQueueAgentThread always sets the destination queue in the\\n   DLH to the queue that the MQQueueAgent is monitoring, even if\\n   the error occurred on another queue.\\n   .\\n   Additional keywords:\\n   WebSphere Application Server WAS X\\'07EE\\' 07EE 000007EE\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  For the RC2030 scenario described above, set the MAXMSGL of the\\n   backout queue to be at least as large as that of the original\\n   queue.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n\\nPROBLEM CONCLUSION\\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n *  Problem summary:\\n   When a candidate message for an MDB has exceeded the backout\\n   threshold for the queue, the MDB listener will attempt to move\\n   it to the backout queue.\\n   If the put to the backout queue fails, then the message is put\\n   on the dead letter queue. The reason code given in the dead\\n   letter header is the reason for the failure of the put to the\\n   backout queue, but the queue name specified is that of the\\n   queue which the message originally came from.\\n   Problem resolution:\\n   The dead letter processing in MQQueueAgentThread has been\\n   changed to complete the dead letter header using the queue name\\n   corresponding to the failure reported by the reason code.\\n   This fix changes the Java component of MQ, and will be\\n   included in FixPack 6.0.2.3 (PK47040 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK47040]).\\n   \\n   \\n    \\n   \\n   \\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK40370\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   008\\n   \\n   \\n * STATUS\\n   CLOSED DUB\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2007-03-02\\n   \\n   \\n * CLOSED DATE\\n   2007-10-12\\n   \\n   \\n * LAST MODIFIED DATE\\n   2008-02-20\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PK32885 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK32885]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n\\nFIX INFORMATION\\n\\nAPPLICABLE COMPONENT LEVELS TECHNOTE (FAQ)\\n\\nQUESTION\\n What are the changes to clustering introduced in WebSphere MQ 7.1, 7.5, 8.0 and 9.0? \\n\\nANSWER\\nSo far, no attributes or object changes related to clusters were done in MQ 9.0.\\nIn WebSphere MQ 7.1, MQ 7.5, and MQ 8.0 the changes related to clustering are:\\n\\n++ Section 1: New attributes for clustering objects \\n\\nFor the whole list of new attributes, see corresponding technote:\\nhttp://www.ibm.com/support/docview.wss?uid=swg21620936 [http://www.ibm.com/support/docview.wss?uid=swg21620936]\\nNew objects and new attributes for objects in WebSphere MQ 7.1, 7.5, 8.0 and 9.0\\n\\nObject: Transmission Queues:\\nNew attribute: CLCHNAME => New in 7.5\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.adm.doc/q085330_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.adm.doc/q085330_.htm]\\nWebSphere MQ > Reference > Administration reference > MQSC reference > The MQSC commands\\nALTER queues\\nCLCHNAME(channel name)\\nCLCHNAME is the generic name of the cluster-sender channels that use this queue as a transmission queue. The attribute specifies which cluster-sender channels send messages to a cluster-receiver channel from this cluster transmission queue.\\n\\nObject: Cluster Receiver Channel (CLUSRCVR)\\nObject: Cluster Sender Channel (CLUSSDR) and Cluster Sender Channel (CLUSSDR)\\nNew attribute: BATCHLIM => New in 7.1\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.con.doc/q114230_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.con.doc/q114230_.htm]\\nWebSphere MQ > WebSphere MQ 8.0.0 > IBM MQ > Reference > Configuration reference > Channel attributes > Channel attributes in alphabetical order > \\nBatch limit (BATCHLIM)\\nThe limit, in kilobytes, of the amount of data that can be sent through a channel before taking a sync point. \\n\\nObject: Cluster Sender Channel (CLUSSDR) and Cluster Sender Channel (CLUSSDR)\\nNew attribute: RESETSEQ => New in 7.1\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.adm.doc/q086040_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.adm.doc/q086040_.htm]\\nWebSphere MQ > WebSphere MQ 8.0.0 > IBM MQ > Reference > Administration reference > MQSC reference > The MQSC commands > \\nDISPLAY CHANNEL\\nRESETSEQ\\nPending reset sequence number.\\nThis is the sequence number from an outstanding request and it indicates a user RESET CHANNEL command request is outstanding.\\n\\nObject: Cluster Sender Channel (CLUSSDR) and Cluster Sender Channel (CLUSSDR)\\nNew attribute: USEDLQ => New in 7.1\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.con.doc/q082280_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.con.doc/q082280_.htm]\\nWebSphere MQ > WebSphere MQ 8.0.0 > IBM MQ > Reference > Configuration reference > Channel attributes > Channel attributes in alphabetical order > \\nUse Dead-Letter Queue (USEDLQ)\\nDetermines whether the dead-letter queue is used when messages cannot be delivered by channels.\\n\\nObject: Topic\\nNew attribute: CLROUTE => New in 8.0\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.adm.doc/q085770_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.ref.adm.doc/q085770_.htm]\\nWebSphere MQ 8.0.0 > WebSphere MQ > Reference > Administration reference > MQSC reference > The MQSC commands >\\nDEFINE TOPIC\\nCLROUTE - The routing behavior to use for topics in the cluster defined by the CLUSTER parameter.\\n\\n++ Section 2: New attributes in DISPLAY QMGR for new or migrated queue managers\\n\\nFor the whole list of new attributes, see corresponding technote:\\nhttp://www.ibm.com/support/docview.wss?rs=171&uid=swg21578742 [http://www-01.ibm.com/support/docview.wss?rs=171&uid=swg21578742]\\nNew WebSphere MQ 7.1, 7.5, 8.0 and 9.0 attributes in DISPLAY QMGR for new or migrated queue managers\\n[http://www.ibm.com/support/docview.wss?rs=171&uid=swg21578742]\\nAttribute DEFCLXQ (New in 7.5):\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_7.5.0/com.ibm.mq.con.doc/q004790_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_7.5.0/com.ibm.mq.con.doc/q004790_.htm]\\nIBM WebSphere MQ information center, Version 7.5 Operating Systems: UNIX, Linux, Windows\\nWebSphere MQ > Configuring > Configuring a queue manager cluster > Clustering: Best practices >\\nHow to choose what type of cluster transmission queue to use \\nFrom version 7.5 onwards, you can choose which cluster transmission queue is associated with a cluster-sender channel.\\nAll cluster-sender channels are automatically associated with a separate cluster transmission queue. \\nThe queues are created by the queue manager from the model queue SYSTEM.CLUSTER.TRANSMIT.MODEL.QUEUE and named SYSTEM.CLUSTER.TRANSMIT.ChannelName. \\nSelect this alternative default option by setting the queue manager attribute DEFCLXQ to CHANNEL. \\n\\nAttribute PSCLUS (New in 7.1):\\nControls whether this queue manager participates in publish subscribe activity across any clusters in which it is a member. No clustered topic objects can exist in any cluster when modifying from ENABLED to DISABLED.\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_7.5.0/com.ibm.mq.ref.adm.doc/q086240_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_7.5.0/com.ibm.mq.ref.adm.doc/q086240_.htm]\\nWebSphere MQ > Reference > Administration reference > MQSC reference > The MQSC commands\\nDISPLAY QMGR\\nPSCLUS\\nControls whether this queue manager participates in publish subscribe activity across any clusters in which it is a member. No clustered topic objects can exist in any cluster when modifying from ENABLED to DISABLED.\\n\\n++ Section 3: New SYSTEM queues related to clustering\\n\\nFor the whole list of new SYSTEM queues, see corresponding technote:\\nhttp://www.ibm.com/support/docview.wss?uid=swg21608033 [http://www.ibm.com/support/docview.wss?uid=swg21608033]\\nNew SYSTEM queues added in WebSphere MQ 7.1, 7.5, 8.0 and 9.0\\n\\nFor MQ 7.5:\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_7.5.0/com.ibm.mq.con.doc/q017380_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_7.5.0/com.ibm.mq.con.doc/q017380_.htm]\\nWebSphere MQ > Configuring > Configuring a queue manager cluster > Managing WebSphere MQ cluster >\\nAdding a queue manager to a cluster – separate transmission queues\\nEvery time the queue manager creates a cluster-sender channel to send a message to a queue manager, it creates a cluster transmission queue. \\nThe transmission queue is used only by this cluster-sender channel. \\nThe transmission queue is permanent-dynamic. It is created from the model queue, SYSTEM.CLUSTER.TRANSMIT.MODEL.QUEUE, with the name SYSTEM.CLUSTER.TRANSMIT.ChannelName.\\n\\n++ Section 4: Sample: Cluster Queue Monitoring sample program (AMQSCLM)\\n\\nStarting with MQ 7.1:\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.dev.doc/q024620_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_8.0.0/com.ibm.mq.dev.doc/q024620_.htm]\\nWebSphere MQ > Developing applications > Developing MQI applications with IBM MQ > Sample IBM MQ procedural programs > Sample procedural programs (platforms except z/OS) >\\nThe Cluster Queue Monitoring sample program (AMQSCLM) \\nThis sample uses the built-in WebSphere® MQ cluster workload balancing features to direct messages to instances of queues that have consuming applications attached. This automatic direction prevents the build-up of messages on an instance of a cluster queue to which no consuming application is attached.\\n\\n \\n\\n\\n\\nCross reference information Segment Product Component Platform Version Edition Business Integration IBM MQ \\nPRODUCT ALIAS/SYNONYM\\n WebSphere MQ WMQz/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  RC=2082 (MQRC_UNKNOWN_ALIAS_BASE_Q) returned when a COA message\\n   is put to a qalias that resolves to a clustered queue.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 0 Modification 1                     *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: COA/COD Report messages generated from  *\\n   *                      a PUT with The MQMD ReplyToQ field      *\\n   *                      pointing to a non-clustered QAlias      *\\n   *                      whose Base Name is a Clustered queue    *\\n   *                      are not resolved when the ReplyToQmgr   *\\n   *                      equates to the local qmanager name of   *\\n   *                      the qmgr at which the final put to the  *\\n   *                      QAlias occurs and the COA/COD message   *\\n   *                      goes to the Dead Letter Queue with      *\\n   *                      MQRC_UNKNOWN_ALIAS_BASE_Q.              *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   This Apar fixes a difference between all the Distributed\\n   platform\\'s implementations of COA/COD/Expiry report processing\\n   and the z/OS codebase. This occurs in specific configurations\\n   when the ReplyToQ in the original MQMD is a non-clustered QAlias\\n   queue on a z/OS QMGR, with a base queue name of a CLUSTER queue\\n   and ReplyToQmgr is non-blank and at the time of the PUT of the\\n   Report message is the Local qmgr name.\\n   e.g When the original message MQPUT is to a queue on QMGR A, but\\n       the ReplyToQ is a QAlias on QMGR A or B with a base queue\\n       name of a clustered queue and ReplyToQmgr has the QMGR name\\n       of the qmgr where the QAlias exists.\\n   The resolution of the QAlias is done on the qmgr where it exists\\n   and at that time the ReplyToQmgr value is used as ObjectQmgrName\\n   in the MQOD used to open the QAlias. Because it is the Local\\n   QMGR the z/OS code does not attempt a cluster lookup when the\\n   QAlias base queue name does not exist on that qmgr and the\\n   Report message is put to the local Dead Letter Queue with a\\n   Reason code of 2082 (MQRC_UNKNOWN_ALIAS_BASE_Q).\\n   This does NOT occur IF the QAlias itself is clustered or if a\\n   QMGR Alias is used as ReplyToQmgr.\\n   .\\n   On all the Distributed platform versions of MQ this problem does\\n   NOT occur, the QAlias base name is resolved via a lookup of the\\n   name in the fastnet cache and the Report message arrives\\n   correctly at its intended destination via cluster workload\\n   balancing.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  Module CSQMREPM creates the Report messages for Expiration, COA\\n   & COD. When a Report message is about to be PUT in module\\n   csqmrepm, the ReplyToQmgr field in the original Mhed is checked\\n   to see if the LOCAL qmgr or the QSG name was specified - in\\n   which case the fix blanks the ObjectQMgrName in the MQOD to be\\n   used to open the ReplyToQ. This causes CSQMOVAL to set the\\n   fOtherQmgr flag in the MHND and allows a cluster lookup for the\\n   queue in CSQMOAQ1 to occur when a QAlias base queue name is not\\n   defined in the local qmgr.\\n   In addition, in the CHINIT in rriAddMessage (CSQXRMMQ) when a\\n   receiver channel is about to PUT a message to its destination,\\n   queue checks are added to determine if it is a Report message\\n   for Expiration, COA or COD that is NOT being put to the Dead\\n   Letter Queue AND the destination qmgr is the LOCAL qmgr name or\\n   the QSG name - then as above the ObjectQMgrName in the MQOD is\\n   set to blanks to allow cluster lookups for the QAlias base\\n   queue name.\\n   010Y\\n   CSQMREPM\\n   CSQXRMMQ\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PM18426\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   010\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2010-07-15\\n   \\n   \\n * CLOSED DATE\\n   2010-10-15\\n   \\n   \\n * LAST MODIFIED DATE\\n   2010-12-02\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PM11753 [http://www-01.ibm.com/support/docview.wss?uid=swg1PM11753]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK61360\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQMREPM CSQXRMMQ\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R010 PSY UK61360 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK61360]\\n   UP10/11/06 P F011\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  A REFRESH CLUSTER(clusname) REPOS(NO) command was issued.\\n   Afterward, some other queue managers in the cluster were\\n   unreachable from the queue manager where the REFRESH was done.\\n   Applications received MQRC 2087 when attempting to send reply\\n   messages back to these queue managers.\\n   \\n   The issue occurs when a cluster sender channel is active at the\\n   time that the refresh is issued but then goes inactive before\\n   the full repository has republished that cluster-sender record\\n   in response to the refresh.\\n   \\n   Additional Symptom(s) Search Keyword(s):\\n   MQRC_UNKNOWN_REMOTE_Q_MGR CLUSSDR\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  Do another REFRESH CLUSTER command or simply alter the\\n   description of the CLUSRCVR channel on the target queue manager\\n   to cause the cluster subscription to be republished.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of IBM MQ for z/OS Version 9       *\\n   *                 Release 0 Modification 0                     *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: A Partial Repository Queue Manager may  *\\n   *                      incorrectly lose sight of other Qmgrs   *\\n   *                      in a cluster, after a REFRESH CLUSTER   *\\n   *                      REPOS(NO) command is issued.            *\\n   ****************************************************************\\n   If auto-defined cluster sender channels are running when a\\n   REFRESH CLUSTER REPOS(NO) command is issued, but the channel\\n   stops prior to the REFRESH command processing completes, the\\n   republish of cluster Qmgr records may not be correctly added to\\n   the local cluster cache by reconcile processing. As a result the\\n   Qmgr may lose sight of the remote cluster Qmgr until the\\n   internal subscription for the Qmgr expires or the remote Qmgr\\'s\\n   cluster receive definition is altered.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  Cluster refresh and reconcile processing has been corrected,\\n   to ensure the republished cluster Qmgr record is added to the\\n   cluster cache, when a running cluster channel stops prior to\\n   refresh processing completing.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI98289\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   IBM MQ Z/OS V9\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655MQ900\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2018-05-23\\n   \\n   \\n * CLOSED DATE\\n   2018-07-06\\n   \\n   \\n * LAST MODIFIED DATE\\n   2018-07-25\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PI96848 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI96848]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UI57067\\n   \\n   \\n\\nMODULES/MACROS\\n *  CMQXRECO CMQXRREF\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   IBM MQ Z/OS V9\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655MQ900\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UI57067 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI57067]\\n   UP18/07/25 I 1000\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  Abend 0C4-00000004 in the CHIN log when\\n   trying to remove cluster queue from cluster.\\n   .\\n   It seems after putting a message to the Dead Letter Queue (DLQ)\\n   the rriAddDLQMessage does exception reporting as requested\\n   in the MQ message descriptor (MQMD).\\n   .\\n   You can get an 0C4-00000004 when WMQ copies the original MQMD\\n   to the reply message.\\n   .\\n   In the CHIN job-log you will see the following:\\n   .\\n   +CSQX202E :MQxx CSQXRCTL Connection or remote listener\\n   unavailable,\\n    channel TO.QA1.CHA01,\\n    connection (xxx.xxx.xxx.xxx)\\n    TRPTYPE=TCP RC=00000468 (ECONNREFUSED) reason=00000000\\n   +CSQX501I :MQxx CSQXRCTL Channel TO.QA1.CHA01 is no longer\\n   active\\n   IEA794I SVC DUMP HAS CAPTURED:\\n   DUMPID=002 REQUESTED BY JOB (MQxxCHIN)\\n   DUMP TITLE=M86A,ABN= 0C4-00000004,C=R3600.701.CHIN,M=CSQXPRCR,L\\n              OC=CSQXRCTL.CSQXRMMQ+01658\\n    CSQX054E :MQxx CSQXPRCR Repository manager ended abnormally,\\n    reason=0C4000-00000004 .\\n   +CSQX054E :MQxx CSQXPRCR Repository manager ended abnormally,\\n    reason=378000-00000014 .\\n   In the dump you will see the following:\\n   ..\\n   |ABN= 0C4-00000004,C=R3600.701.CHIN,M=CSQXPRCR,LOC=CSQXRCTL.|\\n   |CSQXRMMQ+01658                                             |\\n   .\\n   RIDS/CSQXRCTL#L RIDS/#UNKNOWN AB/S00C4 PRCS/00000004\\n   REGS/06BFA REGS/30328\\n   .\\n   Symptom             Description\\n     -------             -----------\\n     RIDS/CSQXRCTL#L     Load module name: CSQXRCTL\\n     RIDS/#UNKNOWN       Csect name: #UNKNOWN\\n     AB/S00C4            System abend code: 00C4\\n     PRCS/00000004       Abend reason code: 00000004\\n     REGS/06BFA          Register/PSW difference for R06: BFA\\n     REGS/30328          Register/PSW difference for R03:-0328\\n   .\\n   Time of Error Information\\n   .\\n   PSW: 07850000 80000000 00000000 0C50A880\\n   Instruction length: 04   Interrupt code: 0004\\n   Failing instruction text: D218BFFF 5898A784 002758E0\\n   .\\n   Breaking event address: 00000000_0C5A379A\\n   Registers 0-3\\n    GR: 00000001 0D4C276C 0D4C120C 0C50ABA8\\n    AR: 009FF890 00000000 00000000 00000000\\n   Registers 4-7\\n    GR: 0C3AE5B0 00000000 8C509C86 0000001C\\n    AR:00000000 00000000 00000000 00000000\\n   .\\n   Registers 8-15\\n    GR: 00000000 00000000 00000000 0D678FE8\\n    AR: 00000000 00000000 00000000 00000000\\n   Registers 11-15\\n    GR:0D676068 0D4C2720 E7D8C840 00000000\\n    AR:00000000 00000000 00000000 00000000\\n   .\\n   Home ASID: 0058 Primary ASID: 0058  Secondary ASID: 0058\\n   PKM: 00C0       AX: 000C            EAX: 0000\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 0 Modification 1.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: ABEND0C4 in CSQXRMMQ.                   *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   When an Exception report has been requested (for example, after\\n   a cluster queue has been removed from a cluster) a reply-to\\n   message is constructed.\\n   In CSQXRMMQ routine rriAddDlqMessage a check is done to see if\\n   there is any ROUTE information, but the pointer to the block\\n   that would hold this information is always zero, when called\\n   from the repository task, so an ABEND0C4 occurs.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  CSQXRMMQ has been altered to check for a zero pSess pointer\\n   before using it.\\n   010Y\\n   CSQXRMMQ\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PM49925\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   018\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2011-10-12\\n   \\n   \\n * CLOSED DATE\\n   2011-11-08\\n   \\n   \\n * LAST MODIFIED DATE\\n   2012-01-01\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK73636\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQXRMMQ\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R010 PSY UK73636 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK73636]\\n   UP11/12/02 P F112\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  The QMGR attribute SQQMNAME controls how the ObjectQmgrName is\\n   treated for puts to shared queues. This attribute defaults to\\n   \\'USE\\', which means that the ObjectQmgrName will be used in name\\n   resolution and the appropriate transmission queue will be used.\\n   See the Name Resolution chart at\\n   http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/topic/com.ib\\n   m.mq.csqzal.doc/fg12170_.htm [http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/topic/com.ibm.mq.csqzal.doc/fg12170_.htm]\\n   The resolved name might be SYSTEM.CLUSTER.TRANSMIT.QUEUE or\\n   SYSTEM.QSG.TRANSMIT.QUEUE\\n   .\\n   The alternative is to set it to \\'IGNORE\\'.  This means that the\\n   ObjectQmgrName will be ignored, and the put will be performed\\n   locally if the queue is shared and ObjectQmgrName specifies a\\n   queue manager in the same Queue Sharing Group (QSG).\\n   .\\n   This attribute does not (and should not) apply to cluster\\n   queues.  However, the check on whether the queue is a cluster\\n   queue also incorrectly catches the case where the queue is not\\n   clustered, but the route to the remote queue manager that has\\n   been found is a cluster channel. For such cases the default\\n   behavior will still be used, regardless of the setting of\\n   SQQMNAME.\\n   .\\n   If SQQMNAME(IGNORE) works correctly, messages are put directly\\n   to the shared queue, which may improve performance slightly over\\n   sending the messages over a channel.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 0 Modification 1.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Messages put to a shared queue using a  *\\n   *                      fully qualified remote name are not     *\\n   *                      put directly to the queue when SQQMNAME *\\n   *                      is set to IGNORE.                       *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   When putting to a remote queue on a qmgr in the same qsg as the\\n   local qmgr, messages should be put directly to the shared queue\\n   by the local qmgr instead of being sent via channels/IGQ if the\\n   qmgr object specifies SQQMNAME(IGNORE).\\n   If there is no default xmitq for the remote qmgr (i.e. a queue\\n   with the same name as the remote qmgr), and both qmgrs belong\\n   to the same cluster, CSQMOPEN will open the cluster transmit\\n   queue and use the existing cluster channel to put the message.\\n   If SQQMNAME(IGNORE) is set, it incorrectly assumes that the\\n   queue must be clustered, and so cannot be put to directly and\\n   continues to send the message over the channel.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  CSQMOPEN is changed to allow SQQMNAME(IGNORE) to take effect\\n   when putting to non-clustered shared queues on another qmgr in\\n   the qsg, even if the qmgrs are in a cluster.\\n   010Y\\n   CSQMOPEN\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PM22372\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   010\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2010-09-13\\n   \\n   \\n * CLOSED DATE\\n   2010-09-30\\n   \\n   \\n * LAST MODIFIED DATE\\n   2010-11-02\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PM22462 [http://www-01.ibm.com/support/docview.wss?uid=swg1PM22462] UK61047\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQMOPEN\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R010 PSY UK61047 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK61047]\\n   UP10/10/15 P F010\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  There are two QMgrs in a QSG, and activated the intragroup\\n   queuing.\\n   The two QMgrs do not have a dead letter queue assigned.\\n   There is a queue on QMgr A which is not shared.\\n   From QMgr B a message is put to a queue in QMgr A via a Qremote\\n   definition. The Qremote definition point a queue on QMgr A\\n   which does not exist. The message goes through IGQ tranmit queue\\n   but cannot be delivered. The message appears to disappear and\\n   CSQM064I is issued repeatedly.\\n   .\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of Websphere MQ for z/OS           *\\n   *                 Version 6 Release 0 Modification 0           *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Messages are lost when the Intra Group  *\\n   *                      Queuing Agent tries to put messages     *\\n   *                      to the dead letter queue when this is   *\\n   *                      defined on the Queue Manager object as  *\\n   *                      either SYSTEM.QSG.TRANSMIT.QUEUE or     *\\n   *                      it is undefined.                        *\\n   ****************************************************************\\n   * RECOMMENDATION: Ensure the Dead Letter Queue as defined on   *\\n   *                 the Queue Manager object has a valid queue   *\\n   *                 definition.                                  *\\n   ****************************************************************\\n   The Intra Group Queueing Agent fails to put a message to the\\n   intended target queue so instead tries to put it to the dead\\n   letter queue. The inquire of the Queue Manager object is\\n   successful and a value obtained for the name of the dead letter.\\n   A later test of this value means that no processing is performed\\n   on the message if the DLQ value is either\\n   SYSTEM.QSG.TRANSMIT.QUEUE or it is undefined and thus blank.\\n   This processing is then bypassed and the message is discarded.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  If the DLQ value on the Queue Manager object is either blank or\\n   SYSTEM.QSG.TRANSMIT.QUEUE when an attempt is made to place a\\n   message on the DLQ, then the attempted put will be returned as\\n   invalid, the get from the s.q.t.q and subsequent put will be\\n   backed out, and the processing of IGQ will go into retry.\\n   This will be indicated to user by IGQ task issuing message\\n   CSQM063E.\\n   .\\n   Messages and Codes Manual should be updated to include the\\n   following.\\n   .\\n   CSQM063E csect specified Dead Letter Queue name unacceptable.\\n   \\n   Explanation:The Intra Group Queuing Agent (IGQ) has attempted\\n   to put a persistent message on the Dead Letter Queue that is\\n   specified on the Queue Manager object. The Dead Letter Queue\\n   specified is SYSTEM.QSG.TRANSMIT.QUEUE or there is no Dead\\n   Letter Queue name specified.\\n   .\\n   Severity: 4\\n   .\\n   System Action: The put of the message to the Dead Letter Queue\\n   is backed out, the message is placed back on the\\n   SYSTEM.QSG.TRANSMIT.QUEUE and the Intra Group Queueing agent\\n   will go into retry.\\n   .\\n   System Programmer Response: Ensure the Queue Manager object has\\n   a Dead Letter queue specified which is neither blank nor\\n   SYSTEM.QSG.TRANSMIT.QUEUE. Examine the the message to determine\\n   the reason for its placement on the Dead Letter Queue.\\n   000Y\\n   001Y\\n   002Y\\n   003Y\\n   004Y\\n   CSQFMDIC\\n   CSQFMDIE\\n   CSQFMDIK\\n   CSQFMDIU\\n   CSQFMDTA\\n   CSQFMTXC\\n   CSQFMTXE\\n   CSQFMTXK\\n   CSQFMTXU\\n   CSQFXTXC\\n   CSQFXTXE\\n   CSQFXTXK\\n   CSQFXTXU\\n   CSQMINTC\\n   CSQMMNUM\\n   CSQMPRUM\\n   CSQMPTDQ\\n   CSQMPTRP\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n *  *********\\n   * HIPER *\\n   *********\\n   \\n   \\n    \\n   \\n   \\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK26947\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   YesHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2006-06-23\\n   \\n   \\n * CLOSED DATE\\n   2006-07-13\\n   \\n   \\n * LAST MODIFIED DATE\\n   2006-09-05\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PK25458\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK16129 UK16146 UK16158 UK16159 UK16163\\n   \\n   \\n\\nMODULES/MACROS\\n *     CSQFMDIC CSQFMDIE CSQFMDIK CSQFMDIU CSQFMDTA\\n   CSQFMTXC CSQFMTXE CSQFMTXK CSQFMTXU CSQFXTXC CSQFXTXE CSQFXTXK\\n   CSQFXTXU CSQMINTC CSQMMNUM CSQMPRUM CSQMPTDQ CSQMPTRP\\n   \\n   \\n    \\n   \\n   \\n\\nPublications Referenced GC34660200 FIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655L8200\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UK16129 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK16129]\\n   UP06/08/05 P F608\\n   \\n   \\n * R001 PSY UK16146 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK16146]\\n   UP06/08/05 P F608\\n   \\n   \\n * R002 PSY UK16158 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK16158]\\n   UP06/08/05 P F608\\n   \\n   \\n * R003 PSY UK16159 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK16159]\\n   UP06/08/05 P F608\\n   \\n   \\n * R004 PSY UK16163 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK16163]\\n   UP06/08/05 P F608\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.wste_webcast; WSTE; Support Technical Exchange; education; presentation; webcast; STEwebcastDocument; MQ Cluster; websphere mq clustering; mq clustering; administration errors; cluster troubleshooting; cluster mistakes; clustering pitfalls WEBCAST\\n\\nABSTRACT\\n IBM Ask the Experts discussion about MQ Clustering Gotchas! Avoiding Cluster Administration Pitfalls on 19 November 2013. \\nMQ Clustering was introduced in v5.1 and customers have adopted the clustering method of messaging for ease of admin. and workload balance. Certain rules apply for config and admin, which if not followed, cause adverse effects. This session discusses ways to avoid MQ Cluster administration mistakes. \\n\\nCONTENT\\nAsk the Experts sessions are conducted in a question and answer format. The table below provides a time index (minutes:seconds) to the recording and describes the questions or topics discussed. You can fast forward to any question using the time index. A table containing the panel of experts is also included.\\n\\n\\n\\n\\nTo play or download the audio of this Ask the Experts session, see the Audio Section of this document (audio not available until after the webcast).\\n\\n\\nSee the Related Information Section of this document for a list of documents referenced during the presentation.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk the Experts session \\n19 November 2013 - 11:00 a.m. - 12:00 p.m. EST \\n\\nTime Questions asked 00:00 General introduction 02:50 Technical introduction 04:00 Improperly removing a cluster queue manager from the cluster. 10:46 Improper use of Refresh Cluster and Reset Cluster administration commands. 18:04 Ignoring cluster related messages in the queue manager error logs. 21:50 Improperly using the MQ clustering feature for High Availability or Disaster Recovery purposes. 26:33 Improperly configuring MQ Cluster channels and repositories. 30:58 HIPER APAR IV25030 34:18 Open lines for live question and answer period 34:28 Have you had any reported issues with multi-instance queue managers? Should any special steps be taken when joining a multi-instance QM to a cluster? 37:24 Slide 17, Workload Balancing, how to troubleshoot imbalanced workload among the cluster? 39:26 Pertaining to HIPER APAR IV25030, would you only demote or refresh the full repositories if you\\'re seeing this corruption? 43:27 Recently my servers moved data centers, when the queue managers and repositories came up in the new data center, their IP addresses were changed. What would be the best way to recover? 46:17 We run Queue Managers on a High Availability server, meaning that they may be started on different hosts. Can you explain why the Queue Managers shold not be shut down? 52:14 Clearing out the SYSTEM.CLUSTER.TRANSMIT.QUEUE, is there an updated Support Pack? 58:37 Imbalanced workload among Queue Managers in remote clusters, would destination sequence factors affect the load balancing? 01:03:41 We are running MQ 7.0.1.10 on Solaris 10 and run into an issue that a new qmgr on another server can not join this cluster; the repository manager process keeps dying and messages in SYSTEM.CLUSTER.COMMAND.QUEUE are queing up. What could cause this issue? 01:06:43 Aside from the stranded messages, will other messages behind them also be hung pending delivery to other queue managers? 01:07:20 Is the administration error missing on slide 21? 01:07:43 Is there an updated support pack to remove stranded messages from SYSTEM.CLUSTER.TRANSMIT.QUEUE - the one referencd is for MQ V5.3? 01:08:48 what fixpack level would be EXPOSED to the problem in IV25030? 01:09:43 Slide 21: Create a new channel to a full repository queue manager. Is this done on the queue manager with the partial repository? 01:10:12 When adding Partial Repositories to a cluster, what are the considerations for which Full Repository we should aim the manual CLUSSNDR at? Is it fine to aim all Partial Repositories\\' manual CLUSSNDRs to one Full Repository? The partial will immediately create automatic cluster senders to both fulls anyway, so is it irrelevant which FR the partial aims its manual CLUSSNDR at? 01:11:29 I have two QM ids (with same QM) showing in full repos. Old QM id still exists even after the QM is taken out of cluster. All i need to is to reset the cluster to remove the QM id every time. How do i make the old QM ID (with the same QM) to go automatically? 01:12:35 Are there any best practices or considerations you can post with regards to DR setups? For instance, when bringing up Full Repo\\'s in DR mode (for testing purposes). Should we rely solely on network seperation, etc.? 01:13:46 Closing remarks 01:14:51 End of Call \\nPanel of Experts: Andy Emmett WebSphere MQ Distributed L3 Support Yana Johnson WebSphere MQ Distributed L2 Support Valerie Lampkin WebSphere MQ Distributed L2 Support William (Bill) Moss Websphere MQ Distributed L2 Support Miguel Rodriguez WebSphere MQ Distributed L2 Support \\n\\n\\nPRESENTATION\\n[http://www.ibm.com/i/v16/icons/weblecture.gif]WSTE-11192013-AskTheExperts-MQ_Clustering_Gotchas.pdf [/support/docview.wss?uid=swg27039756&aid=2]WSTE-11192013-AskTheExperts-MQ_Clustering_Gotchas.pdf [/support/docview.wss?uid=swg27039756&aid=1] \\n\\n\\n\\nGet Adobe Reader [http://www.adobe.com/products/acrobat/readstep2.html] to view PDF\\n\\n\\nAUDIO\\n[http://www.ibm.com/i/v16/icons/aud.gif]To replay the recording of this conference call, click on Download Audio [http://public.dhe.ibm.com/software/websphere/techexchange/November-19-2013-Emmett-Johnson-Lampkin-Moss-Rodriguez.mp3] (audio in MP3 format). Right-click and select Save As to store the file on your local computer for later playback. Remember that you can fast forward to any question using the time index.\\n\\n\\n\\nRELATED INFORMATION\\n WebSphere Support Technical Exchange [http://www.ibm.com/software/websphere/webcast]\\n\\n\\n\\n\\n\\n\\nCross reference information Segment Product Component Platform Version Edition Business Integration IBM MQ Clustering AIX, HP-UX, Linux, OpenVMS, Solaris, Windows 9.0, 8.0, 7.5, 7.1, 7.05724H7206 5724H7226 DeadLetter Queue Handler IBMi setup STRMQMDLQ TECHNOTE (TROUBLESHOOTING)\\n\\nPROBLEM(ABSTRACT)\\n This document explains how to set up and use the WebSphere MQ (WMQ) Dead Letter Queue (DLQ) handler on IBMi. \\n\\nRESOLVING THE PROBLEM\\nYou must create a rules table, add the desired rules, and start the dead letter queue handler.\\nCreate a member of a source file, for example, QRULE in <QMGRLIB>/QMQSC and then add the control data and rules.\\n\\nUse the STRMQMDLQ command to invoke the Dead Letter Queue (DLQ) handler. If you do not specify a dead letter queue or a queue manager name, the default queue manager is used along with the\\nDLQ that belongs to that queue manager. The STRMQMDLQ command takes its input from the rules table.\\n\\n1. CRTSRCPF FILE(<QMGRLIB>/QMQSC) MBR(QRULE) TEXT(\\'Websphere MQ\\').\\n2. STRSEU SRCFILE(<QMGRLIB>/QMQSC) SRCMBR(QRULE) OPTION(2).\\n3. Add the control data and rules. \\n4. Once the control data and rules have been added, Press the F3 key to EXIT.\\n5. Next, press Enter to save the member.\\n6. To execute the rules table for a queue manager use the STRMQMDLQ command.\\nExample: STRMQMDLQ UDLMSGQ(SYSTEM.DEAD.LETTER.QUEUE) SRCMBR(QRULE) \\nSRCFILE(<QMGRLIB>/QMQSC) MQMNAME(<QMGRNAME>) \\n\\nNOTE\\n(1): It is recommended that you submit the STRMQMDLQ command to run in batch\\nbecause the utility remains active indefinitely, by default.\\n\\n(2): If a value is specified both in the rules table and on input to the STRMQMDLQ\\ncommand, the value specified on the STRMQMDLQ command takes precedence.\\n\\n(3): For rules that span multiple lines, they may be concatenated with a \\'+\\' sign.\\n\\n* Control Data Examples\\n* --------------------- \\n* Use the system default dead-letter queue for the default queue manager\\n* and leave the DLQ handler active indefinitely.\\n* inputqm = QUEUE MANAGER which owns Dead Letter Queue\\n* inputq = Name of DEAD LETTER QUEUE \\n* wait = Whether to wait for more messages to arrive after all messages processed on DLQ \\nINPUTQ (\\' \\') INPUTQM (\\' \\') WAIT (YES) \\n\\n* Rule Examples\\n* -------------\\n* If a message is placed on the DLQ that was destined for the HOLIDAY\\n* queue, forward all the messages to the HOLIDAY_BACKUP queue on the\\n* queue manager specified in the DestQMgrName field of the MQDLH structure and\\n* remove the DLQ header before forwarding the message. \\n* NOTE: Always remove the DLQH before forwarding messages from the DLQ \\n\\nDESTQ(HOLIDAY) ACTION(FWD) FWDQ(HOLIDAY_BACKUP) +\\nFWDQM(&DESTQM) HEADER(NO) \\n\\n* If a message is placed on the DLQ because of a put inhibited\\n* condition, attempt to retry putting the message to its\\n* destination queue. Make 5 attempts at approximately\\n* 60-second intervals.\\n\\nREASON(MQRC_PUT_INHIBITED) ACTION(RETRY) RETRY(5)\\n\\n* If a message is placed on the DLQ because its destination\\n* queue is full, attempt to retry putting the message to its\\n* destination queue. Make 5 attempts at approximately\\n* 60-second intervals.\\n\\nREASON(MQRC_Q_FULL) ACTION(RETRY) RETRY(5) \\n\\n* If a message is placed on the DLQ and none of the above rules is match\\n* then use the catchall rule below. Put the message on the manual\\n* intervention queue for the system\\'s default queue manager.\\n* The DLQH is left on the message for problem analysis. \\n\\nACTION(FWD) FWDQ(DEADQ.MANUAL.INTERVENTION)\\n\\nSample_Rules.txt [/support/docview.wss?uid=swg21622347&aid=2]Sample_Rules.txt [/support/docview.wss?uid=swg21622347&aid=1]\\n\\nRELATED INFORMATION\\n Control and Rules Keywords [http://www.ibm.com/support/knowledgecenter/SSFKSJ_7.0.1/com.ibm.mq.amqwag.doc/ia11620_.htm]\\nSample Rules Table [http://www.ibm.com/support/knowledgecenter/SSFKSJ_7.0.1/com.ibm.mq.amqwag.doc/ia11700_.htm]\\nWebcast replay: Automating WebSphere MQ for IBM i V6 an [http://www.ibm.com/support/docview.wss?uid=swg27019350]\\n\\n\\n\\n\\nPRODUCT ALIAS/SYNONYM\\n WMQ MQ TECHNOTE (FAQ)\\n\\nQUESTION\\n You are trying to run the amqsdlq utility (Dead Letter Queue handler) in WebSphere MQ and you get the error:\\nodq-001 Environment variable \"ODQ_MSG\" not set \\n\\nANSWER\\nPrior to issuing amqsdlq, you need to define the environment variable ODQ_MSG to indicate where to find the file that has the information or error messages for amqsdlq that can be customized by the user:\\nexport ODQ_MSG=/opt/mqm/samp/dlq/amqsdlq.msg\\n\\nFor more information on amqsdlq see the following 2 links from the MQ V7 Information Center:\\n\\nhttp://www.ibm.com/support/knowledgecenter/SSFKSJ_9.0.0/com.ibm.mq.adm.doc/q005650_.htm [http://www.ibm.com/support/knowledgecenter/SSFKSJ_9.0.0/com.ibm.mq.adm.doc/q005650_.htm]\\nWebSphere MQ > WebSphere MQ 9.0.0 > IBM MQ > Administering > Administering local IBM MQ objects > Working with dead-letter queues > Processing messages on a dead-letter queue > Invoking the DLQ handler > \\nThe sample DLQ handler, amqsdlq \\n\\nhttp://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q024150_.htm [http://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q024150_.htm]\\nWebSphere MQ > WebSphere MQ 9.0.0 > IBM MQ > Developing applications > Developing MQI applications with IBM MQ > Sample IBM MQ procedural programs > Sample procedural programs (platforms except z/OS) >\\nDead-letter queue handler sample\\n\\nWebSphere MQ > Application Programming Guide > Sample WebSphere MQ programs > Sample programs (platforms except z/OS)\\nDead-letter queue handler sample [http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/index.jsp?topic=%2Fcom.ibm.mq.csqzal.doc%2Ffg17920_.htm]\\n+ begin\\nThe sample is similar to the dead-letter handler provided within the product but trace and error reporting are different. There are two environment variables available to you: \\nODQ_MSG\\nSet to the name of the file containing error and information messages. The file provided is called amqsdlq.msg.\\n+ end\\n\\nIn UNIX (other than AIX), the default location of the file that contains the message file for amqsdlq is:\\n/opt/mqm/samp/dlq/amqsdlq.msg\\nIn AIX it is:\\n/usr/mqm/samp/dlq/amqsdlq.msg\\nIn Windows it is:\\nC:\\\\Program Files\\\\IBM\\\\WebSphere MQ\\\\tools\\\\c\\\\Samples\\\\dlq\\\\amqsdlq.msg\\n\\nPlease see the following Presentation for the details on how to use the DLQ handler:\\nHow to Automate Handling of WebSphere MQ Dead Letter Messages [http://www.ibm.com/support/docview.wss?&uid=swg27008375]\\n\\nThis WebSphere Support Technical Exchange is designed to discuss techniques provided in WebSphere MQ for automating the way you handle messages which arrive on your dead letter queue. The primary focus of this presentation is on the runmqdlq command and the rules table which is associated with that command. The discussions in this presentation are based upon WebSphere MQ v5.3 on the distributed platforms, but they are applicable to WebSphere MQ v6.0 as well.\\nIn particular, pages 22 thru 26. Those pages talk about runmqdlq, but the invocation is the same for amqsdlq.\\n\\nThe comments in the header of the following file:\\n/opt/mqm/samp/dlq/amqodqka.c\\n... provide an explanation of the syntax for invoking amqsdlq and runmqdlq, and the options:\\n/* runmqdlq <queue_name <queue_mgr_name> < <InputFile */ \\n\\n++ Scenario\\n\\nThis scenario shows how to create a rule that moves all the messages from the SYSTEM DLQ in QMGR-1 into the queue Q1 in QMGR-1 without the DLQ headers:\\n\\na) Export the environment variable (as in UNIX):\\nexport ODQ_MSG=/opt/mqm/samp/dlq/amqsdlq.msg\\n\\nb) Create file \"/tmp/dlq.rules\"\\nc) Add the line:\\nACTION(FWD) FWDQ(Q1) FWDQM(\\'\\') HEADER(NO)\\n\\nd) Issue the command:\\n$ amqsdlq SYSTEM.DEAD.LETTER.QUEUE QM_VER < /tmp/dlq.rules\\n08/11/11 13:48:52 odq-8: Dead-letter queue handler started to process INPUTQ(SYSTEM.DEAD.LETTER.QUEUE).\\n... (wait for processing to be done) ...\\nCtrl-C (to end amqsdlq)\\n\\nNotes:\\n- The rules from /tmp/dlq.rules will be used and amqsdlq will not end, because it will keep monitoring the DLQ for incoming messages. You will need to manually terminate the program with Ctrl-C.\\n- These are the details of the particular action listed in the rules file:\\nACTION(FWD) => Forward the message\\nFWDQ(Q1) FWDQM(\\'\\') => To Queue Q1 in the current queue manager (represented by single quote, single quote)\\nHEADER(NO) => Do not include the DLQ Header. Whether the MQDLH should remain on a message for which ACTION (FWD) is requested. By default, the MQDLH remains on the message. The HEADER keyword is not valid for actions other than FWD.\\n\\n \\n\\nPRODUCT ALIAS/SYNONYM\\n WMQ MQ MQSeries  FIXES ARE AVAILABLE\\nWebSphere MQ V7.0.1 for i5/OS Fix Pack 7.0.1.3 [http://www-01.ibm.com/support/docview.wss?uid=swg24026933]\\nWebSphere MQ V7.0 Fix Pack 7.0.1.3 [http://www-01.ibm.com/support/docview.wss?uid=swg24026932]\\nWebSphere MQ V6.0 Fix Pack 6.0.2.11 [http://www-01.ibm.com/support/docview.wss?uid=swg24029382]\\nWebSphere MQ V6.0 for iSeries Fix Pack 6.0.2.11 [http://www-01.ibm.com/support/docview.wss?uid=swg24029383]\\nWebSphere MQ 6.0 for HP OpenVMS Alpha and Itanium - Fix Pack 6.0.2.11 (FP 04) [http://www-01.ibm.com/support/docview.wss?uid=swg24032452]\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  An application receives a MQRC_UNKNOWN_OBJECT_NAME (2085)\\n   reason code from an MQOPEN/MQPUT1 call to a clustered queue\\n   object which exists in one of the clusters to which the queue\\n   manager belongs. The queue manager to which the application is\\n   connecting to is shared in more than one cluster.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  na\\n   keywords: cluster\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   USERS AFFECTED:\\n   Any users of WebSphere MQ connecting to a queue manager shared\\n   in more than one cluster and putting messages to a clustered\\n   queue present in a subset of those clusters.\\n   \\n   Platforms affected:\\n   All Distributed (iSeries, all Unix and Windows)\\n   ****************************************************************\\n   PROBLEM SUMMARY:\\n   When an application issues an MQOPEN/MQPUT1 against a clustered\\n   queue which the local queue manager does not yet know about,\\n   subscriptions are created and sent to the full repositories of\\n   which the local queue manager is a member of.\\n   \\n   A 10 second wait period is then entered where one of the\\n   following should occur.\\n   \\n   1. If any objects are returned within 10 seconds, the API call\\n   should continue processing without needing to wait any further.\\n   \\n   2. If no objects are returned within 10 seconds, and not all\\n   full repositories have acknowledged the subscription request,\\n   then a MQRC_CLUSTER_RESOLUTION_ERROR should be returned to the\\n   application. This is to indicate a possible communications\\n   problem with some of the full repositories.\\n   \\n   3. If all full repositories have acknowledged the subscription\\n   and none of them are aware of any matching object definitions,\\n   the API call should immediately return with\\n   MQRC_UNKNOWN_OBJECT_NAME.\\n   \\n   Where a queue manager shared in many clusters is putting\\n   messages to a queue hosted in a only subset of these, it is\\n   likely that the first full repository to reply does not know\\n   about the object in question. In this scenario, the problem was\\n   caused because the local queue manager did not wait to see if\\n   other full repositories did know about the object subscribed\\n   for, but instead returned MQRC_UNKNOWN_OBJECT_NAME before 10\\n   seconds had elapsed.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  The MQOPEN/MQPUT1 logic against a remote clustered queue was\\n   modified to ensure that it does not return as soon as the first\\n   reply is received from a full repository for a subscription to\\n   the clustered queue.\\n   \\n   ---------------------------------------------------------------\\n   The fix is targeted for delivery in the following PTFs:\\n   \\n                      v6.0\\n   Platform           Fix Pack 6.0.2.11\\n   --------           --------------------\\n   Windows            tbc_p600_0_2_11\\n   AIX                tbc_p600_0_2_11\\n   HP-UX (PA-RISC)    tbc_p600_0_2_11\\n   HP-UX (Itanium)    tbc_p600_0_2_11\\n   Solaris (SPARC)    tbc_p600_0_2_11\\n   Solaris (x86-64)   tbc_p600_0_2_11\\n   iSeries            tbc_p600_0_2_11\\n   Linux (x86)        tbc_p600_0_2_11\\n   Linux (x86-64)     tbc_p600_0_2_11\\n   Linux (zSeries)    tbc_p600_0_2_11\\n   Linux (Power)      tbc_p600_0_2_11\\n   Linux (s390x)      tbc_p600_0_2_11\\n   \\n                      v7.0\\n   Platform           Fix Pack 7.0.1.3\\n   --------           --------------------\\n   Windows            U200320\\n   AIX                U834987\\n   HP-UX (PA-RISC)    U834414\\n   HP-UX (Itanium)    U834413\\n   Solaris (SPARC)    U834986\\n   Solaris (x86-64)   U834210\\n   iSeries            tbc_p700_0_1_3\\n   Linux (x86)        U834415\\n   Linux (x86-64)     U834985\\n   Linux (zSeries)    U834412\\n   Linux (Power)      U835662\\n   \\n   The latest available maintenance can be obtained from\\n   \\'WebSphere MQ Recommended Fixes\\'\\n   http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006037 [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006037]\\n   \\n   If the maintenance level is not yet available, information on\\n   its planned availability can be found in \\'WebSphere MQ\\n   Planned Maintenance Release Dates\\'\\n   http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006309 [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006309]\\n   ---------------------------------------------------------------\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   IZ70611\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ HP V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5724H7202\\n   \\n   \\n * REPORTED RELEASE\\n   600\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2010-02-18\\n   \\n   \\n * CLOSED DATE\\n   2010-05-28\\n   \\n   \\n * LAST MODIFIED DATE\\n   2011-01-03\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PM15497\\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ HP V6\\n   \\n   \\n * FIXED COMPONENT ID\\n   5724H7202\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R600 PSY\\n   UP SUBSCRIBE TO THIS APAR\\nBy subscribing, you receive periodic emails alerting you to the status of the APAR, along with a link to the fix after it becomes available. You can track this item individually or track all items by product.\\n\\nNotify me when this APAR changes.\\n\\nNotify me when an APAR for this component changes.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  If a cluster queue is set to PUT(disabled) and messages\\n   are in transit the target queue manager can throw an FDC\\n   \"Incorrect application process opener context stored in\\n   MQopen for MQPUT\" or the cluster channel to this queue\\n   manager may be terminated.\\n   .\\n   FDC:\\n     Probe Id   :- QS220012\\n     Component  :- qslHandlePut\\n     Comment1   :- Incorrect application process opener\\n                   context stored in MQopen for MQPUT\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *   Channel termination and occasionally\\n   creation of FDC with probeID \\'QS220012\\' (Component\\n   qslHandlePut) happen when PUT(disabled) on a cluster\\n   queue is performed while messages are in transit. The\\n   target cluster queue and the SCTQ of the target QMGR\\n   have to reside on different queue servers. No DLQ has\\n   to be configured on the target QMGR. There is another\\n   cluster queue like the target queue in the cluster.\\n   The likelihood of FDC creation increases when the\\n   queue servers on target QMGR manage many queue opens.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *   Problem has been solved. After the cluster\\n   queue is PUT(disabled) the messages are rerouted to the other\\n   cluster queue without channel termination.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n *   Assign the cluster queue and the SCTQ of the\\n   target QMGR to the same queue server.\\n   \\n   \\n    \\n   \\n   \\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   IT24999\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WEBS MQ NSS ITA\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5724A3902\\n   \\n   \\n * REPORTED RELEASE\\n   531\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2018-05-10\\n   \\n   \\n * CLOSED DATE\\n   2018-05-16\\n   \\n   \\n * LAST MODIFIED DATE\\n   2018-05-16\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WEBS MQ NSS ITA\\n   \\n   \\n * FIXED COMPONENT ID\\n   5724A3902\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELSqueue full; CLUSSDR; amqcrsta; MRTRY; MRTMR 2053 0x00000805 MQRC_Q_FULL TECHNOTE (TROUBLESHOOTING)\\n\\nPROBLEM(ABSTRACT)\\n Several applications running on different WebSphere MQ (WMQ) queue managers are putting messages to the same queue that is shared in the cluster. A problem occurs when the target cluster queue fills. The messages are routed to the dead letter queue (DLQ). Then the DLQ also fills. The resulting MCA failure causes the cluster sender channels to go to RETRY processing. The retry processing causes redundant amqcrsta processes to be spawned and this results in a serious machine overload. \\n\\n2053 0x00000805 MQRC_Q_FULL \\n\\nCAUSE\\nA message is sent over a channel, and arrives at the remote end. The amqcrsta (listener process) attempts to deliver the message to the destination queue. If the queue is full, it tries MRTRY (message retry) times to put the message waiting MRTMR (message retry timer) milliseconds between each attempt. If this does not work (because the queue is full on each attempted put) it closes the destination queue, opens the QMGR object and inquires the QMGR object to check for a dead letter queue. It then closes the QMGR object. If a dead letter queue exists the amqcrsta process then repeats the process of attempting to put the message a number of times. During this time the channel is in \\'PAUSED\\' status and the remote end of the channel (that started the conversation) is in RUNNING status. If the put failed (because the dead letter queue is also full) the amqcrsta process sends a message to the conversation initiator to say that the put failed and the channel should be retried. The amqcrsta then ends. \\nA number of factors caused this problem which resulted in a serious machine overload. The trace supplied shows that amqcrsta processes are ending, however they cannot end fast enough. As the machine\\'s CPUs are working so hard, serving thousands of processes, amqcrsta processes do not stop in time before the RETRY interval on the conversation initiator side has started another channel and therefore another amqcrsta. Now there are even more processes to give a slice of CPU time to. This situation gets exponentially worse with more and more processes using more and more CPU time which causes the problem. \\n\\nThere are a number of architectural changes that can be made to the system to prevent this situation from becoming the serious problem, which it has been seen to be.\\n\\n\\nRESOLVING THE PROBLEM\\n The best course of action is to prevent the problem from happening. \\n\\n * Determine why the messages are not being removed from the queue \\n * Increase the MAXDEPTH(integer) for the cluster queue and dead letter queue\\n\\n\\n WebSphere MQ is capable of holding many messages on deep queues, if required. Although it is not a good design to rely on queues being deep on a regular basis, it is useful for these exceptional circumstances. I would suggest the queue depth is increased to cope with deeper queues in case of a similar failure in the future. Another equally important thing to do is investigate why the getting application was not performing MQGETs. If there is a problem with the application this needs to be addressed. \\n\\nAlthough prevention of the problem would be the ideal option it is obviously not always possible to guarantee this so MQ can be tuned to cope better with this type of problem in the future... \\n\\n \\n\\nTuning recommendations \\n\\n1. Reduce MRTRY and MRTMR \\n\\nAs described above when a channel attempts to put a message to a target queue it tries this a number of times waiting a specified time interval between each attempted MQPUT. Both of these values are configurable. If the length of time we wait between each put and the number of times we attempt the put is reduced the channel program (amqcrsta) will give up and end sooner. This means that the opportunity for multiple amqcrsta processes for the same channel to be started simultaneously is decreased (because the amqcrsta\\'s have a better chance of ending before another one is started). Under normal circumstance (the target queue is not full) decreasing this value will have no effect - the put to the target queue will succeed on the first attempt. Altering this value would only have an affect when the target queue is full. \\n\\n2. Increase SHORTTMR and/or decrease SHORTRTY. \\n\\nThis will change the length of time the sender side waits before trying to start the conversation again. MQ has two types of retrying mode - the short retry and the long retry. First of all, we try short retries. This means MQ attempts to start the conversation at small intervals of time a few times over. By default, it tries to start the channel every 60 seconds 10 times over. If none of these 10 attempts were successful we switch to long retries where we attempt to restart the channel once every 20 minutes. If the length of time waited between \\'short\\' attempts was increased and possibly the number of \\'short\\' attempts were decreased it would give the amqcrsta processes more time to stop before another one is started up. Combined with recommendation 1 this could be configured to give each amqcrsta plenty of opportunity to stop which should prevent the QMGR from becoming over loaded in this way again. \\n\\n3. Use runmqlsr instead of inetd. \\n\\nUsing runmqlsr instead of inetd would have a benefit of running channels as threads within amqrmppa (the channel pooling process) in which we use a much more light weight approach. This would have less of an impact on system resources giving the amqcrsta processes more of a chance of stopping in a timely manner. \\n\\n \\n\\nHISTORICAL NUMBER\\n 15854 499 000 \\n\\nPRODUCT ALIAS/SYNONYM\\n wmq/mqz/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  Cics acts as webservice provider over MQ transport.\\n   A client put a message on the receive queue, that is triggered.\\n   Trigger monitor, then, writes on the INITQ and CPIL is attached\\n   because it is a SOAP message.\\n   CPIL  performs a MQGET on the receive queue. In the MQ message\\n   there is a replyDestination.\\n   CPIL then, starts CPIQ and it performs a MQPUT1 on the\\n   replyToQueue.\\n   But this queue is put-inhibited that cause to be returned:\\n   DFHPI0114 CPIQ The pipeline MQ transport mechanism failed\\n   because a call to WebSphere MQ function MQPUT1 returned with\\n   reason code 2051 MQRC_PUT_INHIBITED) x\\'803\\'\\n   .\\n   At this point CICS should write the message on Dead Letter\\n   Queue as described in the CICS manuals.\\n   .\\n   CPIQ issues a MQPUT1 on DLQ with PASS_ALL_CONTEXT but it fails\\n   with MQRC_CONTEXT_HANDLE_ERROR.\\n   No errors messages are sent for this error.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  n/a\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All CICS users.                              *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: CICS webservice request using WMQ       *\\n   *                      transport fails with 2097 on DLQ        *\\n   *                      when attempting to write to the DLQ.    *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   A CICS webservice request using WMQ transport fails because the\\n   specified reply-to queue is put disabled, (MQRC=2051) meaning\\n   the reply message cannot be written. The reply message should\\n   then be placed on the dead letter queue, but this also fails\\n   with a context handle error (MQRC 2097).\\n   \\n   Keyword(s): MQRC_CONTEXT_HANDLE_ERROR MQRC_PUT_INHIBITED\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  DFHPITQ1 has been altered so that the MQPUT1 which writes\\n   to the dead letter queue uses the mq_pmo.Option of\\n   MQPMO_SET_ALL_CONTEXT instead of MQPMO_PASS_ALL_CONTEXT.\\n   Additionally, the module is also changed so that whenever\\n   an MQPUT1 fails the failing message is written to the dead\\n   letter queue immediately.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n *  FIX AVAILABLE BY PTF ONLY\\n   \\n   \\n    \\n   \\n   \\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI42870\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   CICS TS Z/OS V4\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655S9700\\n   \\n   \\n * REPORTED RELEASE\\n   700\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2015-06-12\\n   \\n   \\n * CLOSED DATE\\n   2016-03-21\\n   \\n   \\n * LAST MODIFIED DATE\\n   2016-04-05\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PI48932 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI48932] PI56444 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI56444] UI36458\\n   \\n   \\n\\nMODULES/MACROS\\n *  DFHPITQ1\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   CICS TS Z/OS V4\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655S9700\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R700 PSY UI36458 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI36458]\\n   UP16/04/01 P F603\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES. SUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  ******* (Do NOT alter/erase this or next 3 lines) *******\\n   * EQUIVALENT ABSTRACT:\\n   MQM400 CSD09 CUMULATIVE SERVICE _MQSERIES C__ AND WEBSPHERE_\\n   *\\n   Collective service delivery for the set of MQ520 CSD09 PTFs\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  Collective service delivery for the set of MQ520 CSD09 PTFs\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  SA96028 [http://www-01.ibm.com/support/docview.wss?uid=swg1SA96028] - MQM400 CHGMQMQ NOT RECOGNIZING *SAME PARAMETER OPTION\\n             When using the CHGMQMQ command to change the Put\\n             Enabled, Triggering Enabled, Sharing Enabled or Get\\n             Enabled parm from *YES to *SAME, the parameter changes\\n             to *NO.\\n   SA96202 [http://www-01.ibm.com/support/docview.wss?uid=swg1SA96202] - MQM400 USER MANAGED JOURNAL RECEIVER DOES NOT SWITCH\\n             User Managed Journal Receiver: The RCDMQMIMG *ALL *ALL\\n             *YES command does not switch when a journal receiver\\n             size has exceeded its THRESHOLD value.  The receiver\\n             keeps on growing until it is MANUALLY detached.\\n   SA96224 [http://www-01.ibm.com/support/docview.wss?uid=swg1SA96224] - MQM400 RPG APPLICATION FAILS WITH ERROR CODE RC2023\\n             In RPG programs, MQINQ returns RC 2023\\n             (MQRC_INT_ATTRS_ARRAY_ERROR) when IACNT (IntAttrCount)\\n             is 0, or RC 2067 (MQRC_SELECTOR_ERROR) when SELCNT\\n             (SelectorCount) is 0.\\n   SA96244 [http://www-01.ibm.com/support/docview.wss?uid=swg1SA96244] - MQM400 CLUSQMGR FIELD OF CLUSTER QUEUE CREATED USING C\\n             CRTMQMQ IS INVALID WHEN MESSAGE QUEUE MANAGER NAME\\n             IS *DFT\\n             Clustering: The field \\'Cluster queue manager name\\n             (CLUSQMGR) in a queue definition is displayed\\n             incorrectly when using runmqsc or WRKMQMCLQ list if\\n             the parameter MQMNAME is set to *DFT when the queue\\n             was created by CRTMQMQ or CPYMQMQ or modified by\\n             CHGMQMQ.\\n   SA96247 [http://www-01.ibm.com/support/docview.wss?uid=swg1SA96247] - MQM400-F/AMQXSSMS_N-MSGMCH3601 ANY MQ COMMAND HANGS\\n             AND FDC IS THROWN FOR PROBEID XY353001 COMPONENT\\n             XSTCONNSHAREDMEMSET.\\n             The problem MCH3601 is most likely caused by the\\n             MQSeries SharedMemSet control block getting corrupted\\n             by another process. This APAR is shipping a partial\\n             fix which avoids the evaluate pointer exception, and\\n             logs a new FDC report with probe 1 in function\\n             xstConnSharedMemSet, which includes a dump of the\\n             corrupted data area.\\n   SA96249 [http://www-01.ibm.com/support/docview.wss?uid=swg1SA96249] - MQM400 MCH0601 (STATEMENT 116) FOR DECODEANDRUNCOMMAND\\n             RUNNING AN MQSC COMMAND WHICH IS NOT BEING PROPERLY\\n             NULL TERMINATED.\\n             Running an MQSC command which is not being properly\\n             null terminated fails.  The job log of the application\\n             failing with MCH0601 shows that we are failing at\\n             statement 116 in procedure DecodeAndRunCommand for\\n             program AMQOCPPX in module AMQOCPPX_N.\\n   SA96280 [http://www-01.ibm.com/support/docview.wss?uid=swg1SA96280] - MQM400 - STRMQMMQSC DISPLAY QLOCAL(*) FAILS MSGMCH6902\\n             Issuing the MQSC command DISPLAY QLOCAL(*) fails with\\n             a MCH6902 and C2M1212 for the STRMQMMQSC command.\\n   SA96283 [http://www-01.ibm.com/support/docview.wss?uid=swg1SA96283] - CHGMQMQ COMMAND DOES NOT WORK AFTER CSD08 IS APPLIED\\n             After applying CSD08, the CHGMQMQ command does not\\n             work. When a change is attempted, the AMQ8008 message\\n             is displayed stating that the change has occurred, but\\n             a display of the queue shows the attributes have not\\n             been changed. This problem is not reported when using\\n             the ALTER MQSC command to change the queue attributes.\\n   .\\n   IY48070 [http://www-01.ibm.com/support/docview.wss?uid=swg1IY48070] - MQRC_NO_MSG_AVAILABLE 2033 WITH MESSAGES AVAILABLE ON\\n             THE QUEUE\\n             Customer writes groups of messages to a queue.  Later,\\n             if they try to get a message from this queue, they\\n             will receive reason code 2033 (0x000007f1 NO MSG\\n             AVAILABLE MQRC_NO_MSG_AVAILABLE).  When this problem\\n             occurs, if the customer restarts their queue manager,\\n             then the same application is able to get the message\\n             successfully.\\n   IY48319 - MQPUT1 ERROR 2141 DLH_ERROR FROM COMMAND SERVER.\\n             If the reply message from the command server cannot be\\n             put to the reply queue passed in the PCF command, the\\n             command server fails to put the reply message to the\\n             DLQ with an MQPUT1 failure of 2141 MQRC_DLH_ERROR.\\n   IY49278 - MQGET RC=MQRC_TRUNCATED_MSG_FAILED WHEN THE BUFFER IS\\n             BIG ENOUGH\\n             When an application puts multiple messages to a queue\\n             and they are segmented by the queue manager, it is\\n             possible that the program which gets the messages may\\n             receive a reason code of MQRC_TRUNCATED_MSG_FAILED if\\n             its buffer is not large enough to contain the largest\\n             messages on the queue.\\n   IY49438 - MQBEGIN FAILS WITH RC= 2128 ( MQRC_UOW_IN_PROGRESS ).\\n             Two applications connect to the same Qmgr and they\\n             execute 2 PC transaction at the same time. If one of\\n             them executes transaction (not 2PC transaction)\\n             without MQBEGIN after next MQBEGIN of the other\\n             application fails with 2128.\\n   IY49686 [http://www-01.ibm.com/support/docview.wss?uid=swg1IY49686] - MULTI-THREADED C++ APP STARTS SEVERAL THREADS, A NEW\\n             QMGR OBJECT CAN ONLY BE CREATED ONCE THE GET OF THE\\n             IMQQUEUE HAS FINISHED.\\n             If a multithreaded C++ client application attempts to\\n             connect to a Queue Manager which is unreachable the\\n             MQCONN waits while the client attempts to connect to\\n             the Queue Manager.  During this time all other MQI\\n             calls are unable to complete and hang as they are\\n             waiting on the MQCONN to return.  This meant\\n             potentially other threads would also hang until the\\n             MQCONN returned.\\n   IY49687 - UNINITIALIZED FIELDS IN AMQ7217.\\n             Message for error AMQ7217 has blank fields.\\n   IY50188 - CLUSSDR CHANNEL FAILS TO START WHEN MESSAGES ARE PUT\\n             UNDER SYNCPOINT.\\n             Clustering: sender channels fail to start\\n             automatically when messages arrive on the\\n             SYSTEM.CLUSTER.TRANSMIT.QUEUE. These channels remain\\n             in INACTIVE state after disconnecting normally due to\\n             reaching DISCINT.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   SA96306\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   MQSERIES FOR AS\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5733A3800\\n   \\n   \\n * REPORTED RELEASE\\n   520\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2004-01-21\\n   \\n   \\n * CLOSED DATE\\n   2004-01-21\\n   \\n   \\n * LAST MODIFIED DATE\\n   2004-11-10\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    SF67770\\n   \\n   \\n\\nMODULES/MACROS\\n *  AMQWASCX AMQWASOA IMQDPUTN IMQDPUTR IMQSGETN\\n   IMQSGETR IMQSPUTN IMQSPUTR IMQWRLDN IMQWRLDR\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   MQSERIES FOR AS\\n   \\n   \\n * FIXED COMPONENT ID\\n   5733A3800\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R520 PSY SF67795\\n   UP04/11/10 I 1000wste_webcast; WSTE; Support Technical Exchange; education; presentation; webcast; STEwebcastDocument; Failover; HA Clustering; multi-instance WEBCAST\\n\\nABSTRACT\\n This Ask the Experts session addresses questions concerning failover options and features available for WebSphere MQ on distributed platforms. This includes high availability clustering as well as the new multi-instance queue manager feature included with WebSphere MQ version 7.0.1. \\n\\nCONTENT\\nAsk the Experts sessions are conducted in a question and answer format. The table below provides a time index (minutes:seconds) to the recording and describes the questions or topics discussed. You can fast forward to any question using the time index, a table containing the panel of experts is also included.\\n\\n\\nTo play or download the audio of this Ask the Experts session, see the Audio Section of this document.\\n\\nSee the Related Information Section of this document for a list of documents referenced during the presentation.\\n\\n\\n\\n\\n\\n\\n\\n\\nAsk the Experts session \\n30 March 2011 - 11:00 a.m. - 12:00 p.m. EDT \\n\\nTime Questions asked 00:00 Silence 00:00 General introduction 03:47 Technical introduction 04:23 What failover options are available for WebSphere MQ? 05:40 How should I choose between multi-instance queue managers and HA clusters? 08:02 Why was MC91 withdrawn and what replaces it? 10:24 What should I consider when setting up a multi-instance queue manager within a cluster in a mixed MQ version 6 & 7 environment? 14:57 How do you migrate or update MQ in a multi-instance or HA environment? 16:23 If we are looking for to share/run WMQ (HUB) on multiple platforms using the new capabilities added in MQV7 (i.e. MultiInstances), is it necessary to recommend/use an external NFS storage device, in order to meet the customer expectations in terms of HA? And if yes, how do we integrate them? 18:38 What needs to be changed within Windows registry to allow for the proper startup of WMQ Services on a “backup server” when manually failing over outside the control of something like MSCS? Given a configuration of two servers (A & B) both having WMQ installed and having used the hamvmqm utility to move the qmgr to a “common” drive that is only mapped on the “active” server - what needs to happen on server B within the registry to ensure that WMQ services are start up automatically? 21:30 We are running SUSE 10.3 on a distributed platform. Is SUSE 11.x required to support multi-instance? Also, is it correct to assume that it doesn\\'t work with NSF3 due to the auto-locking capabilities enabled by NFS4? 23:11 I have an iSeries, MQ version 7, multi-instance queue manager wishing to join cluster, where the full repository is mainframe, MQ version 6. The current security policy prevents the use of default port (1414). Our conclusion at the moment is that this isn\\'t possible to configure (making use of the auto switching). Our current plan is to set the cluster receiver channel CONNAME to be the DNS(port) of the system where the queue manager is running. E.g. if the iASP is active on PROD, then I will set the CONNAME of the cluster receiver to \"prod.hsbc.com(99999)\". Then, when the iASP is varied onto the CONT iSeries, we will set the CONNAME to \"cont.hsbc.com(99999)\", & perform a \"refresh cluster\" (we have found that the refresh is required, please advise if you think that this is not the case?). Can you please provide some inputs on the scenario above? 26:33 Is using multiple IP addresses in CONNAME to connect to remote queue manager supported when they aren\\'t configured as multi-instance queue managers? 27:38 Does CONNAME support multiple addresses in WebSphere MQ for z/OS? 28:35 What kind of events causes a client connection switch from an active IP address to another? 29:59 I am running V6 queue managers on HP-UX. HA configuration was done using the MC91 support pack. We use the Veritas (VCS) agent that is provided in MC91. I understand that MC91 is fully embedded in MQ V7 and that it has much more than MC91, but how do I migrate my existing V6 queue managers to V7? Also, optionally, is it possible to continue using the existing configuration of MC91 but still upgrade to V7? 31:08 In the scenario where I create brand new queue managers in V7 – let us say I create them as multi-instance queue managers using the new flags as documented. My data disk is different than /var. I understand MQ will monitor and offline the queue managers as necessary. In case of failover, how will MQ bring the virtual IP and disk online on the second node? Will it have enough permissions to do that? Or is it necessary to give root authority to the MQM id? 32:51 We are currently migrating from V6 to V7 and one of the issues that came up was relative addressing for XSLT and schemas. We had some misunderstanding about the use of shared storage and working storage. We are concerned about where WebSphere Message Broker in a multi-instance environment will look for the XSLT? 35:50 I have encountered a couple of situations with customers where we have 2 queue managers communicating over standard MCH channels - sender and receive pairs. We are trying to establish a HA capacity between them (this is between one company and an external partner). Is there a way to configure the channels and the transmit queue to allow one queue manager to automatically switch to an alternate queue manager at a different geographic location to continue the message flow if say the partner QM fails? 38:28 I have a question related to slide 35, does the client connection switch only work for both MQ managers, or can the client using a channel definition table, where there are multiple available candidates for connecting to, connect the failing one where it is connected to and switch it to another one on the list from the CCDT. 39:50 In relation to WebSphere Message Broker, if we are using multi-instance and We have services on Oracle databases now, do we lose our resources when we failover to the JDBC resources? 40:54 We are currently running WebSphere MQ V6 on windows. Recently we tried to do a recovery test where we rebuilt the MQ from scratch and re-created it again. We were able to recover all the queue manager objects via script, but one thing that we went into was that in Windows, we discovered that there are some configuration settings which are embedded in the registry itself, such as maxchannel. We have done some research and found that we cannot modify the maxchannel setting via script. My question is are there any tricks or registry settings that we should be concerned about when we perform recovery like this? 42:57 We are running in a Windows environment and I just wanted to know if there is any workaround to making multi-instance queue managers work if we cannot have domain controllers. 44:49 When we use multi-instance queue managers and when they failover from the active instance to the standby instance, how should I let the application know that the queue manager has is running on the standby instance? 47:23 How long does it take for the queue manager to failover from the primary instance to the standby instance, and what factors affect this time? 50:27 Is it possible to use queue managers in an active-active scenario instead of an an active-standby scenario? 53:01 We currently have a single system running on AIX that hosts both the MQ manager and the application together. We want to separate the MQ manager by moving it to a new Linux system and keep the application on the AIX system. What is your recommended way to go about doing this if the goal is to maximize performance? 55:47 I set my Message Broker up to run as a service on MQ, then run it as an MQM user. If I want to run it as an MQSI user, is there any documentation on how to modify the start/stop script to do that? 58:15 I have a question on a multi-instance set up. Let\\'s say we have two queue managers configured as multi-instance, one on machine #1, and the other on machine #2. If we don\\'t want these machines to stay idle all the time, how do we implement a start-up/shutdown script to automatically bring the idle up or to shut it down? 59:52 We are planning to upgrade our MQ from V6 to V7. We are using IBM MC91 agents and are planning to replace them. Are there any special considerations we should be aware of? 62:29 Could you please repeat the options of choosing between when to use HA and Multi-instance? 63:56 Closing remarks 64:27 End of Call \\nPanel of Experts: Barry Robbins Team Lead, WebSphere MQ Distributed Support Snezhana Johnson WebSphere MQ Distributed Support Tameka Woody WebSphere MQ System i - Windows Support Andrew Schofield Senior Software Engineer, WebSphere MQ Jason Edmeades Service Architect, WebSphere MQ Jonathan Rumsey Lead System i Developer, WebSphere MQ \\n\\nPRESENTATION\\n[http://www.ibm.com/i/v16/icons/weblecture.gif]WSTE-03302011-AskTheExperts-WebSphereMQFailoverDistributedPlatforms-Robbins.pdf [/support/docview.wss?uid=swg27020934&aid=2] WSTE-03302011-AskTheExperts-WebSphereMQFailoverDistributedPlatforms-Robbins.pdf [/support/docview.wss?uid=swg27020934&aid=1]\\n\\nGet Adobe Reader [http://www.adobe.com/products/acrobat/readstep2.html] to view PDF\\n\\nAUDIO\\n[http://www.ibm.com/i/v16/icons/aud.gif]To replay the recording of this 64 minutes conference call, click on Download Audio [http://public.dhe.ibm.com/software/websphere/techexchange/Mar-30-2011-Robbins.mp3] (7.0MB - audio in MP3 format). Right-click and select Save As to store the file on your local computer for later playback. Remember that you can fast forward to any question using the time index.\\n\\nRELATED INFORMATION\\n WebSphere Support Technical Exchange [http://www.ibm.com/software/websphere/webcast]\\n\\n\\n\\n\\nCross reference information Segment Product Component Platform Version Edition Business Integration WebSphere MQ Recovery PRODUCT ALIAS/SYNONYM\\n WMQ WebSphere MQ TECHNOTE (FAQ)\\n\\nQUESTION\\n How to route MQ messages dynamically using MQOD structure? \\n\\nANSWER\\nIn order to dynamically route response message to remote queue manager, one can inject MQOD structure by using custom stylesheet in DataPower. \\n\\nIt should be noted that the local queue manager should be configured to route message to remote queue manager using sender channel and the remote queue manager should have the respective receiver channel to get the message. \\n\\nRefer to the following link that describes how two queue managers communicate with each other in a distributed environment: http://publib.boulder.ibm.com/infocenter/wmqfte/v7r0/index.jsp?topic=%2Fcom.ibm.wmqfte.doc%2Fconfiguring_v6_qms.htm [http://publib.boulder.ibm.com/infocenter/wmqfte/v7r0/index.jsp?topic=%2Fcom.ibm.wmqfte.doc%2Fconfiguring_v6_qms.htm]. \\n\\nFor information about WebSphere MQ queue manager clusters and how to configure them, see the link: http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/index.jsp?topic=%2Fcom.ibm.mq.csqzah.doc%2Fqc10220_.htm [http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/index.jsp?topic=%2Fcom.ibm.mq.csqzah.doc%2Fqc10220_.htm]. \\n\\n \\n\\n(1) Save the MQMD.ReplyToQ and MQMD.ReplyQMgr header values in the request flow:\\n<xsl:variable name=\"rule-type\" select=\"dp:variable(\\'var://service/transaction-rule-type\\')\"/>\\n<xsl:choose>\\n<!-- Request Rule only -->\\n<xsl:when test=\"$rule-type = \\'request\\'\">\\n<xsl:variable name=\"entries\" select=\"dp:request-header(\\'MQMD\\')\"/>\\n<xsl:variable name=\"header\" select=\"dp:parse($entries)\"/>\\n<xsl:variable name=\"RQ\" select=\"$header//ReplyToQ\"/>\\n<xsl:variable name=\"RQMgr\" select=\"$header//ReplyToQMgr\"/>\\n<dp:set-variable name=\"\\'var://context/MYMQMD/ReplyToQ\\'\" value=\"$RQ\"/>\\n<dp:set-variable name=\"\\'var://context/MYMQMD/ReplyToQMgr\\'\" value=\"$RQMgr\"/>\\n<xsl:message dp:priority=\"debug\">\\n<xsl:value-of select=\"concat (\\'Request MQMD : \\', dp:request-header(\\'MQMD\\'))\"/>\\n</xsl:message>\\n</xsl:when> \\n\\n(2) Populate the MQOD headers in the response flow:\\n<!-- Response rule only -->\\n<xsl:when test=\"$rule-type = \\'response\\'\">\\n<xsl:variable name=\"custMQODStr\">\\n<MQOD>\\n<Version>2</Version>\\n<ObjectName>\\n<xsl:value-of select=\"dp:variable(\\'var://context/MYMQMD/ReplyToQ\\')\"/>\\n</ObjectName>\\n<ObjectQMgrName>\\n<xsl:value-of select=\"dp:variable(\\'var://context/MYMQMD/ReplyToQMgr\\')\"/>\\n</ObjectQMgrName>\\n</MQOD>\\n</xsl:variable>\\n<xsl:variable name=\"mqodStr\">\\n<dp:serialize select=\"$custMQODStr\" omit-xml-decl=\"yes\"/>\\n</xsl:variable>\\n<xsl:message dp:priority=\"debug\">\\n<xsl:value-of select=\"concat(\\'Response MQOD : \\', $mqodStr)\"/>\\n</xsl:message>\\n\\n(3) Inject the MQOD structure in the response flow:\\n\\n<dp:set-response-header name=\"\\'MQOD\\'\" value=\"$mqodStr\"/>\\n</xsl:when>\\n</xsl:choos \\n\\n(4) Send the message using the mq-qm object that uses the local queue manager \\n\\n(5) For cluster environment, the MQOD structure should not contain the \"ObjectQMgrName\" element as MQ cluster environment can resolve queue manager names. \\n\\nFor MQOD headers, refer to the following link: http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/topic/com.ibm.mq.csqzak.doc/fr13630_.htm [http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/topic/com.ibm.mq.csqzak.doc/fr13630_.htm]cluster cluster cluster Cluster clustering 2085 2189 MQRC_CLUSTER_RESOLUTION_ERROR Workload balancing round robin queue DEFBIND(*NOTFIXED) default bind not fixed MQOO_BIND_NOT_FIXED repository common problems Cluster clustering 2085 2189 MQRC_CLUSTER_RESOLUTION_ERROR Workload balancing round robin queue DEFBIND(*NOTFIXED) default bind not fixed MQOO_BIND_NOT_FIXED repository common problems Cluster clustering 2085 2189 MQRC_CLUSTER_RESOLUTION_ERROR Workload balancing round robin queue DEFBIND(*NOTFIXED) default bind not fixed MQOO_BIND_NOT_FIXED repository common problems Cluster clustering 2085 2189 MQRC_CLUSTER_RESOLUTION_ERROR Workload balancing round robin queue PremiumSupportTips TECHNOTE (FAQ)\\n\\nQUESTION\\n This is a compilation of the most common problems and questions regarding WebSphere MQ Clustering and it is intended for new MQ Cluster users. \\n\\nCAUSE\\n1. You are new to WebSphere MQ Clustering\\n\\n2. You have questions regarding WebSphere MQ Clustering\\n\\n3. You are experiencing a WebSphere MQ Cluster problem\\n\\nANSWER\\nThis article is intended for new MQ Cluster users and contains the following sections: \\n\\n [/support/docview.wss?uid=swg21229905&amp;aid=1] Clustering hints and tips \\n[/support/docview.wss?uid=swg21229905&amp;aid=1] Workload balancing - round robin processing\\n[/support/docview.wss?uid=swg21229905&amp;aid=1] Collecting documentation\\n[/support/docview.wss?uid=swg21229905&amp;aid=1] Learning more about this problem / component\\n\\n\\nClustering hints and tips \\n\\n * Review the cluster manual. [http://publibfp.boulder.ibm.com/epubs/pdf/csqzah07.pdf]This book describes how to create and use clusters. It explains the concepts, terminology, and benefits of clustering. It shows a number of examples that you can perform to set up and maintain clusters of queue managers. \\n * Stay current on MQ maintenance. \\n * Remember to include the cluster parameter on all of the cluster object definitions (queues, channels, queue manager, and so on). \\n * Verify that your cluster channels connecting to the repository queue manager go to running status when the queue manager starts. Access to the repository queue manager is the key to clustering. \\n * Getting messages from remote cluster queues is not supported within the clustering component. \\n   Back to top\\n   \\n   \\n\\n\\n\\n\\nWorkload balancing - round robin processing \\n\\nAn MQ cluster can have more than one definition for the same queue, and can therefore benefit from increased availability and workload balancing in your network. If you are going to use round robin processing to several instances of the same queue on different queue managers, then consider the following:  * Your cluster queue should specify the parameter DEFBIND(*NOTFIXED) and the queue should be opened with the open option MQOO_BIND_NOT_FIXED. \\n * If you have a local instance of the cluster queue, then all messages are routed to that local instance of the cluster queue. You can get around this by disabling the queue for puts in WebSphere MQ V6.0 by using the CLWLUSEQ object attribute. \\n * You will get a fairly even distribution of messages to a given cluster queue only when all relevant cluster channels are running or inactive, and all instances of that cluster queue is enabled for puts. \\n * When you have clusters containing more than one instance of the same queue, WebSphere MQ uses a workload management algorithm to determine the best queue manager to route a message to. The workload management algorithm selects the local queue manager whenever possible, unless in WebSphere MQ V6.0 you modify the CLWLUSEQ attribute. If the workload algorithm routes messages to remote queue manager, then it chooses destinations based on the state of the channel (including any priority you might have assigned to the channel) and the availability of the queue manager and queue. The algorithm uses a round-robin approach to finalize its choice between the suitable queue managers. \\n\\n\\nBack to top\\n\\n\\n Collecting documentation\\n\\n\\nDocumentation required by the support team. \\n\\n 1. Go to the MustGather: Read First document. [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg21229861] \\n 2. Select the operating system where the problem occurs. \\n 3. Select the clustering component\\n    \\n    Back to top\\n    \\n    \\n\\n\\n\\nLearning more about this problem / component \\n\\nHow long does cluster information remain in the cluster repository? [http://publibfp.boulder.ibm.com/epubs/html/csqzah06/csqzah0618.htm#Header_129] \\n\\nYouTube MQonTV - WebSphere MQ Clustering [http://www.youtube.com/watch?v=BXsfKIWMTK8] \\n\\nWebSphere MQ - Graphical Clustering Demo [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg24006438] \\n\\nMQSeries - Design considerations for large Clusters [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg24006367] \\n\\nWebSphere MQ Information Center [http://www.ibm.com/software/integration/wmq/library/] [http://www-1.ibm.com/support/docview.wss?rs=171&context=SSFKSJ&dc=DB520&uid=swg21229905] \\n\\nBack to top\\n\\nRELATED INFORMATION\\n#WebSphere MQ Library [http://www.ibm.com/software/integration/wmq/library/]\\nWebSphere MQ Queue Manager Clusters Manual [http://publibfp.boulder.ibm.com/epubs/pdf/csqzah07.pdf]\\n\\n\\n \\n\\nPRODUCT ALIAS/SYNONYM\\n WMQ MQz/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  As stated in technote 1225022, there can be a delay of up to 10\\n   seconds while a partial repository waits for replies to\\n   subscriptions made to two full repositories.  If there is no\\n   reply, 2085 MQRC_UNKNOWN_OBJECT_NAME or 2189\\n   MQRC_CLUSTER_RESOLUTION_ERROR will occur.\\n   .\\n   The circumstances for this 10-second delay differ between z/OS\\n   and distributed queue managers. The functionality should be the\\n   same across the platforms.\\n   *\\n   An example scenario is:\\n   A test on WebSphere MQ for Windows (level 6.0.2.3) where\\n     QMGR1 = full    for CLUSA\\n     QMGR2 = partial for CLUSA\\n     QMGR3 = partial for CLUSA\\n   If all the cluster channels were active, QMGR1 got an immediate\\n   2085, and  QMGR2/QMGR3 got 2085 within a second or two.  If all\\n   the cluster channels were stopped, QMGR3 still got an immediate\\n   2085, and QMGR2/QMGR3 got the ten-second delay.\\n   .\\n   With\\n     QMGR1 = partial for CLUSB (still full for CLUSA)\\n     QMGR4 = full    for CLUSB\\n     QMGR5 = full    for CLUSB\\n   attempts to put to a non-existent queue from QMGR1 either got an\\n   immediate 2085 or fairly quick one.  There was not a 10-second\\n   delay.\\n   .\\n   On WebSphere MQ for z/OS:\\n     RTPL  = full    for CLUSC\\n     QMGR4 = partial for CLUSC (still full for CLUSB)\\n     QMGR5 = partial for CLUSC (still full for CLUSB)\\n   A put from the full repository RTPL got an immediate 2085.\\n   .\\n   For:\\n     RTPJ = partial for CLUSA\\n   even with all cluster channels active, there was a 10-second\\n   delay,\\n   which is different from the observation on Windows.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS version 6 *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: In a clustering environment the first   *\\n   *                      OPEN of a queue that DOES NOT EXIST     *\\n   *                      locally or in the cluster on a QMGR     *\\n   *                      that is a PARTIAL repository ALWAYS     *\\n   *                      takes a minimum of 10 seconds.          *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   The code in CSQMZLOO delays an OPEN for 10 seconds waiting\\n   for positive response to its inquiry from a full repository to\\n   be returned even if it has had a negative response.\\n   This causes application delays that are not acceptable for\\n   online systems (e.g CICS).\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  MQ z/OS Module CSQMZLOO has been changed to work the same way as\\n   the MQ Distributed code, and allow the OPEN to proceed on\\n   receipt of the first response from one of the full repositories.\\n   000Y\\n   CSQMZLOO\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK66962\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2008-05-30\\n   \\n   \\n * CLOSED DATE\\n   2008-11-21\\n   \\n   \\n * LAST MODIFIED DATE\\n   2009-01-02\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK41820 PK76183 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK76183]\\n   \\n   \\n\\nMODULES/MACROS\\n *     CSQMZLOO\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655L8200\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UK41820 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK41820]\\n   UP08/12/13 P F812\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  The customer is using PUBSUB and they are receiving the\\n   following messages repeatedly:\\n   .\\n   CSQT882E CSQ1 CSQXFCTL Message written to the dead-letter queue,\\n   for reason 2052\\n   CSQT882E CSQ1 CSQXFCTL Message written to the dead-letter queue,\\n   for reason 2052\\n   CSQT882E CSQ1 CSQXFCTL Message written to the dead-letter queue,\\n   for reason 2052\\n   CSQT882E CSQ1 CSQXFCTL Message written to the dead-letter queue,\\n   for reason 2052\\n   CSQX053E CSQ1 CSQXFFST Error information recorded in CSQSNAP\\n   data set\\n   .\\n   The queues which are causing the reason 2052 are\\n   TEMPORARY_DYNAMIC queues which are already \"marked Logically\\n   Deleted\". The SNAPs are for fqxCheckAuthority - a check is made\\n   to see if a user is authorized to put to a queue.\\n   .\\n   The Change Team is able to successfully recreate the CSQSNAPs\\n   that occurred in the publisher applications. The CSQSNAPS only\\n   occur when fqxCheckAuthority gets returned MQRC 2052\\n   MQRC_Q_DELETED. If the MQRC 2052 is received elsewhere in the\\n   process of putting to the reply queue, no CSQSNAP is issued.\\n   This is inconsistent, and suggests that the CSQSNAPs should not\\n   be being issued in this circumstance. A secondary problem is the\\n   issuing of the CSQT882E error messages - the description of this\\n   message suggests it should only be issued periodically for a\\n   particular stream, but currently every reply message being\\n   written to the DLQ by the stream is causing the message to be\\n   issued.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 0 Modification 1.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: CSQT882E reports a message being        *\\n   *                      written to the dead letter queue for    *\\n   *                      every such message put by a queued      *\\n   *                      pubsub stream.                          *\\n   *                                                              *\\n   *                      The error may be accompanied by a       *\\n   *                      CSQSNAP in fqxCheckAuthority, reported  *\\n   *                      by CSQX053E.                            *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   A request message is put to the stream queue for a stream\\n   running in the queued pubsub daemon. A temporary dynamic queue\\n   is specified as the reply queue, and the report options request\\n   a reply message is generated.\\n   If the application closes the reply queue (causing it to be\\n   deleted) before the reply message is written, the reply message\\n   will be put to the dead letter queue.\\n   Each time a message is written to the dead letter queue,\\n   CSQT882E is issued, resulting in a flood of messages to the\\n   joblog.\\n   Additionally, if the reply queue is marked logically deleted\\n   because another job still has a handle, and the reply message\\n   is being written for a different user and/or queue than the\\n   last processed by the stream, fqxCheckAuthority will get\\n   MQRC_Q_DELETED (2052) returned by MQOPEN. In this scenario\\n   it incorrectly takes a CSQSNAP.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  When messages are put to the deadletter queue by a pubsub\\n   stream, CSQT882E will only be issued periodically as documented\\n   in the description of the message.\\n   The CSQSNAP will no longer be taken if fqxCheckAuthority gets\\n   MQRC_Q_DELETED when issuing MQOPEN for the reply queue.\\n   010Y\\n   CMQXFQXA\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PM27593\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   010\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2010-11-26\\n   \\n   \\n * CLOSED DATE\\n   2010-12-23\\n   \\n   \\n * LAST MODIFIED DATE\\n   2011-02-14\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK63465\\n   \\n   \\n\\nMODULES/MACROS\\n *  CMQXFQXA\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R010 PSY UK63465 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK63465]\\n   UP11/01/15 P F101\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  The problem is as follows:\\n   .\\n   Two queue managers (QMA and QMB) are members\\n   of a QSG and are also members of a cluster.\\n   The QSG has a shared queue (SQ1) which is defined\\n   as being in the cluster. This results in both\\n   queue managers advertising an instance of that\\n   queue to other members of the cluster.\\n   .\\n   SQ1 is then deleted. This should cause both\\n   queue managers to send an update to the cluster\\n   to notify other members that the queue manager\\n   no longer hosts an instance of that clustered\\n   queue. However, for shared queues this update\\n   does not happen (at least, not straight away).\\n   .\\n   The result of this is that the cluster cache\\n   on each qmgr has two records for the queue\\n   (one for each qmgr), but neither has an instance\\n   of the queue to put messages to.\\n   .\\n   When a message is put with a queue name\\n   SQ1 on QMA, it detects that there isn\\'t a local\\n   queue instance, so it uses the cluster cache to\\n   resolve the location of the queue name.\\n   As no local instance exists, it selects the only\\n   other entry for the queue (QMB) and puts the\\n   message to the SYSTEM.CLUSTER.TRANSMIT.QUEUE to\\n   be sent to QMB.\\n   .\\n   When the message is sent over the channel,\\n   QMB also detects that there is no local instance\\n   of the queue, so goes to the cluster cache and\\n   determines that QMA is the only available instance.\\n   .\\n   The message loops between the two qmgrs. This\\n   causes high CPU, and if the message is persistent\\n   then it also causes the high logging volume\\n   seen by the customer.\\n   .\\n   Additional Symptom(s) Search Keyword(s):\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  Restart the QMQRs. The cache did get updated after the queue\\n   managers were restarted.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 8 *\\n   *                 Release 0 Modification 0.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Deleting a shared cluster queue may     *\\n   *                      result in the cluster definitions for   *\\n   *                      the shared queue remaining in the       *\\n   *                      cluster after a successful shared queue *\\n   *                      delete.                                 *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   If multiple members of a QSG are also members of the same\\n   cluster, when a shared cluster queue is deleted, the cluster\\n   records for the queue may continue to exist in the cluster. This\\n   can result the cluster hosting records for queues which no\\n   longer are valid. If messages are put to one of these queues,\\n   cluster resolution will attempt to put the message to another\\n   QMGR in the cluster where the queue was previously hosted, which\\n   result in further cluster resolution and subsequent puts to\\n   other cluster QMGRs, which can result in infinite loop of\\n   cluster resolution and puts to other QMGRs. This is due to\\n   shared queue deletes not correctly broadcasting the delete of\\n   the cluster queue in this case.\\n   \\n   The looping between QMGRs can result in high CPU usage on all\\n   the QMGRs involved. If the message put was persistent, this will\\n   also result in high logging volumes. When this scenario is\\n   encountered, a cancel may be required to stop the QMGR.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  Shared queue delete broadcast for cluster queues has been\\n   corrected to ensure cluster records are correctly deleted when a\\n   delete shared queue command is issued.\\n   000Y\\n   CSQMUQLC\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n *  *********\\n   * HIPER *\\n   *********\\n   \\n   \\n    \\n   \\n   \\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI76942\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS 8\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655W9700\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   YesHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2017-02-21\\n   \\n   \\n * CLOSED DATE\\n   2017-04-13\\n   \\n   \\n * LAST MODIFIED DATE\\n   2017-09-16\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PI79259 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI79259] UI46399\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQMUQLC\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS 8\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655W9700\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UI46399 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI46399]\\n   UP17/06/06 P F706 ¢\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.AIX  FIXES ARE AVAILABLE\\nWebSphere MQ V5.3 and WebSphere MQ Express V5.3 - Fix Pack 12 (CSD12) [http://www-01.ibm.com/support/docview.wss?uid=swg24011423]\\nWebSphere MQ V5.3 & WebSphere MQ Express V5.3 - Fix Pack 10 (CSD10) [http://www-01.ibm.com/support/docview.wss?uid=swg24009688]\\nWebSphere MQ V5.3 for iSeries - Fix Pack 12 (CSD12) [http://www-01.ibm.com/support/docview.wss?uid=swg24011422]\\nWebSphere MQ V5.3 and WebSphere MQ Express V5.3 - Fix Pack 13 (CSD13) [http://www-01.ibm.com/support/docview.wss?uid=swg24014128]\\nWebSphere MQ V5.3 and WebSphere MQ Express V5.3 - Fix Pack 14 (CSD14) [http://www-01.ibm.com/support/docview.wss?uid=swg24017668]\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  A message on the cluster command queue can be read multiple\\n   times. The message is written to the command queue when a\\n   channel stops, and is used by the repository manager to drive\\n   the logic to reallocate messages to other queue managers in the\\n   cluster. If the reallocate fails such that the message read\\n   from the cluster transmission queue cannot be put to any queue\\n   in the cluster, for example because the queue no longer exists,\\n   or there is a 2189 (MQRC_CLUSTER_RESOLUTION_ERROR), the message\\n   is backed out to the cluster transmission queue again. This\\n   backout also backs out the message on the cluster command\\n   queue, and so the message is read again. The solution is to\\n   commit the message read from the command queue before reading\\n   any messages from the transmission queue.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   USERS AFFECTED:\\n   Any cluster user. The problem may occur if the queues in the\\n   cluster are deleted such that messages have no possible\\n   destination, and there is no dead letter queue defined on the\\n   queue manager.\\n   \\n   Platforms affected:\\n    All Distributed (iSeries, all Unix and Windows)\\n   ****************************************************************\\n   PROBLEM SUMMARY:\\n   If there is a problem when reallocating messages to other queue\\n   managers in the cluster after a cluster channel has ended such\\n   that there is no possible destination in the cluster, or the\\n   destination cannot be resolved, the command message is backed\\n   out to the command queue, and so is read again (and again...)\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  Commit the get of the command message before processing\\n   messages on the cluster transmission queue.\\n   \\n   The fix will be included in the following PTFs:\\n   \\n                      v5.3\\n   Platform           Fix Pack 10\\n   --------           --------------------\\n   Windows            U200226\\n   AIX                U800391\\n   HP-UX (PA-RISC)    U800392\\n   Solaris (SPARC)    U800393\\n   iSeries            SI17950\\n   Linux (x86)        U800394\\n   Linux (zSeries)    U800395\\n   Linux (Power)      Not applicable\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   IY66462\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WEB MQ FOR AIX\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5724B4101\\n   \\n   \\n * REPORTED RELEASE\\n   530\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2005-01-05\\n   \\n   \\n * CLOSED DATE\\n   2005-01-07\\n   \\n   \\n * LAST MODIFIED DATE\\n   2007-04-26\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n\\nFIX INFORMATION\\n\\nAPPLICABLE COMPONENT LEVELS\\n * R530 PSY\\n   UP SUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  On a subset repository queue manager in a cluster with a\\n   CLUSSDRA channel in running state :-\\n   Enter REFRESH CLUSTER.\\n   The refresh operation appears to complete successfully, but the\\n   running channel is marked \\'TBD\\'; which means that when the\\n   channel finally stops - it is DELETED.\\n   This APAR will be used to fix two additional problems :-\\n   * CSQX038E UNABLE TO PUT MESSAGE TO SYSTEM.CLUSTER.COMMAND.\\n   * ref IC26573\\n   * The channel initiator receives these messages:\\n   * CSQX410I CSQXREPO Repository manager started .\\n   * CSQX038E CSQXREPO Unable to put message to\\n   * SYSTEM.CLUSTER.COMMAND.QUEUE, MQCC=2 MQRC=2189.\\n   * CSQX448E CSQXREPO Repository manager stopping because of error\\n   * Restart in 600 seconds.\\n   *\\n   * This causes the queue to be disabled, which may result in\\n   * subsequent messages:\\n   * CSQX037E Unable to get msg from SYSTEM.CLUSTER.COMMAND.QUEUE\\n   * CC=2 RC=2016\\n   * CSQX038E Unable to Put msg to SYSTEM.CLUSTER.COMMAND.QUEUE\\n   * CC=2 RC=2269.\\n   * CSQX411I Repository Manager Stopped\\n   * MQCD DOES NOT CONTAIN THE QMGR\\'S NAME WHEN THE CHADEXIT\\n   * IS LOADED FOR A CLUSTER RECEIVER CHANNEL.\\n   * ref IC26863\\n   * We want to have an exit running on the receiving side of\\n   * a cluster sender-receiver pair.\\n   * To do so a chad exit was specified on the receiving QMGR,\\n   * to have it insert an exit function.\\n   * Yet the MQCD, passed to the chad exit, does not contain the\\n   * QMGR\\'s name when it is loaded for the CLUSRCVR channel.\\n   * (Note: for a CLUSSNDR, the QMGR\\'s name is loaded correctly in\\n   * the MQCD).\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  On a subset repository queue manager in a cluster with a\\n   CLUSSDRA channel in running state :-\\n   Enter REFRESH CLUSTER.\\n   The refresh operation appears to complete successfully, but the\\n   running channel is marked \\'TBD\\'; which means that when the\\n   channel finally stops - it is DELETED.\\n   This APAR will be used to fix two additional problems :-\\n   * CSQX038E UNABLE TO PUT MESSAGE TO SYSTEM.CLUSTER.COMMAND.\\n   * ref IC26573\\n   * The channel initiator receives these messages:\\n   * CSQX410I CSQXREPO Repository manager started .\\n   * CSQX038E CSQXREPO Unable to put message to\\n   * SYSTEM.CLUSTER.COMMAND.QUEUE, MQCC=2 MQRC=2189.\\n   * CSQX448E CSQXREPO Repository manager stopping because of error\\n   * Restart in 600 seconds.\\n   *\\n   * This causes the queue to be disabled, which may result in\\n   * subsequent messages:\\n   * CSQX037E Unable to get msg from SYSTEM.CLUSTER.COMMAND.QUEUE\\n   * CC=2 RC=2016\\n   * CSQX038E Unable to Put msg to SYSTEM.CLUSTER.COMMAND.QUEUE\\n   * CC=2 RC=2269.\\n   * CSQX411I Repository Manager Stopped\\n   * MQCD DOES NOT CONTAIN THE QMGR\\'S NAME WHEN THE CHADEXIT\\n   * IS LOADED FOR A CLUSTER RECEIVER CHANNEL.\\n   * ref IC26863\\n   * We want to have an exit running on the receiving side of\\n   * a cluster sender-receiver pair.\\n   * To do so a chad exit was specified on the receiving QMGR,\\n   * to have it insert an exit function.\\n   * Yet the MQCD, passed to the chad exit, does not contain the\\n   * QMGR\\'s name when it is loaded for the CLUSRCVR channel.\\n   * (Note: for a CLUSSNDR, the QMGR\\'s name is loaded correctly in\\n   * the MQCD).\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  The problems with using cluster channels and queues have been\\n   fixed.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   SA89637\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   MQSERIES FOR AS\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5733A3800\\n   \\n   \\n * REPORTED RELEASE\\n   510\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2000-06-20\\n   \\n   \\n * CLOSED DATE\\n   2000-06-20\\n   \\n   \\n * LAST MODIFIED DATE\\n   2003-06-24\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    SF63030\\n   \\n   \\n\\nMODULES/MACROS\\n *  AMQRRMFA LMQMR    LMQMRR\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   MQSERIES FOR AS\\n   \\n   \\n * FIXED COMPONENT ID\\n   5733A3800\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R510 PSY SF63030\\n   UP00/08/24 P 0294 SUBSCRIBE TO THIS APAR\\nBy subscribing, you receive periodic emails alerting you to the status of the APAR, along with a link to the fix after it becomes available. You can track this item individually or track all items by product.\\n\\nNotify me when this APAR changes.\\n\\nNotify me when an APAR for this component changes.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  After altering the queue manager\\'s CCSID, the following messages\\n   might be issued from the clustering component of the queue\\n   manager.  The queue manager will eventually terminate, unless it\\n   is ended manually.  However, restarting the queue manager\\n   resolves the issue permanently.\\n   \\n   The errors relate to character data within internal MQ command\\n   messages used by the clustering component of the queue manager.\\n   \\n   AMQ9510: Messages cannot be retrieved from a queue.\\n   EXPLANATION:\\n   The attempt to get messages from queue\\n   \\'SYSTEM.CLUSTER.COMMAND.QUEUE\\' on queue manager \\'QMNAME\\' failed\\n   with reason code 2150.\\n   \\n   AMQ6174: The dynamically loadable shared library\\n   \\'/var/mqm/exits64/MQADMIN_r\\' was not found.\\n   \\n   AMQ9448: Repository manager failed.  Retry in 10 minutes, queue\\n   manager will terminate in 7050 minutes.\\n   \\n   After the fix for IT15735, the sequence of errors is different,\\n   and updates to local cluster cache state are dropped,\\n   incorrectly.  If the queue manager has a dead letter queue, one\\n   or more messages will be put there, but there is no action the\\n   user can then take to handle those messages.\\n   \\n   Additionally it is possible this problem will be seen for\\n   certain types of internal MQ cluster state data arriving from\\n   remote cluster queue managers to this one.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  Correct the CCSID, if it was wrongly typed.\\n   Restart the queue manager so that the same CCSID is used\\n   throughout the queue manager, including the repository manager\\n   program.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   USERS AFFECTED:\\n   Users who alter their QMGR CCSID, or (they cannot control this)\\n   receive internal cluster state messages with unconvertible\\n   characters from remote systems.\\n   \\n   \\n   Platforms affected:\\n   MultiPlatform\\n   \\n   ****************************************************************\\n   PROBLEM DESCRIPTION:\\n   The errors relate to character data within internal MQ command\\n   messages used by the clustering component of the queue manager.\\n   \\n   The cluster repository manager program (amqrrmfa) gets these\\n   messages from the SYSTEM.CLUSTER.COMMAND.QUEUE using MQGET with\\n   MQGMO_CONVERT.\\n   \\n   Messages will be put containing character fields encoded with\\n   the new CCSID.\\n   \\n   However, by design amqrrmfa (as with all application programs\\n   that connected before the change in the CCSID) will have cached\\n   the previous value of the CCSID at the time it connected.\\n   Therefore MQGET tries to convert the messages.\\n   \\n   If there is a defined conversion from the new CCSID to the\\n   previous (cached) CCSID, this will be likely to succeed.  But\\n   even so, there are some byte sequences that will not convert\\n   successfully.\\n   \\n   If the user has used unconvertible byte sequences in fields such\\n   as queue or channel descriptions, then these will cause the\\n   conversion to fail with reason codes such as 2150\\n   MQRC_DBCS_ERROR.\\n   \\n   At this point the behaviour is different at different fix packs:\\n   At levels not including the fix for IT15735: message at the top\\n   of the queue became a poison message, and caused the queue\\n   manager to end after a prolonged period where it is retried\\n   without success.\\n   At levels including the fix for IT15735: the message at the top\\n   of the queue is moved to the DLQ, if one is defined.  The\\n   benefit is that the message is no longer a poison message.  But\\n   the downside is that that update was not made to the local\\n   cluster cache.  In the worst case this resulted in queues\\n   becoming expired and so invisible to applications.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  The MQ queue manager code has been updated with additional logic\\n   to handle the situation at the point when 2150 MQRC_DBCS_ERROR\\n   is returned from MQGET.\\n   \\n   The field(s) which might contain unconvertible characters will\\n   be blanked out and the conversion will be retried.  If the\\n   conversion succeeds, the code can proceed to store that update\\n   with a blank description.  Error messages AMQ6174 and AMQ9880\\n   will be written, to inform the administrator.\\n   \\n   If the message is still unconvertible then it will be moved to\\n   the Dead Letter Queue and an error message will be written.\\n   \\n   ---------------------------------------------------------------\\n   The fix is targeted for delivery in the following PTFs:\\n   \\n   Version    Maintenance Level\\n   v7.5       7.5.0.9\\n   v8.0       8.0.0.8\\n   v9.0 CD    9.0.4\\n   v9.0 LTS   9.0.0.2\\n   \\n   The latest available maintenance can be obtained from\\n   \\'WebSphere MQ Recommended Fixes\\'\\n   http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006037 [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006037]\\n   \\n   If the maintenance level is not yet available information on\\n   its planned availability can be found in \\'WebSphere MQ\\n   Planned Maintenance Release Dates\\'\\n   http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006309 [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006309]\\n   ---------------------------------------------------------------\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   IT19598\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ BASE MULTIP\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5724H7241\\n   \\n   \\n * REPORTED RELEASE\\n   750\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   YesHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2017-03-08\\n   \\n   \\n * CLOSED DATE\\n   2017-08-02\\n   \\n   \\n * LAST MODIFIED DATE\\n   2017-12-04\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PI82623\\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ BASE MULTIP\\n   \\n   \\n * FIXED COMPONENT ID\\n   5724H7241\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R750 PSY\\n   UPz/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  I have two queue managers, RM2D and RM4D. They belong to a queue\\n   sharing group QSG2.They also belong to a cluster named CLUSTER2.\\n   There is one queue defined on QM2D named REQUESTQ. REQUESTQ\\n   has been added to the CLUSTER2 and is therefore visible on queue\\n   manager QM4D. There is one queue defined on RM4D named REPLYQ.ue\\n    I have enabled intra-group queuing and specified IGQAUT as Q.\\n   ONLYIGQ; the IGQ user id is blank.\\n   \\n   If I connect to RM4D and put a (small) message to queue\\n   REQUESTQ, MQ  uses IGQ (rather than the cluster) to deliver the\\n   message from RM4D to RM2D (as it is less than 63K). This is as\\n   expected and works ok.\\n   reply-to QM of RM4D and a reply-to Q of REPLYQ, I get ich408i\\n   RACF error with the blank user id when MQ tries to generate the\\n   report message when the message is put to REQUESTQ .\\n   If I repeat the same test, but request an expired report message\\n   instead, MQ uses the MQMD user id as expected to generate the\\n   report message.\\n   If I repeat the same tests and change the IGQAUT from ONLYIGQ\\n   to DEF it works ok (it uses the RM4DMSTR address space user id)\\n   or to ALTIGQ it works ok (it uses the MQMD user id).\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of Websphere MQ for z/OS Version   *\\n   *                 6.                                           *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Security Violation message issued when  *\\n   *                      putting a message to a Cluster Queue    *\\n   *                      where the Confirm-On-Arrival report     *\\n   *                      option has been used.                   *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   Customer has two Queue Managers A & B, both in the same SHARING\\n   GROUP and CLUSTER. There exists a Cluster Queue hosted by Queue\\n   Manager A and visibile to Queue Manager B.\\n   .\\n   When Queue Manager B attempts to put a message to this Cluster\\n   Queue it determines that Intra-Group-Queuing can be used to\\n   transmit te message. This is due to the fact that both Source\\n   and Target Queue Managers are in the same Sharing Group.\\n   .\\n   When the message is put, one where the COA (Confirm On Arrival)\\n   report option has been specified, a RACF message is issued;\\n   .\\n   ICH408I USER(        ) GROUP(        ) NAME(???)\\n     LOGON/JOB INITIATION - USER AT TERMINAL          NOT RACF-D\\n   IRR012I  VERIFICATION FAILED. USER PROFILE NOT FOUND.\\n   .\\n   This only occurs when using the COA report option and the\\n   Intra-Group Queuing (IGQ) options have the authorization set to\\n   ONLYIGQ and the IGQ userid is BLANK.\\n   .\\n   The cause of this security violation was traced to the open\\n   processing performed by the Intra-Group-Queuing agent for the\\n   Reply-To-Queue in order to delivery the COA message.\\n   .\\n   CSQMIGQA, the Intra Group Queuing Agent, reads messages from the\\n   SYSTEM.QSG.TRANSMIT.QUEUE and puts the to their destination\\n   queue using a call to CSQMPUT. CSQMPUT detects the need for a\\n   report message and calls CSQMREPM handle the report message\\n   processing.\\n   .\\n   From the options passed CSQMREPM determines the type of report\\n   message required and builds the appropriate MQMD and message\\n   content. For a COA report message is assumes that the correct\\n   userid data has been setup by CSQMPUT. However this is not the\\n   case when messages are put by the Intra Group Queuing Agent. It\\n   used a special module CSQMSUID to analyse the userid options set\\n   for IGQ and to set userid data direct in control blocks used for\\n   the put. The result of this is that when security checks are\\n   conducted for the opening of the Reply-To-Queue the userids\\n   extract are blank which will led to the \"Blank-Userid\" security\\n   violation.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  CSQMREPM processing for a Confirm-On-Arrival report message\\n   request has been amended. The code will now establish the\\n   identify of the process it is running under. If it is the\\n   Intra-Group-Queuing Agent CSQMSUID is called to setup userids in\\n   accordance with the IGQ setting of \"Put authority\" and \"IGQ user\\n   ID\". This will ensure that when any security checks are\\n   conducted the correct userid information will be available.\\n   000Y\\n   CSQMREPM\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK17023\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WEBS MQ FOR ZOS\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2005-12-21\\n   \\n   \\n * CLOSED DATE\\n   2006-01-13\\n   \\n   \\n * LAST MODIFIED DATE\\n   2006-03-02\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PK16109 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK16109]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK10709\\n   \\n   \\n\\nMODULES/MACROS\\n *     CSQMREPM\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WEBS MQ FOR ZOS\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655L8200\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UK10709 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK10709]\\n   UP06/02/03 P F602\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES. TECHNOTE (TROUBLESHOOTING)\\n\\n问题（摘要）\\n 当您试图往WMQ集群队列发送消息时得到报错消息，MQ返回码：MQRC 2189（MQRC_CLUSTER_RESOLUTION_ERROR ）。当您在部分存储库队列管理器使用 DISPLAY QCLUSTER 命令时，会显示集群队列的不完整列表。 \\n\\n原因\\n您的完整存储库队列管理器手工定义了指向此部分存储库的队列管理器，MQ在这种情况下会认为手工定义的CLUSSDR通道指向的也是完整存储库队列管理器。这样，存储库的更新就会发往此队列管理器，而如果此队列管理器不是完整存储库队列管理器，你将在此队列管理器的日志看到下面错误信息： \\n\\nAMQ9430: \\'Unexpected cluster queue manager publication\\' received in the queue manager error log. \\n\\n。\\n\\n\\n\\n解决问题\\n解决方法如下 \\n\\n1. 通过执行下面命令把部分存储库队列管理器定义成完整存储库： \\n\\nALTER QMGR REPOS(<cluster name>) \\n\\n2. 参照 MQ信息中心Removing a queue manager from a cluster [http://pic.dhe.ibm.com/infocenter/wmqv7/v7r1/topic/com.ibm.mq.doc/zc15110_.htm] 一章，将队列管理器从集群中移除，把REPOS and REPOSNL 属性置为空。 \\n\\n3. 参照Adding a queue manager that hosts a queue locally [http://pic.dhe.ibm.com/infocenter/wmqv7/v7r1/topic/com.ibm.mq.doc/qc12240_.htm] 将此队列管理器再加回到集群。 SUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  A scheduled transfer with a triggering condition is created\\n   in WebSphere MQ File Transfer Edition (WMQFTE) v7.0.2.\\n   \\n   Whenever the scheduled transfer runs and the triggering\\n   condition is not met, two FTE reply messages are sent to\\n   WMQFTE.<some identifier> queue on the FTE queue manager.\\n   Messages should not be going to WMQFTE.<some identifier>\\n   since it does not exist.\\n   Since there is no such queue on the FTE queue manager,\\n   the messages are sent to the dead letter queue (DLQ).\\n   These messages continue to build up on the DLQ every\\n   time the condition is not met.\\n   Example of messages generated to the DLQ:\\n   <?xml version=\"1.0\" encoding=\"UTF-8\"?><reply version=\"3.00\"\\n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n   xsi:noNamespaceSchemaLocation=\"Reply.xsd\"\\n   ID=\"414d51205253544746544543202020204bcd28c220581972\">\\n   <status resultCode=\"42\"></status></reply>\\n   \\n   <?xml version=\"1.0\" encoding=\"UTF-8\"?><reply version=\"3.00\"\\n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n   xsi:noNamespaceSchemaLocation=\"Reply.xsd\"\\n   ID=\"414d51205253544746544543202020204bcd28c220581972\">\\n   <status resultCode=\"-2\"></status></reply>\\n   Note the status resultCodes in each message: \"42\" and \"-2\"\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  When a schedule is created on the command line a temporary reply\\n   queue is added to the schedule definition. This reply queue only\\n   lasts for the life of the command. The reply queue name is not\\n   removed from the Scedule definition, which will cause a reply to\\n   be sent to a queue that does not exist, and hence to the Queue\\n   Manager dead letter queue.\\n   \\n   \\n   USERS AFFECTED:\\n   Those creating scheduled transfers from the command line\\n   \\n   PLATFORMS AFFECTED:\\n   All supported platforms\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  The reply destination is now removed from the scedule definition\\n   at the time the scheduled transfer is run.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n *  An iFix is available on request\\n   \\n   \\n    \\n   \\n   \\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   IC68561\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ FILE TRANSF\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5724R1000\\n   \\n   \\n * REPORTED RELEASE\\n   700\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2010-05-11\\n   \\n   \\n * CLOSED DATE\\n   2010-05-21\\n   \\n   \\n * LAST MODIFIED DATE\\n   2010-05-21\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ FILE TRANSF\\n   \\n   \\n * FIXED COMPONENT ID\\n   5724R1000\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R702 PSY\\n   UP\\n   \\n   \\n * R700 PSN\\n   UP\\n   \\n   \\n * R701 PSN\\n   UPz/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  The QMGR attribute SQQMNAME controls how the ObjectQmgrName is\\n   treated for puts to shared queues. This attribute defaults to\\n   \\'USE\\', which means that the ObjectQmgrName will be used in name\\n   resolution and the appropriate transmission queue will be used.\\n   See the Name Resolution chart at\\n   http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/topic/com.ib\\n   m.mq.csqzal.doc/fg12170_.htm [http://publib.boulder.ibm.com/infocenter/wmqv7/v7r0/topic/com.ibm.mq.csqzal.doc/fg12170_.htm]\\n   The resolved name might be SYSTEM.CLUSTER.TRANSMIT.QUEUE or\\n   SYSTEM.QSG.TRANSMIT.QUEUE\\n   .\\n   The alternative is to set it to \\'IGNORE\\'.  This means that the\\n   ObjectQmgrName will be ignored, and the put will be performed\\n   locally if the queue is shared and ObjectQmgrName specifies a\\n   queue manager in the same Queue Sharing Group (QSG).\\n   .\\n   This attribute does not (and should not) apply to cluster\\n   queues.  However, the check on whether the queue is a cluster\\n   queue also incorrectly catches the case where the queue is not\\n   clustered, but the route to the remote queue manager that has\\n   been found is a cluster channel. For such cases the default\\n   behavior will still be used, regardless of the setting of\\n   SQQMNAME.\\n   .\\n   If SQQMNAME(IGNORE) works correctly, messages are put directly\\n   to the shared queue, which may improve performance slightly over\\n   sending the messages over a channel.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 6 *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Messages put to a shared queue using a  *\\n   *                      fully qualified remote name are not     *\\n   *                      put directly to the queue when SQQMNAME *\\n   *                      is set to IGNORE.                       *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   When putting to a remote queue on a qmgr in the same qsg as the\\n   local qmgr, messages should be put directly to the shared queue\\n   by the local qmgr instead of being sent via channels/IGQ if the\\n   qmgr object specifies SQQMNAME(IGNORE).\\n   If there is no default xmitq for the remote qmgr (i.e. a queue\\n   with the same name as the remote qmgr), and both qmgrs belong\\n   to the same cluster, CSQMOPEN will open the cluster transmit\\n   queue and use the existing cluster channel to put the message.\\n   If SQQMNAME(IGNORE) is set, it incorrectly assumes that the\\n   queue must be clustered, and so cannot be put to directly and\\n   continues to send the message over the channel.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  CSQMOPEN is changed to allow SQQMNAME(IGNORE) to take effect\\n   when putting to non-clustered shared queues on another qmgr in\\n   the qsg, even if the qmgrs are in a cluster.\\n   000Y\\n   CSQMOPEN\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PM22462\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2010-09-14\\n   \\n   \\n * CLOSED DATE\\n   2010-10-04\\n   \\n   \\n * LAST MODIFIED DATE\\n   2010-11-02\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PM22372 [http://www-01.ibm.com/support/docview.wss?uid=swg1PM22372]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK61051\\n   \\n   \\n\\nMODULES/MACROS\\n *     CSQMOPEN\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655L8200\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UK61051 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK61051]\\n   UP10/10/15 P F010\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  Slow consumption of the messages from the\\n   SYSTEM.CLUSTER.COMMAND.QUEUE is reported on v701 with increased\\n   work load but not on v6.\\n   The problem here is that rrmMaintenance keeps rescheduling\\n   itself to run immediately, and as a result the repository task\\n   doesn\\'t get a chance to get messages from the\\n   SYSTEM.CLUSTER.COMMAND.QUEUE.\\n   It also explains why we would see RC=2189, because cluster\\n   resolution hasn\\'t occurred within the 10 seconds. The reason is\\n   that the publications from the full repository are at\\n   the end of the command queue, and it takes longer than\\n   10 seconds for them to be processed.\\n   Symptom:\\n   Cluster Repository manager is stopped and is not restartable\\n   with below message:\\n   CSQX038E ;CSQ1 CSQXREPO Unable to put message to\\n   SYSTEM.CLUSTER.TRANSMIT.QUEUE, MQCC=2 MQRC=2269\\n   CSQX411I ;CSQ1 CSQXREPO Repository manager stopped.\\n   And applications trying to open cluster queues encounter\\n   problems (MQ (RC2189):MQRC_CLUSTER_RESOLUTION_ERROR)/failures.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  If recycling the MSTR and CHINIT does not bypass the\\n   problem.  PUT inhibit the SYSTEM.CLUSTER.COMMAND.QUEUE\\n   and allow the messages to drain.  Recycle the CHINIT.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS version 7 *\\n   *                 Release 0 Modification 1.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Slow processing of messages on the      *\\n   *                      SYSTEM.CLUSTER.COMMAND.QUEUE.           *\\n   *                      Also high CHINIT CPU may be seen,       *\\n   *                      especially during CHINIT                *\\n   *                      initialisation.                         *\\n   *                      Applications may get RC=2189 when       *\\n   *                      putting to previously unreferenced      *\\n   *                      cluster queues.                         *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   As part of CHINIT start-up rrmRepository first executes\\n   rrmInitRepos to create the cache (if required) and then\\n   adds a timer element to run rrmMaintenance.\\n   Later rrmRepository calls rrmGetMsg. This first checks the\\n   timer chain by running rrmRunTimers, and finding the timer\\n   element it calls rrmMaintenance.\\n   rrmMaintenance finds that the reconcile2 flag is still on so\\n   it reschedules itself immediately and exits. rrmRunTimers\\n   finds this new element on the timer chain and schedules\\n   rrmMaintenance and so this process continues for a long time,\\n   preventing rrmGetMsg from proceeding to process the messages\\n   from the SYSTEM.CLUSTER.COMMAND.QUEUE.\\n   The reconcile2 flag will not be set to zero until a new message\\n   arrives on the command queue or the queue is empty, and\\n   therefore rrmMaintenance will continue to be rescheduled\\n   in this way.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  rrmMaintenance has been changed so that it always reschedules\\n   itself at hourly intervals.\\n   Also, rrmMaintenance will not be scheduled to run unless the\\n   reconcile2 flag has been switched off.\\n   010Y\\n   AMQRRMHA\\n   CSQXRCTL\\n   CSQXRRMF\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n *  *********\\n   * HIPER *\\n   *********\\n   \\n   \\n    \\n   \\n   \\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PM44820\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   YesHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2011-08-01\\n   \\n   \\n * CLOSED DATE\\n   2011-08-23\\n   \\n   \\n * LAST MODIFIED DATE\\n   2012-05-07\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK71098 IV09401 [http://www-01.ibm.com/support/docview.wss?uid=swg1IV09401]\\n   \\n   \\n\\nMODULES/MACROS\\n *  AMQRRMHA CSQXRCTL CSQXRRMF\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R010 PSY UK71098 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK71098]\\n   UP11/09/10 P F109\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS UNREPRODUCIBLE IN NEXT RELEASE.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  When a duplicate CLUSRCVR channel name is defined and is\\n   introduced to a Queue Manager Cluster, the Full Repository\\n   queue manager writes an FDC record with Probe ID RM217001.\\n   \\n   The FDC dump data areas will contain the queue manager ID\\n   (QMID) such as QMGR1_2011-11-23_12.37.49.\\n   \\n   Probe Id          :- RM217001\\n   Component         :- rrmRecoClqMgr\\n   Program Name      :- amqrrmfa\\n   MQM Function Stack\\n   rrmRepository\\n   rrmProcessMsg\\n   rrmChangeClqMgr\\n   rrmRecoClqMgr\\n   xcsFFST\\n   \\n   The FDC had been written to help users guard against accidental\\n   insertion of a Queue Manager with a duplicated CLUSRCVR name.\\n   \\n   But, if the the new Queue Manager with the same CLUSRCVR name\\n   is intentionally being added, then the FDC is not needed, and\\n   so is distracting.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Modification 0 Release 1.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: When adding a QMGR to a cluster where   *\\n   *                      there already exists another QMGR with  *\\n   *                      the same name and channel name then an  *\\n   *                      FFST is issued.                         *\\n   *                      This will happen when a QMGR is deleted *\\n   *                      and recreated and it then rejoins the   *\\n   *                      cluster.                                *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   As the FFST can be seen as information-only in some cases, it\\n   caused unnecessary concern.\\n   There is still a need, however, for the administrator to be able\\n   to see when a QMGR has been introduced via an already known\\n   cluster receiver channel name.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n *  This APAR introduces a new error message: CSQX468I. It will\\n   describe the situation better than the FFST. It is still\\n   necessary for an administrator to read the CSQX468I message and\\n   ensure the activity is intentional.\\n   In case where it is accidental, the MQ administrator will still\\n   need to investigate their systems and rectify the situation.\\n   \\n   \\n   A new message is added to the infocenter under:\\n     \"z/OS Messages and Codes\"\\n       \"Messages\"\\n         \"Distributed queuing messages (CSQX...)\":\\n   \\n   \"\\n   CSQX468I: csect-name Queue manager qmgr-uuid1 has replaced\\n   queue manager qmgr-uuid2 in a cluster due to reuse of channel\\n   channel-name\\n   \\n   Explanation\\n   \\n   Queue manager qmgr-uuid1 has joined a cluster using a cluster\\n   receiver channel with the same name as one that has already\\n   been defined by queue manager qmgr-uuid2. All cluster receiver\\n   channels used within a cluster must be uniquely named.\\n   \\n   Severity\\n   \\n   0\\n   \\n   System action\\n   \\n   Queue manager qmgr-uuid1 uses channel channel-name. Queue\\n   manager qmgr-uuid2 cannot successfully participate in the\\n   cluster while queue manager qmgr-uuid1 is a member.\\n   \\n   System programmer response\\n   \\n   The use of a channel name currently associated with a different\\n   queue manager in the cluster can be intentional, for example it\\n   is possible the original queue manager has been deleted and\\n   recreated as a new queue manager. However, accidental\\n   duplication of a channel name across multiple queue managers\\n   would also result in this behavior. If this action was not\\n   intended review the configuration of the queue managers.\\n   \"\\n   \\n   \\n    \\n   \\n   \\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PM52988\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   010\\n   \\n   \\n * STATUS\\n   CLOSED UR1\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2011-11-30\\n   \\n   \\n * CLOSED DATE\\n   2012-02-28\\n   \\n   \\n * LAST MODIFIED DATE\\n   2012-04-03\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    IV08537 [http://www-01.ibm.com/support/docview.wss?uid=swg1IV08537]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK76540 UK76547 UK76548 UK76551 UK76552\\n   \\n   \\n\\nMODULES/MACROS\\n *  CMQXRRCH CSQFMSGC CSQFMSGD CSQFMSGX CSQFXDTA\\n   CSQFXTXC CSQFXTXE CSQFXTXK CSQFXTXU CSQXMNUM CSQXRRMF\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R010 PSY UK76540 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK76540]\\n   UP12/03/22 P F203\\n   \\n   \\n * R011 PSY UK76547 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK76547]\\n   UP12/03/22 P F203\\n   \\n   \\n * R012 PSY UK76548 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK76548]\\n   UP12/03/22 P F203\\n   \\n   \\n * R013 PSY UK76551 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK76551]\\n   UP12/03/22 P F203\\n   \\n   \\n * R014 PSY UK76552 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK76552]\\n   UP12/03/22 P F203\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  A MQCMD_CLUS_CHANNEL_FAIL msg on the cluster command queue can\\n   be read multiple times. This msg is written to the cmdq when the\\n   channel stops, and is used by the repos mgr to drive the logic\\n   to reallocate msgs to other qmgrs in the cluster, in\\n   rrmReallocMsgs. If the reallocate fails such the msg read from\\n   the cluster xmitq cannot be put to any queue in the cluster,\\n   e.g. because the queue no longer exists, or there is a 2189\\n   (MQRC_CLUSTER_RESOLUTION_ERROR), the msg is backed out to the\\n   cluster xmitq again. This backout also backs out the msg on the\\n   cluster cmdq, and so the msg is read again when the\\n   rrmProcessMsg finishes. The solution is to commit the msg read\\n   from the cmdq before reading any msgs from the xmitq.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All Users of Websphere MQ for z/OS           *\\n   *                 Version 6 using Clustering                   *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Messages can remain on the              *\\n   *                      SYSTEM.CLUSTER.TRANSMIT.QUEUE and       *\\n   *                      retried constantly if there is no       *\\n   *                      longer a valid destination for them     *\\n   *                      and there is no dead-letter queue.      *\\n   *                      The reallocation is constantly being    *\\n   *                      redriven by a message on the            *\\n   *                      SYSTEM.CLUSTER.COMMAND.QUEUE.           *\\n   ****************************************************************\\n   * RECOMMENDATION: Apply this fix                               *\\n   ****************************************************************\\n   There is a problem when reallocating messages to other qmgrs in\\n   the cluster after a cluster channel has ended such that there\\n   is no possible destination in the cluster, or the destination\\n   cannot be resolved. If there is no dead-letter queue for the\\n   messages to be backed out to, they remain on the\\n   SYSTEM.CLUSTER.TRANSMIT.QUEUE but the command message is also\\n   backed out to the SYSTEM.CLUSTER.COMMAND.QUEUE, re-driving the\\n   reallocation process and causing the messages to be retried\\n   again and again.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  If the message on the command queue is MQCMD_CLUS_CHANNEL_FAIL\\n   then rrmProcessMsg commits it before invoking rrmReallocMsgs to\\n   read messages from the SYSTEM.CLUSTER.TRANSMIT.QUEUE. This\\n   prevents the reallocation process from being redriven forever.\\n   000Y\\n   CSQXRMRC\\n   CSQXRRMF\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK36884\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2007-01-05\\n   \\n   \\n * CLOSED DATE\\n   2007-03-30\\n   \\n   \\n * LAST MODIFIED DATE\\n   2007-04-18\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PK36900 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK36900]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK23536\\n   \\n   \\n\\nMODULES/MACROS\\n *     CSQXRMRC CSQXRRMF\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655L8200\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UK23536 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK23536]\\n   UP07/04/18 I 1000\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  A REFRESH CLUSTER(clusname) REPOS(NO) command was issued.\\n   Afterward, some other queue managers in the cluster were\\n   unreachable from the queue manager where the REFRESH was done.\\n   Applications received MQRC 2087 when attempting to send reply\\n   messages back to these queue managers.\\n   \\n   The issue occurs when a cluster sender channel is active at the\\n   time that the refresh is issued but then goes inactive before\\n   the full repository has republished that cluster-sender record\\n   in response to the refresh.\\n   \\n   Additional Symptom(s) Search Keyword(s):\\n   MQRC_UNKNOWN_REMOTE_Q_MGR CLUSSDR\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  Do another REFRESH CLUSTER command or simply alter the\\n   description of the CLUSRCVR channel on the target queue manager\\n   to cause the cluster subscription to be republished.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of IBM MQ for z/OS Version 8       *\\n   *                 Release 0 Modification 0                     *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: A Partial Repository Queue Manager may  *\\n   *                      incorrectly lose sight of other Qmgrs   *\\n   *                      in a cluster, after a REFRESH CLUSTER   *\\n   *                      REPOS(NO) command is issued.            *\\n   ****************************************************************\\n   If auto-defined cluster sender channels are running when a\\n   REFRESH CLUSTER REPOS(NO) command is issued, but the channel\\n   stops prior to the REFRESH command processing completes, the\\n   republish of cluster Qmgr records may not be correctly added to\\n   the local cluster cache by reconcile processing. As a result the\\n   Qmgr may lose sight of the remote cluster Qmgr until the\\n   internal subscription for the Qmgr expires or the remote Qmgr\\'s\\n   cluster receive definition is altered.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  Cluster refresh and reconcile processing has been corrected,\\n   to ensure the republished cluster Qmgr record is added to the\\n   cluster cache, when a running cluster channel stops prior to\\n   refresh processing completing.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI96848\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   IBM MQ Z/OS V8\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655W9700\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   YesHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2018-04-17\\n   \\n   \\n * CLOSED DATE\\n   2018-06-29\\n   \\n   \\n * LAST MODIFIED DATE\\n   2018-07-25\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PI98289 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI98289] UI56966\\n   \\n   \\n\\nMODULES/MACROS\\n *  CMQXRECO CMQXRREF\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   IBM MQ Z/OS V8\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655W9700\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UI56966 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI56966]\\n   UP18/07/25 I 1000 ¢\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  CSQX438E messages with MQCC=2 MQRC=2161 (MQRC_Q_MGR_QUIESCING)\\n   following a STOP QMGR command for cluster channels.\\n   Looks like CSQXADPC should be getting invoked to post the\\n   get-waiters, but that isn\\'t happening.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 8 *\\n   *                 Release 0 Modification 0.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: When a STOP QMGR command is issued,     *\\n   *                      CSQX438E messages with MQRC 2161        *\\n   *                      (MQRC_Q_MGR_QUIESCING) are issued for   *\\n   *                      cluster-sender channels indicating      *\\n   *                      that message reallocation could not     *\\n   *                      be started for the channel.             *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   When a channel initiator is stopping because the queue manager\\n   is quiescing after a STOP QMGR command has been issued, sender\\n   channels that are in a get-wait are not posted until after\\n   TCP/IP calls have been cancelled for the process. This means\\n   that when the channel attempts to send a heartbeat, the send()\\n   fails with RC=00000480 (ECANCELED). Alternatively, if a\\n   get-wait expires before TCP/IP calls have been cancelled, the\\n   channel\\'s attempt to receive a reply to a heartbeat may fail\\n   as uninitialized storage in CSQXADPO can cause the sender\\n   channel to be treated as a receiver channel, resulting in the\\n   recv() call failing with RC=00000480 (ECANCELED). Either of\\n   these failures cause the channel initiator to look up the name\\n   of the transmission queue for cluster-sender channels before\\n   starting message reallocation, but this fails as the queue\\n   manager is quiescing and message CSQX438E with MQRC 2161\\n   (MQRC_Q_MGR_QUIESCING) is issued.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  CSQXSPRT has been updated to call CSQXADPC to post channels\\n   in a get-wait earlier during channel initiator shutdown when\\n   the connection to the queue manager is quiescing.\\n   CSQXADPO has been updated to initialize the storage used to\\n   indicate whether a sender or a receiver channel is being\\n   processed.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI30429\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS 8\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655W9700\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2014-11-26\\n   \\n   \\n * CLOSED DATE\\n   2015-01-05\\n   \\n   \\n * LAST MODIFIED DATE\\n   2015-04-02\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PI24496 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI24496]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UI24207\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQXADPC CSQXADPO CSQXSPRT\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS 8\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655W9700\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UI24207 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI24207]\\n   UP15/03/10 P F503\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  POK has two queue managers, one recently migrated to V7 and one\\n   V6, both running on z/os 1.10 LPARS, members of a cluster and\\n   members of a queue sharing group.  Some of their messages\\n   started failing when they would default to use the intra queue\\n   agent. Disabling the intra queue agent in the queue manager\\n   sending the message works as a workaround. Messages land in the\\n   dead letter queue of the queue manager receiving the message\\n   with a reason code of MQFB_XMIT_Q_MSG_ERROR, the field for\\n   orginal format doesn\\'t show up properly and the fields\\n   Destination queue and destination queue manager show blank. If\\n   they disable intra queue agent, message arrives at the proper\\n   queue with MQSTR in the format.\\n   .\\n   The Change Team reviewed the doc determined the following:\\n   This problem is caused by module CSQMPRPF inserting an MQRFH2\\n   immediately following the MQMD if an MQMDE does not exist in\\n   the message and totally ignoring the presence of an MQXQH\\n   which will exist if we are putting via a qsgdisp=shared XMITQ\\n   (which IGQ uses (SYSTEM.QSG.TRANSMIT.QUEUE)). Module CSQMIGQA\\n   expects an MQXQH immediately after the MQMD, but finds an ASCII\\n   MQRFH2 which he does not consider valid - Hence the CSQM064I\\n   message when he puts the input message to the DLQ with reason\\n   code MQFB_XMIT_Q_MSG_ERROR.\\n   CSQX548E Messages sent to local dead-letter queue,\\n   channel channel-name, reason=271\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 0 Modification 0 and Modification 1  *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Multiple problems with messages that    *\\n   *                      contain RFH2 headers or message         *\\n   *                      properties, and also contain either an  *\\n   *                      XQH and/or MDE                          *\\n   *                                                              *\\n   *                      At v700 the following problems have     *\\n   *                        been identified:                      *\\n   *                      - Messages put to a shared queue        *\\n   *                        contain the RFH2 and XQH/MDE headers  *\\n   *                        in the wrong order, preventing them   *\\n   *                        from being successfully gotten.       *\\n   *                      - Messages put to a shared queue that   *\\n   *                        contain the RFH2 and XQH/MDE headers  *\\n   *                        in the correct order (for example,    *\\n   *                        put by a v6 qmgr) are returned by an  *\\n   *                        MQGET with the headers in the wrong   *\\n   *                        order.                                *\\n   *                      Symptoms of these problems include      *\\n   *                        message CSQM064I, and                 *\\n   *                      - messages written to the DLQ with RC   *\\n   *                        MQFB_XMIT_Q_MSG_ERROR                 *\\n   *                      - Messages got from a queue which also  *\\n   *                        contains a message with an MDE which  *\\n   *                        is \\'not suitable\\' for the getter (for *\\n   *                        example, the groupid does not match   *\\n   *                        the search criteria) may result in an *\\n   *                        additional 48 bytes of nulls being    *\\n   *                        added to a subsequently returned      *\\n   *                        suitable message.                     *\\n   *                      At v700 and v701 the following problem  *\\n   *                        has been identified:                  *\\n   *                      - Messages containing an RFH2 header or *\\n   *                        message properties, and both an XQH   *\\n   *                        and MDE abend 5C6-00D40095 when put   *\\n   *                        to a shared queue.                    *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   During MQPUT of a message with properties OR an RFH2 header to\\n   a SHARED QUEUE in a QSG, the properties are \\'flattened\\' by\\n   module CSQMPRPF and written as an RFH2 as part of the message\\n   body. When an MDE or an XQH is also present the RFH2 is\\n   incorrectly positioned before the XQH/MDE. The message is\\n   written in this BAD format to the CF - so even a V600 QMGR in\\n   the same QSG will see the problem. At MQGET by a channel or IGQ\\n   the message CSQM064I will result and the message will be put to\\n   the DLQ with feedback MQFB_XMIT_Q_MSG_ERROR. For GROUP messages\\n   the grouping information may be ignored.\\n   As well as the above problems at PUT time, even a validly\\n   formatted message on a SHARED queue (e.g put by a V6 QMGR) that\\n   contains an RFH2 with an MDE or XQH is not returned correctly\\n   by MQGET.\\n   The above problems are fixed at V701 by Apar PK97364 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK97364], but there\\n   is an additional problem fixed by THIS Apar :-\\n   FOR A GROUP MESSAGE WITH AN RFH2 PUT TO A REMOTE QUEUE VIA A\\n   SHARED XMITQ or IGQ anAbend 5C6-00D40095 will result at PUT\\n   time due to all the freespace in the MDMCI control block\\n   having been used because the free offset is not properly\\n   decremented when an entry is deleted.\\n   During testing at V700 an additional problem was discovered\\n   where when an \\'unsuitable\\' (i.e MsgId/CorrelId mismatch) message\\n   with an MDE exists on a LOCAL (QMGR) queue that is skipped over\\n   before a valid message is found causes an extra 72 bytes to be\\n   appended to the retrieved message. This has also been fixed in\\n   this Apar by correctly initialising a re-used irh8_info block\\n   in csqimge9 (This fix is already in Base V701 code).\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  At V700 level\\n     Code is added to module CSQMPRPF to correctly chain the MQMD,\\n     MQXQH, MQMDE AND RFH2, and to order the entries in the MDMCI\\n     correctly.\\n     Code is added to module CSQIMPUS to correct the order in\\n     which the MDMCI entries are written to the CF.\\n     Code is added to module CSQMHDRS to return the RFH2 to mqget\\n     in the correct position in the user\\'s buffer.\\n       ALL the above changes are included in Apar PK97364 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK97364] at V701\\n     Code is added to module CSQIMGE9 in proc generate_handle to\\n     correctly initialise the irh8_info structure when it is\\n     re-used.\\n       This change is already present in the V701 base code.\\n   At V700 and V701 levels\\n     Code is added to module CSQMPOM to bypass adding the chained\\n     headers entry to the MDMCI when the queue is a shared queue.\\n     This prevents the Abend 5C6-00D40095 when messages with an\\n     XQH, MDE, and RFH2 are put to a shared queue.\\n   000Y\\n   010Y\\n   CSQIMGE9\\n   CSQIMPUS\\n   CSQMHDRS\\n   CSQMPOM\\n   CSQMPRPF\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n *  *********\\n   * HIPER *\\n   *********\\n   \\n   \\n    \\n   \\n   \\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK86875\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   YesHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2009-05-18\\n   \\n   \\n * CLOSED DATE\\n   2009-11-27\\n   \\n   \\n * LAST MODIFIED DATE\\n   2010-01-05\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK52366 UK52367\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQIMGE9 CSQIMPUS CSQMHDRS CSQMPOM  CSQMPRPF\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UK52366 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK52366]\\n   UP09/12/11 P F912\\n   \\n   \\n * R010 PSY UK52367 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK52367]\\n   UP09/12/11 P F912\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.Troubleshooting Guide; Troubleshooting; Trouble; Shooting; Guide; Clusters; Cluster; Clustering; CLUSSDRA; CLUSSDRB; CLUSSDR; CLUSRCVR; QCLUSTER; CLUSTERQ; TCLUSTER; CLUSTERT; CLUSQMGR; Full Repository; Partial Repository; Respository; Cluster Sender; Cluster; Cluster Receiver; Workload Balancing; Round-Robin; Round Robin; Round; Robin TECHNOTE (TROUBLESHOOTING)\\n\\nPROBLEM(ABSTRACT)\\n Your IBM MQ cluster is having a problem and you need to know how to troubleshoot it. This document describes several cluster issues and how to address them. \\n\\nRESOLVING THE PROBLEM\\n\\n\\n\\nCLUSTER HINTS AND TIPS\\n * Avoid using the REFRESH CLUSTER command until you have exhausted other options.\\n\\n * If you are running IBM MQ V7.0.1.0 or later, make sure you have a SYSTEM.CLUSTER.HISTORY.QUEUE [http://www.ibm.com/support/docview.wss?uid=swg21429699] in place before using REFRESH CLUSTER. If IBM support are involved in the problem, the history saved to this queue can help determine the root cause of your clustering issue.\\n\\n\\n * Make sure your cluster objects have either the CLUSTER or CLUSTERNL attribute set, depending on whether they appear in one or more clusters.\\n\\n\\n * When setting up or extending a cluster, you should define a CLUSSDR channel from each partial repository to a full repository, never the other way around.\\n\\n\\n * To move a queue manager to a new address, use SUSPEND QMGR to suspend it from the cluster, update the CONNAME field on its CLUSRCVR channel, then use RESUME QMGR to make it available in the cluster.\\n\\n\\n * If you move one full repository to a new address, make sure the other full repository is available to handle cluster activity during the move. Update the CONNAME on any manually defined CLUSSDR channels in the cluster after the full repository is available at its new address.\\n\\n\\n * Be aware that clustering allows you to send messages to queues elsewhere in the cluster, however, you can get messages only from queues on the local queue manager.\\n\\n * Review the Ten Quick Tips for a Healthy MQ Cluster [http://www.ibm.com/developerworks/mydeveloperworks/blogs/aimsupport/entry/ten_quick_tips_for_healthy_mq_cluster] blog post on developerWorks.\\n\\n\\nBack to top\\n\\n\\n\\nCHECKING THE CLUSTER STATUS\\n * Make sure the cluster repository process (amqrrmfa) for your queue manager is running. If this process has ended abnormally, restart the queue manager in order to get it running again.\\n\\n\\n * Make sure your cluster queue managers and cluster channels are in a good working status:\\n   \\n   CHECKING CLUSTER QUEUE MANAGERS AND CHANNELS\\n   DISPLAY CLUSQMGR(*) ALL\\n   DISPLAY CHSTATUS(*) WHERE(CHLTYPE EQ CLUSSDR) ALL\\n   DISPLAY CHSTATUS(*) WHERE(CHLTYPE EQ CLUSRCVR) ALL\\n   \\n   \\n   \\n   \\n   \\n * If your cluster channels are not working, or your cluster queue managers show a \"SYSTEM.TEMPUUID\" value, which indicates a communications problem, review the MQ Channel Troubleshooting [http://www.ibm.com/support/docview.wss?uid=swg21620770] guide for advice on clearing up channel problems.\\n\\n\\n * Make sure you can see the cluster queues you are using:\\n   \\n   CHECKING CLUSTER QUEUES\\n   DISPLAY QCLUSTER(\\'Your.Queue.Name\\') ALL\\n   DISPLAY Q(\\'Your.Queue.Name) CLUSINFO\\n   \\n   \\n   \\n   \\n   \\n * Be aware that partial repository queue managers will not display cluster queues which they have not accessed recently. If you run a program locally which accesses (MQOPENs) the cluster queue, you should then see it displayed.\\n\\n\\nBack to top\\n\\n\\n\\nWORKLOAD BALANCING AND ROUND-ROBIN PROCESSING\\n * Your cluster queue should have the parameter DEFBIND set to NOTFIXED, otherwise any program opening the queue will send all messages to it rather than spreading them around.\\n\\n\\n * Any MQI application sending messages should use the MQOPEN option MQOO_BIND_NOT_FIXED for precisely the same reason.\\n\\n\\n * Any MQI application opening a cluster queue should leave the queue manager name empty in the object descriptor. If the application sets the MQOD.ObjectQMgrName field, then instances of the cluster queue on other queue managers will be ineligible to receive messages.\\n\\n\\n * If your queue manager has a local instance of a cluster queue, local applications will default to sending all of their messages to it. You can change this behavior by modifying the queue manager:\\n   \\n   CHANGING THE CLUSTER MESSAGE DELIVERY BEHAVIOR\\n   DISPLAY QMGR CLWLUSEQ\\n   ALTER QMGR CLWLUSEQ(ANY)\\n   \\n   \\n   \\n   \\n   \\n * Make sure your cluster channels are running properly in order to achieve an even distribution of messages. Use CLWLRANK rather than CLWLPRTY if you want the cluster workload algorithm to ignore cluster channel status when distributing messages to cluster queues. \\n   \\n   Back to top\\n   \\n   \\n\\n \\n\\nPRODUCT ALIAS/SYNONYM\\n WebSphere MQ WMQz/os SUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS FIXED IF NEXT.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  Cluster workload balancing does not evenly distribute\\n   messages as expected if Intra-Group Queuing is enabled.\\n   When IGQ is enabled, CLWLUSEQ(ANY) processing is always taking\\n   place by the receiving queue manager causing the received\\n   message to bounce between several queue managers before winding\\n   up on a destination queue. To correct this would require a\\n   fundamental change to the way IGQ puts messages to the target\\n   queue. This apar is to be closed as FIN.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  Disable IGQ by issuing ALTER QMGR IGQ(DISABLED)\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 6 *\\n   *                 Release 0.                                   *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: Uneven cluster workload balancing       *\\n   *                      occurs when using CLWLUSEQ ANY with     *\\n   *                      intra-group queuing.                    *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   A cluster has several instances of a queue hosted on queue-\\n   managers which are members of a queue-sharing group. The queues\\n   are defined with CLWLUSEQ ANY. The queue-managers have intra-\\n   group queuing, IGQ, enabled.\\n   \\n   An application program connects to one of the queue-managers\\n   in the queue-sharing group and puts a message to the cluster\\n   queue. As the queue is defined with CLWLUSEQ ANY, cluster\\n   workload balancing determines that the message should be put to\\n   a remote queue-manager. As the remote queue-manager is a member\\n   of the queue-sharing group and IGQ is enabled, the message is\\n   sent via the SYSTEM.QSG.TRANSMIT.QUEUE. The message is received\\n   by the remote queue-manager and the IGQ agent puts the message\\n   to the cluster queue. However, as the queue is also defined with\\n   CLWLUSEQ ANY on the remote queue-manager, cluster workload\\n   balancing is again performed which may result in the message\\n   being sent to another queue-manager. This can lead to an uneven\\n   distribution of messages to the different instances of the\\n   cluster queue.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n *  This APAR is being closed FIN with concurrence from the\\n   submitting customer.\\n   \\n   \\n    \\n   \\n   \\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK32886\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED FIN\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2006-10-12\\n   \\n   \\n * CLOSED DATE\\n   2006-12-29\\n   \\n   \\n * LAST MODIFIED DATE\\n   2006-12-29\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n\\nFIX INFORMATION\\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSN\\n   UPz/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  As stated in technote 1225022, there can be a delay of up to 10\\n   seconds while a partial repository waits for replies to\\n   subscriptions made to two full repositories.  If there is no\\n   reply, 2085 MQRC_UNKNOWN_OBJECT_NAME or 2189\\n   MQRC_CLUSTER_RESOLUTION_ERROR will occur.\\n   .\\n   The circumstances for this 10-second delay differ between z/OS\\n   and distributed queue managers. The functionality should be the\\n   same across the platforms.\\n   *\\n   An example scenario is:\\n   A test on WebSphere MQ for Windows (level 6.0.2.3) where\\n     QMGR1 = full    for CLUSA\\n     QMGR2 = partial for CLUSA\\n     QMGR3 = partial for CLUSA\\n   If all the cluster channels were active, QMGR1 got an immediate\\n   2085, and  QMGR2/QMGR3 got 2085 within a second or two.  If all\\n   the cluster channels were stopped, QMGR3 still got an immediate\\n   2085, and QMGR2/QMGR3 got the ten-second delay.\\n   .\\n   With\\n     QMGR1 = partial for CLUSB (still full for CLUSA)\\n     QMGR4 = full    for CLUSB\\n     QMGR5 = full    for CLUSB\\n   attempts to put to a non-existent queue from QMGR1 either got an\\n   immediate 2085 or fairly quick one.  There was not a 10-second\\n   delay.\\n   .\\n   On WebSphere MQ for z/OS:\\n     RTPL  = full    for CLUSC\\n     QMGR4 = partial for CLUSC (still full for CLUSB)\\n     QMGR5 = partial for CLUSC (still full for CLUSB)\\n   A put from the full repository RTPL got an immediate 2085.\\n   .\\n   For:\\n     RTPJ = partial for CLUSA\\n   even with all cluster channels active, there was a 10-second\\n   delay,\\n   which is different from the observation on Windows.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 0 Modification 1.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: In a clustering environment the first   *\\n   *                      OPEN of a queue that DOES NOT EXIST     *\\n   *                      locally or in the cluster on a QMGR     *\\n   *                      that is a PARTIAL repository ALWAYS     *\\n   *                      takes a minimum of 10 seconds.          *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   The code in CSQMZLOO delays an OPEN for 10 seconds waiting\\n   for positive response to its inquiry from a full repository to\\n   be returned even if it has had a negative response.\\n   This causes application delays that are not acceptable for\\n   online systems (e.g CICS).\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  MQ z/OS Module CSQMZLOO has been changed to work the same way as\\n   the MQ Distributed code, and allow the OPEN to proceed on\\n   receipt of the first response from one of the full repositories.\\n   010Y\\n   CSQMZLOO\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PK76183\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2008-11-21\\n   \\n   \\n * CLOSED DATE\\n   2009-11-25\\n   \\n   \\n * LAST MODIFIED DATE\\n   2010-01-05\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PK66962 [http://www-01.ibm.com/support/docview.wss?uid=swg1PK66962]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UK52284\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQMZLOO\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R010 PSY UK52284 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK52284]\\n   UP09/12/11 P F912\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  Cics acts as webservice provider over MQ transport.\\n   A client put a message on the receive queue, that is triggered.\\n   Trigger monitor, then, writes on the INITQ and CPIL is attached\\n   because it is a SOAP message.\\n   CPIL  performs a MQGET on the receive queue. In the MQ message\\n   there is a replyDestination.\\n   CPIL then, starts CPIQ and it performs a MQPUT1 on the\\n   replyToQueue.\\n   But this queue is put-inhibited that cause to be returned:\\n   DFHPI0114 CPIQ The pipeline MQ transport mechanism failed\\n   because a call to WebSphere MQ function MQPUT1 returned with\\n   reason code 2051 MQRC_PUT_INHIBITED) x\\'803\\'\\n   .\\n   At this point CICS should write the message on Dead Letter\\n   Queue as described in the CICS manuals.\\n   .\\n   CPIQ issues a MQPUT1 on DLQ with PASS_ALL_CONTEXT but it fails\\n   with MQRC_CONTEXT_HANDLE_ERROR.\\n   No errors messages are sent for this error.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  n/a\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All CICS users.                              *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: CICS webservice request using MQ        *\\n   *                      transport fails with 2097 on DLQ        *\\n   *                      when attempting to write to the DLQ.    *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   A CICS webservice request with MQ transport fails because the\\n   specified reply-to queue is put disabled, (MQRC=2051) meaning\\n   the reply message cannot be written. The reply message should\\n   then be placed on the dead letter queue, but this also fails\\n   with a context handle error (MQRC 2097).\\n   \\n   Keyword(s): MQRC_CONTEXT_HANDLE_ERROR MQRC_PUT_INHIBITED\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  DFHPITQ1 has been altered so that the MQPUT1 which writes\\n   to the dead letter queue uses the mq_pmo.Option of\\n   MQPMO_SET_ALL_CONTEXT instead of MQPMO_PASS_ALL_CONTEXT.\\n   Additionally, the module is also changed so that whenever\\n   an MQPUT1 fails the failing message is written to the dead\\n   letter queue immediately.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n *  FIX AVAILABLE BY PTF ONLY\\n   \\n   \\n    \\n   \\n   \\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI48932\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   CICS TS Z/OS V5\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655Y0400\\n   \\n   \\n * REPORTED RELEASE\\n   800\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2015-09-17\\n   \\n   \\n * CLOSED DATE\\n   2016-04-05\\n   \\n   \\n * LAST MODIFIED DATE\\n   2016-05-04\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PI42870 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI42870]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UI36992 UI36993\\n   \\n   \\n\\nMODULES/MACROS\\n *  DFHPITQ1\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   CICS TS Z/OS V5\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655Y0400\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R800 PSY UI36992 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI36992]\\n   UP16/04/22 P F604\\n   \\n   \\n * R900 PSY UI36993 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI36993]\\n   UP16/04/22 P F604\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  CSQX191I messages following a STOP QMGR command for cluster\\n   channels.\\n   Looks like CSQXADPC should be getting invoked to post the\\n   get-waiters, but that isn\\'t happening.\\n   \\n   Additional symptoms:\\n   CSQX208E & CSQx206E messages with TRPTYPE=TCP RC=00000480\\n   .\\n   Cluster channels that experience the problem may be in the\\n   STOPPED status when the CHIN restarts, requiring manual\\n   intervention to restart the channel.\\n    This also applies non-cluster channels.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 1 Modification 0.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: When a STOP QMGR command is issued,     *\\n   *                      CSQX191I messages are issued for        *\\n   *                      cluster-sender channels indicating      *\\n   *                      that the channel is entering message    *\\n   *                      reallocation, accompanied by either     *\\n   *                      message CSQX208E or CSQX206E with       *\\n   *                      TRPTYPE=TCP RC=00000480.                *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   When a channel initiator is stopping because the queue manager\\n   is quiescing after a STOP QMGR command has been issued, sender\\n   channels that are in a get-wait are not posted until after\\n   TCP/IP calls have been cancelled for the process. This means\\n   that when the channel attempts to send a heartbeat, the send()\\n   fails with RC=00000480 (ECANCELED) and message CSQX206E is\\n   issued. Alternatively, if a get-wait expires before TCP/IP\\n   calls have been cancelled, the channel\\'s attempt to receive a\\n   reply to a heartbeat may fail as uninitialized storage in\\n   CSQXADPO can cause the sender channel to be treated as a\\n   receiver channel, resulting in the recv() call failing with\\n   RC=00000480 (ECANCELED) and message CSQX208E being issued.\\n   Either of these failures cause message reallocation to be\\n   started for cluster-sender channels and message CSQX191I to be\\n   issued.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  CSQXSPRT has been updated to call CSQXADPC to post channels\\n   in a get-wait earlier during channel initiator shutdown when\\n   the connection to the queue manager is quiescing.\\n   CSQXADPO has been updated to initialize the storage used to\\n   indicate whether a sender or a receiver channel is being\\n   processed.\\n   100Y\\n   CSQXADPC\\n   CSQXADPO\\n   CSQXSPRT\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI24496\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   100\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2014-08-22\\n   \\n   \\n * CLOSED DATE\\n   2014-12-19\\n   \\n   \\n * LAST MODIFIED DATE\\n   2016-05-16\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PI30429 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI30429] UI24061\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQXADPC CSQXADPO CSQXSPRT\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R100 PSY UI24061 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI24061]\\n   UP15/01/16 P F501\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES. SUBSCRIBE TO THIS APAR\\nBy subscribing, you receive periodic emails alerting you to the status of the APAR, along with a link to the fix after it becomes available. You can track this item individually or track all items by product.\\n\\nNotify me when this APAR changes.\\n\\nNotify me when an APAR for this component changes.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  An administrator makes a change to qm.ini, inadvertently adding\\n   some bad syntax in the CHANNELS or SSL stanzas.\\n   \\n   After restarting the queue manager, because of the error in\\n   qm.ini, the cluster component of the queue manager does not\\n   initialize its data properly in relation to locally-defined\\n   cluster channels.  As a result, no CLUSRCVR or locally-defined\\n   CLUSSDR channels are returned from DISPLAY CLUSQMGR(*) on the\\n   local queue manager.\\n   \\n   Cluster capability of the queue manager mostly ceases to\\n   function because of the above.  For example, applications\\n   calling MQOPEN or MQPUT1 for cluster queue names that are not\\n   yet known on the local queue manager fail with 2189\\n   MQRC_CLUSTER_RESOLUTION_ERROR.  Also, channels do not start\\n   outbound from the local queue manager, so messages remain on the\\n   cluster transmission queue(s).\\n   \\n   Non-clustered channels also do not start.\\n   \\n   FDC records are written with the following details:\\n   \\n   Probe Id :- RM241000\\n   Component :- rrmUpdateClqMgr\\n   Major Errorcode :- rrcE_UNEXPECTED_TOKEN\\n   Probe Description :- AMQ9224: Unexpected .ini file entry.\\n   MQM Function Stack\\n   rrmMain\\n   rrmRepository\\n   rrmGetMsg\\n   rfxReconcile\\n   rfxReconcileCLQMGR\\n   rrmRecoClqMgr\\n   rrmUpdateClqMgr\\n   xcsFFST\\n   \\n   Error logs have both of the following entries : AMQ9224,\\n   AMQ9419.\\n   \\n   (Note that the appearance of AMQ9419 in other circumstances will\\n   be correct, in the case that the user has failed to create the\\n   correct CLUSRCVR channels with the correct parameters.  In this\\n   case, though, the user had done this, but the qm.ini mistakes\\n   caused the queue manager to believe there were no CLUSRCVRs):\\n   \\n   AMQ9224: Unexpected .ini file entry.\\n   EXPLANATION:\\n   The entry with name \\'\\' in stanza \\'\\' in .ini file \\'\\' is either\\n   not a valid keyword or has an invalid value.\\n   ACTION:\\n   Correct the .ini file and retry the operation.\\n   \\n   AMQ9419: No cluster-receiver channels for cluster \\'MYQM\\'\\n   EXPLANATION:\\n   The repository manager has received information about a cluster\\n   for which no cluster-receiver channels are known.\\n   ACTION:\\n   Define cluster-receiver channels for the cluster on the local\\n   queue manager.\\n   \\n   The following FDC might also be seen:\\n   KN051002\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  Correct the qm.ini file and restart the queue manager.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   USERS AFFECTED:\\n   Users who edit their qm.ini file, and mistakenly save it\\n   including typos in the CHANNELS or SSL stanzas.\\n   \\n   \\n   Platforms affected:\\n   MultiPlatform\\n   \\n   ****************************************************************\\n   PROBLEM DESCRIPTION:\\n   Because of the errors in qm.ini, the cluster component of the\\n   queue manager did not initialize properly in relation to\\n   locally-defined cluster channels.\\n   \\n   The FDC records are not required to diagnose this problem.\\n   Instead a better error message was needed from the strmqm\\n   program, to alert the administrator to the fact that this\\n   problem is happening, before they allow applications to start,\\n   and suffer errors such as 2189 MQRC_CLUSTER_RESOLUTION_ERROR.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  The strmqm program has been changed so that it checks the syntax\\n   in the CHANNELS and SSL stanzas early on, before starting the\\n   queue manager fully.  If an error is found, strmqm now outputs\\n   an AMQ9224 error message, describing the full details of the\\n   position of the error in qm.ini.  It also ends immediately\\n   without starting the queue manager.  This makes it much easier\\n   to see what is wrong, and correct it quickly.\\n   \\n   The text of AMQ9224 is also amended to include the line number\\n   of the error within qm.ini.\\n   \\n   The RM241000 FDC will no longer appear in this particular case.\\n   \\n   ---------------------------------------------------------------\\n   The fix is targeted for delivery in the following PTFs:\\n   \\n   Version    Maintenance Level\\n   v7.5       7.5.0.9\\n   v8.0       8.0.0.8\\n   v9.0 CD    9.0.4\\n   v9.0 LTS   9.0.0.2\\n   \\n   The latest available maintenance can be obtained from\\n   \\'WebSphere MQ Recommended Fixes\\'\\n   http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006037 [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006037]\\n   \\n   If the maintenance level is not yet available information on\\n   its planned availability can be found in \\'WebSphere MQ\\n   Planned Maintenance Release Dates\\'\\n   http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006309 [http://www-1.ibm.com/support/docview.wss?rs=171&uid=swg27006309]\\n   ---------------------------------------------------------------\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   IT20323\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ BASE MULTIP\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5724H7241\\n   \\n   \\n * REPORTED RELEASE\\n   750\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2017-04-22\\n   \\n   \\n * CLOSED DATE\\n   2017-08-10\\n   \\n   \\n * LAST MODIFIED DATE\\n   2017-08-11\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PI85740\\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ BASE MULTIP\\n   \\n   \\n * FIXED COMPONENT ID\\n   5724H7241\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R750 PSY\\n   UPz/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  The problem is as follows:\\n   .\\n   Two queue managers (QMA and QMB) are members\\n   of a QSG and are also members of a cluster.\\n   The QSG has a shared queue (SQ1) which is defined\\n   as being in the cluster. This results in both\\n   queue managers advertising an instance of that\\n   queue to other members of the cluster.\\n   .\\n   SQ1 is then deleted. This should cause both\\n   queue managers to send an update to the cluster\\n   to notify other members that the queue manager\\n   no longer hosts an instance of that clustered\\n   queue. However, for shared queues this update\\n   does not happen (at least, not straight away).\\n   .\\n   The result of this is that the cluster cache\\n   on each qmgr has two records for the queue\\n   (one for each qmgr), but neither has an instance\\n   of the queue to put messages to.\\n   .\\n   When a message is put with a queue name\\n   SQ1 on QMA, it detects that there isn\\'t a local\\n   queue instance, so it uses the cluster cache to\\n   resolve the location of the queue name.\\n   As no local instance exists, it selects the only\\n   other entry for the queue (QMB) and puts the\\n   message to the SYSTEM.CLUSTER.TRANSMIT.QUEUE to\\n   be sent to QMB.\\n   .\\n   When the message is sent over the channel,\\n   QMB also detects that there is no local instance\\n   of the queue, so goes to the cluster cache and\\n   determines that QMA is the only available instance.\\n   .\\n   The message loops between the two qmgrs. This\\n   causes high CPU, and if the message is persistent\\n   then it also causes the high logging volume\\n   seen by the customer.\\n   .\\n   Additional Symptom(s) Search Keyword(s):\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  Restart the QMQRs. The cache did get updated after the queue\\n   managers were restarted.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED:                                              *\\n   * All users of IBM MQ for z/OS Version 9 Release 0             *\\n   * Modification 0.                                              *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION:                                         *\\n   * Deleting a shared cluster queue may result in the cluster    *\\n   * definitions for the shared queue remaining in the cluster    *\\n   * after a successful shared queue delete.                      *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   If multiple members of a QSG are also members of the same\\n   cluster, when a shared cluster queue is deleted, the cluster\\n   records for the queue may continue to exist in the cluster. This\\n   can result in the cluster hosting records for queues which no\\n   longer are valid. If messages are put to one of these queues,\\n   cluster resolution will attempt to put the message to another\\n   QMGR in the cluster where the queue was previously hosted, which\\n   results in further cluster resolution and subsequent puts to\\n   other cluster QMGRs, which can result in infinite loop of\\n   cluster resolution and puts to other QMGRs. This is due to\\n   shared queue deletes not correctly broadcasting the delete of\\n   the cluster queue in this case.\\n   \\n   The looping between QMGRs can result in high CPU usage on all\\n   the QMGRs involved. If the message put was persistent, this will\\n   also result in high logging volumes. When this scenario is\\n   encountered, a cancel may be required to stop the QMGR.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  Shared queue delete broadcast for cluster queues has been\\n   corrected to ensure cluster records are correctly deleted when a\\n   delete shared queue command is issued.\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI79259\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   MQ Z/OS V9\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655MQ900\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   YesHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt / Xsystem\\n   \\n   \\n * SUBMITTED DATE\\n   2017-04-03\\n   \\n   \\n * CLOSED DATE\\n   2017-04-20\\n   \\n   \\n * LAST MODIFIED DATE\\n   2017-09-16\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n    PI76942 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI76942]\\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    UI46582\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQMUQLC none\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   MQ Z/OS V9\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655MQ900\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UI46582 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI46582]\\n   UP17/05/19 P F705 ¢\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  The response messages are not being received from the\\n   remote queue-managers. The reason for this is that CSQUTIL will\\n   put the command messages to SYSTEM.COMMAND.INPUT using\\n   MQPER_PERSISTENCE_AS_Q_DEF. For example,when CSQUTIL is\\n   connected to CSQ1 and is processing commands to CSQ2\\n   (TGTQMGR(CSQ1)),the queue-manager detects that there is a\\n   cluster queue-manager object for CSQ2 and therefore the\\n   persistence for the message is taken from the\\n   SYSTEM.CLUSTER.TRANSMIT.QUEUE definition, which has\\n   DEFPSIST(YES). The message is then sent to CSQ2 (actually sent\\n   via IGQ) and a response message is returned via IGQ. The\\n   response message is persistent due to the persistence of the\\n   request, and when IGQ tries to put the message to the temporary\\n   dynamic queue it fails with MQRC_PERSISTENT_NOT_ALLOWED\\n   and the message goes to the dead-letter queue (CSQ1.DEAD.QUEUE).\\n   In this situation it would be expected that the request\\n   messages put by CSQUTIL are still non-persistent.\\n   \\n   \\n   Additional Symptom(s) Search Keyword(s):\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n *  N/A\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 7 *\\n   *                 Release 1 Modification 0.                    *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: The COMMAND function of CSQUTIL fails   *\\n   *                      with \\'CSQU051E Command responses not    *\\n   *                      received after 30 seconds\\' and the      *\\n   *                      CSQUTIL utility returns RC 8 if         *\\n   *                      SYSTEM.COMMAND.REPLY.MODEL is changed   *\\n   *                      to DEFTYPE(TEMPDYN).                    *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   When putting command messages using the COMMAND function,\\n   CSQUTIL specifies MQPER_PERSISTENCE_AS_Q_DEF, causing the\\n   persistence of the command message to be determined by the\\n   DEFPSIST attribute of SYSTEM.COMMAND.INPUT, or if TGTQMGR is\\n   used, the DEFPSIST attribute of the resolved transmission queue\\n   or remote queue definition used to reach the targer queue\\n   manager.\\n   After the command is processed, the reply message is put with\\n   the same persistence as the request message, however if\\n   SYSTEM.COMMAND.REPLY.MODEL has DEFTYPE(TEMPDYN), the reply\\n   queue is a temporary dynamic queue and only non-persistent\\n   messages are allowed.\\n   If the command message (and consequently the reply message)\\n   was persistent, the reply message cannot be put to this\\n   queue, leading to CSQUTIL timing out and reporting CSQU051E.\\n   \\n   The failure to put the reply message may also be reported by\\n   messages in the queue manager joblog including:\\n   \\n    CSQM064I xxxx CSQMIGQA Intragroup queueing agent put\\n    messages to dead-letter queue\\n   or\\n    CSQN212E xxxx COMMAND SERVER ERROR PUTTING TO REPLY TO QUEUE\\n    CSQN203I xxxx <queuename>\\n    MQCC=2 MQRC=2048\\n   \\n   If a dead letter queue is defined the reply messages will be\\n   put to it with DLH.Reason 2048 (MQRC_PERSISTENT_NOT_ALLOWED)\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  CSQUTIL is changed to put the command messages with\\n   MQPER_NOT_PERSISTENT.\\n   100Y\\n   CSQUTIL\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PI31166\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655R3600\\n   \\n   \\n * REPORTED RELEASE\\n   100\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2014-12-08\\n   \\n   \\n * CLOSED DATE\\n   2015-02-17\\n   \\n   \\n * LAST MODIFIED DATE\\n   2015-05-04\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PI34379 [http://www-01.ibm.com/support/docview.wss?uid=swg1PI34379] UI25205\\n   \\n   \\n\\nMODULES/MACROS\\n *  CSQUTIL\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V7\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655R3600\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R100 PSY UI25205 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UI25205]\\n   UP15/04/17 P F504\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES.z/os  A FIX IS AVAILABLE\\nObtain the fix for this APAR.\\n\\n\\nSUBSCRIBE\\nYou can track all active APARs for this component.\\n\\n\\n\\nAPAR STATUS\\n * CLOSED AS PROGRAM ERROR.\\n    \\n   \\n   \\n\\nERROR DESCRIPTION\\n *  RC=2082 (MQRC_UNKNOWN_ALIAS_BASE_Q) returned when a COA message\\n   is put to a qalias that resolves to a clustered queue.\\n   \\n   \\n    \\n   \\n   \\n\\nLOCAL FIX\\n\\nPROBLEM SUMMARY\\n *  ****************************************************************\\n   * USERS AFFECTED: All users of WebSphere MQ for z/OS Version 6 *\\n   ****************************************************************\\n   * PROBLEM DESCRIPTION: COA/COD Report messages generated from  *\\n   *                      a PUT with The MQMD ReplyToQ field      *\\n   *                      pointing to a non-clustered QAlias      *\\n   *                      whose Base Name is a Clustered queue    *\\n   *                      are not resolved when the ReplyToQmgr   *\\n   *                      equates to the local qmanager name of   *\\n   *                      the qmgr at which the final put to the  *\\n   *                      QAlias occurs and the COA/COD message   *\\n   *                      goes to the Dead Letter Queue with      *\\n   *                      MQRC_UNKNOWN_ALIAS_BASE_Q.              *\\n   ****************************************************************\\n   * RECOMMENDATION:                                              *\\n   ****************************************************************\\n   This Apar fixes a difference between all the Distributed\\n   platform\\'s implementations of COA/COD/Expiry report processing\\n   and the z/OS codebase. This occurs in specific configurations\\n   when the ReplyToQ in the original MQMD is a non-clustered QAlias\\n   queue on a z/OS QMGR, with a base queue name of a CLUSTER queue\\n   and ReplyToQmgr is non-blank and at the time of the PUT of the\\n   Report message is the Local qmgr name.\\n   e.g When the original message MQPUT is to a queue on QMGR A, but\\n       the ReplyToQ is a QAlias on QMGR A or B with a base queue\\n       name of a clustered queue and ReplyToQmgr has the QMGR name\\n       of the qmgr where the QAlias exists.\\n   The resolution of the QAlias is done on the qmgr where it exists\\n   and at that time the ReplyToQmgr value is used as ObjectQmgrName\\n   in the MQOD used to open the QAlias. Because it is the Local\\n   QMGR the z/OS code does not attempt a cluster lookup when the\\n   QAlias base queue name does not exist on that qmgr and the\\n   Report message is put to the local Dead Letter Queue with a\\n   Reason code of 2082 (MQRC_UNKNOWN_ALIAS_BASE_Q).\\n   This does NOT occur IF the QAlias itself is clustered or if a\\n   QMGR Alias is used as ReplyToQmgr.\\n   .\\n   On all the Distributed platform versions of MQ this problem does\\n   NOT occur, the QAlias base name is resolved via a lookup of the\\n   name in the fastnet cache and the Report message arrives\\n   correctly at its intended destination via cluster workload\\n   balancing.\\n   \\n   \\n    \\n   \\n   \\n\\nPROBLEM CONCLUSION\\n *  Module CSQMREPM creates the Report messages for Expiration, COA\\n   & COD. When a Report message is about to be PUT in module\\n   csqmrepm, the ReplyToQmgr field in the original Mhed is checked\\n   to see if the LOCAL qmgr or the QSG name was specified - in\\n   which case the fix blanks the ObjectQMgrName in the MQOD to be\\n   used to open the ReplyToQ. This causes CSQMOVAL to set the\\n   fOtherQmgr flag in the MHND and allows a cluster lookup for the\\n   queue in CSQMOAQ1 to occur when a QAlias base queue name is not\\n   defined in the local qmgr.\\n   In addition, in the CHINIT in rriAddMessage (CSQXRMMQ) when a\\n   receiver channel is about to PUT a message to its destination,\\n   queue checks are added to determine if it is a Report message\\n   for Expiration, COA or COD that is NOT being put to the Dead\\n   Letter Queue AND the destination qmgr is the LOCAL qmgr name or\\n   the QSG name - then as above the ObjectQMgrName in the MQOD is\\n   set to blanks to allow cluster lookups for the QAlias base\\n   queue name.\\n   000Y\\n   CSQMREPM\\n   CSQXRMMQ\\n   \\n   \\n    \\n   \\n   \\n\\nTEMPORARY FIX\\n\\nCOMMENTS\\n\\nAPAR INFORMATION\\n * APAR NUMBER\\n   PM11753\\n   \\n   \\n * REPORTED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * REPORTED COMPONENT ID\\n   5655L8200\\n   \\n   \\n * REPORTED RELEASE\\n   000\\n   \\n   \\n * STATUS\\n   CLOSED PER\\n   \\n   \\n * PE\\n   NoPE\\n   \\n   \\n * HIPER\\n   NoHIPER\\n   \\n   \\n * SPECIAL ATTENTION\\n   NoSpecatt\\n   \\n   \\n * SUBMITTED DATE\\n   2010-04-07\\n   \\n   \\n * CLOSED DATE\\n   2010-07-20\\n   \\n   \\n * LAST MODIFIED DATE\\n   2010-09-02\\n   \\n   \\n\\n * APAR IS SYSROUTED FROM ONE OR MORE OF THE FOLLOWING:\\n   \\n   \\n   \\n * APAR IS SYSROUTED TO ONE OR MORE OF THE FOLLOWING:\\n    PM18426 [http://www-01.ibm.com/support/docview.wss?uid=swg1PM18426] UK58950\\n   \\n   \\n\\nMODULES/MACROS\\n *     CSQMREPM CSQXRMMQ\\n   \\n   \\n    \\n   \\n   \\n\\nFIX INFORMATION\\n * FIXED COMPONENT NAME\\n   WMQ Z/OS V6\\n   \\n   \\n * FIXED COMPONENT ID\\n   5655L8200\\n   \\n   \\n\\nAPPLICABLE COMPONENT LEVELS\\n * R000 PSY UK58950 [HTTPS://WWW14.SOFTWARE.IBM.COM/WEBAPP/SET2/ORDERMEDIA/SHOPCART?PTFS=UK58950]\\n   UP10/08/06 P F008\\n   \\n   \\n\\nFIX IS AVAILABLE\\n * SELECT THE PTF APPROPRIATE FOR YOUR COMPONENT LEVEL. YOU WILL BE REQUIRED TO SIGN IN. DISTRIBUTION ON PHYSICAL MEDIA IS NOT AVAILABLE IN ALL COUNTRIES. TECHNOTE (TROUBLESHOOTING)\\n\\nPROBLEM(ABSTRACT)\\n While using OMEGAMON for Messaging, we found messages are not being processed from KQI.AGENT.REPLY QUEUE. Can you please explain the following 3 scenarios: \\n\\n1) Master Broker DEV\\nLPar: 1234/5678\\nIIB Agent: OMEGQI\\nMQ Agent: OMEGMQ\\nTotal IIB Brokers: 13 in each LPar\\n\\nOn Queue Manager (QMGR) QMG1 under Lpar 1234 there are no consumer on KQI.AGENT.REPLY.QUEUE \\nQUEUE. Something is writing messages and has filled up the Q to 10,000 an now messages are spilling into the Dead Letter Queue (DLQ). \\n\\nOn QMGR QMG2 under Lpar 1234- it looks good in MQ. Active consumer on KQI.AGENT.REPLY.QUEUE. \\n\\nOn QMGR QMGA under Lpar 5678 no consumer on Queue KQI.AGENT.REPLY.QUEUE with few messages on it. \\n\\nOn QMGR QMGB under Lpar 5678 no consumer on Queue KQI.AGENT.REPLY.QUEUE with few messages \\non it.\\n\\nOn all these brokers, the Archive accounting statistics with output format XML, SMF and resource statistics are active. However, apart from QMG2BRK none of the other brokers are showing \"Broker status events, resource statistics and msgflow statistics\". \\n\\n2) Master Broker Prod\\nLPar: LPA1/LPA2/LPAA/MBB2\\nBroker tasks: QMP1BRK(LPA1)/QMP2BRK(LPA2)/QMPABRK(LPA1) / QMPBBRK(LPA2)\\nNumber of brokers: 6/Lpar\\n\\nSimilar to above issue the KQI.AGENT.REPLY.QUEUE shows IPPROCS on QMPABRK(LPAA) and QMPBBRK(LPAB) but no consumer for this queue on QMP1BRK(LPA1) and QMP2BRK(LPA2). However, we suspect that QMP1BRK and QMP2BRK are not configured (updating KQIXML member) in the agent and is causing this issue but would like to confirm this. \\n\\n3) EPPLEX standalone\\nLPar: LPOC and LPMB \\nBroker: CBP1BRK and CBP2BRK\\nNumber of brokers: 1/Lpar\\n\\nFollowing commands were issues successfully on both of these brokers:\\n\\nI) Enabling Archive Accounting statistic with xml,smf by customizing\\nand submitting BIPCHMS.\\nmqsichangeflowstats <brokername> -a -g -j -c active -n advanced -t\\nbasic -o xml,smf\\n\\nII) Activate resource statistics by customizing and submitting BIPCHRS\\nmqsichangeresourcestats <brokername> -c active\\n\\nThese brokers are activated with Archive accounting statistics but resource statistics are not showing up in OMEGAMON. \\n\\nRESOLVING THE PROBLEM\\nThe analysis and solutions for each of the three scenarios are given below: \\n\\nScenario #1:\\nThe issue is due to the number of threads available to the agent being too few for the number of brokers.\\nThere are some error trace messages indicating \"Add a pending thread\" and other near messages indicate the threads would be the \"Qmgr receiver\", which is what opens and reads the agent reply queue on the broker\\'s queue manager. So the parameter change maximumAgentCollectionThreads is required....\\n\\nSince you are monitoring more than 10 brokers in some IIB monitoring agents, you should add an agent parameter called maximumAgentCollectionThreads in the KQIXML member of &rhilev.RKANDATV. This parameter is not shipped in the base copy of KQIXML, so it needs to be added.\\n\\nThe default value is 64, which is enough for 10 brokers. Each additional broker over 10 requires you to add 6 to the value for each broker. So, for example, for 13 brokers being monitored, you should\\nmake the value 82. There is not a problem with increasing this value on any agent, even if it is not monitoring so many brokers, so you can decide if 82 is appropriate for your site. \\n\\nIf you are modifying KQIXML in &rhilev.RKANDATV directly outside of PARMGEN, you would insert the parameter in the <KqiAGENT section of parameters, for example the environment below shows the parm\\nmaximumAgentCollectionThreads being insered: \\n\\n<KqiAGENT version=\"730\"\\nagentid=\"\"\\ndefaultRetainBrokerEvents=\"10\"\\ndefaultRetainFlowEvents=\"10\"\\nretainProductEvents=\"10\"\\ndiscoveryInterval=\"86400\"\\ndefaultStatisticInterval=\"60\"\\ndefaultFlowEventInterval=\"15\"\\ndefaultHistoricalAccountingType=\"ARCHIVE\"\\ndefaultRetainRecentSnapshotSamples=\"15\"\\ndefaultRetainRecentArchiveSamples=\"5\"\\ndefaultRetainRecentResourceSamples=\"1\"\\nholdTimeForQuery=\"180\"\\ndefaultReplyQueueName=\"KQI.AGENT.REPLY.QUEUE\"\\ndefaultReplyQueueModel=\"SYSTEM.BROKER.MODEL.QUEUE\"\\ndefaultCollectNodeData=\"NO\"\\ndefaultTakeActionAuthUsers=\"*\"\\nmaximumMessageLength=\"10240\"\\ndefaultPersistentBrokerData=\"NO\"\\ndefaultRefreshInterval=\"300\"\\nmaximumAgentCollectionThreads=\"82\"\\npersistentDataPath=\"/u/temp/data\"\\nkqiUSSPath=\"/u/temp/kqi\">\\n<MonitorBroker name=\"BRK\"\\nalias=\"QMGRBRK\"\\nenvfileDirectory=\"/wmqi/brokers/QMGRBRK\">\\n</MonitorBroker>\\n....\\n</KqiAGENT>\\n\\nModify via PARMGEN, you should insert the parameter line in the\\n<KqiAGENT section of parameters before the line \" |XIAGENT| \" in the\\nKQIXML member of your related &hilev.TKANDATV dataset (note this is\\nTKANDATV, not RKANDATV), as shown below in the line inserted for\\nmaximumAgentCollectionThreads. After that, when job KCIJ%USP runs, it\\nwill update the RKANDATV member with the new parameter, and the\\nparameter will not disappear later out of RKANDATV when you re-run\\nPARMGEN.\\n\\n<KqiAGENT version=\"730\"\\nmaximumAgentCollectionThreads=\"82\"\\n|XIAGENT|\\ndefaultRetainBrokerEvents=\"|XIBERET|\"\\ndefaultRetainFlowEvents=\"|XIFERET|\"\\nretainProductEvents=\"|XIPERET|\"\\ndiscoveryInterval=\"|XIDISINT|\"\\ndefaultStatisticInterval=\"|XISTINT|\"\\ndefaultFlowEventInterval=\"|XIFEINT|\"\\ndefaultHistoricalAccountingType=\"|XIHACCTT|\"\\ndefaultRetainRecentSnapshotSamples=\"|XISSRET|\"\\ndefaultRetainRecentArchiveSamples=\"|XIASRET|\"\\ndefaultRetainRecentResourceSamples=\"|XIRSRET|\"\\nholdTimeForQuery=\"|XIHTQRY|\"\\ndefaultReplyQueueName=\"|XIREQNME|\"\\ndefaultReplyQueueModel=\"|XIREQMDL|\"\\ndefaultCollectNodeData=\"|XICOLNDD|\"\\ndefaultTakeActionAuthUsers=\"|XIDUSERS|\"\\nmaximumMessageLength=\"|XIMAXML|\"\\ndefaultPersistentBrokerData=\"|XIPERBKD|\"\\ndefaultRefreshInterval=\"|XIREFINT|\"\\npersistentDataPath=\"|XIPERDTP|\"\\nkqiUSSPath=\"|XIUSSPAT|\">\\n<MonitorBroker name=\"|XIMBNAME|\"\\nalias=\"|XIMBALIA|\"\\nenvfileDirectory=\"|XIMBDIR|\">\\n</MonitorBroker>\\n</KqiAGENT>\\n\\nThis is the documentation link to description of the parameter:\\nhttps://www.ibm.com/support/knowledgecenter/SSRLD6_7.3.0/kqi_userguide/r-kqiagent_maximumagentcollectionthreads.html [https://www.ibm.com/support/knowledgecenter/SSRLD6_7.3.0/kqi_userguide/r-kqiagent_maximumagentcollectionthreads.html]\\n\\n\\nScenario #2:\\nConfirm that the brokers related to the KQI.AGENT.REPLY.QUEUE queues with 0 opens for input (no IPPROCS) are being monitored by the agent. When you update your agent parameters to add the required extra MonitorBroker section for each broker to be monitored, then queues will then be open for input by the agent as expected, and the IPPROCS value will become non-zero. So for this issue, you need to add statements like the following to your KQIXML for each broker to be monitored, and for PARMGEN, you would update your WCONFIG(KQI$XML) member as follows:\\n\\n<MonitorBroker name=\"CEP2BRK\"\\nalias=\"CEP2BRK\"\\nenvfileDirectory=\"/opt/local/lpp/mqsi/home/CEP2BRK\">\\n</MonitorBroker>\\n\\n\\nScenario #3:\\nThe agents are getting a reason code of 2035 from the broker\\'s queue manager when opening the agent\\'s reply queue, which means it is not authorized for opening the KQI.AGENT.REPLY.QUEUE. This may be an authorization issue with the model queue being used to create the reply queue (SYSTEM.BROKER.MODEL.QUEUE), or it may be an authorization issue for the reply queue being created (KQI.AGENT.REPLY.QUEUE), so you have to check authorization for the agent\\'s user ID for both. Please\\nupdate the security specifications for the queue managers involved and restart the agent.\\n\\nThese are the agent log messages that indicate the problem:\\n\\nagent log #1:\\n2017.309 05:09:41.48 KQIA044W (kqiaqopn.023) Open failed for queue KQI.AGENT.REPLY.QUEUE on queue manager CBP1, reason code 2035.\\n2017.309 05:09:41.49 KQIA038W (kqiaqrcv.003) Required queue KQI.AGENT.REPLY.QUEUE on CBP1 is failing to open with reason code 203\\n2017.309 05:09:41.49 5; periodic open retry will occur.\\n\\nagent log #2:\\n2017.309 05:10:47.00 KQIA044W (kqiaqopn.023) Open failed for queue\\nKQI.AGENT.REPLY.QUEUE on queue manager CBP2, reason code 2035.\\n2017.309 05:10:47.00 KQIA038W (kqiaqrcv.003) Required queue\\nKQI.AGENT.REPLY.QUEUE on CBP2 is failing to open with reason code 203\\n2017.309 05:10:47.00 5; periodic open retry will occur.\\n\\nRegarding the error messages given below from the logs with the mqsireport commands, the broker was not running when the commands were issued, so it cannot respond. The mqsireportbroker command works when the broker is not running, but the mqisreportflowstats and mqsireportresourcestats commands do not work when the broker is not running. The log gives a BIP8019E message for each of the 2 mqsireportflowstats commands and a BIP1919S message for the mqsireportresourcestats command, which is all for the same reason.\\n\\nBIP8019E: Integration node \\'CBP1BRK\\' stopped.\\n\\nThis integration node is stopped; the command you issued cannot be processed when an integration node is stopped. A previous command has been issued to stop this integration node, or this integration node has never been started. This integration node can be started, changed, or deleted.\\n\\nBIP1919S: The integration node \\'CBP1BRK\\' cannot be reached. \\n\\nCheck that the integration node is running. The message was \\'Failed to connect to the integration node. Return code: 0\\'.\\n\\n\\n\\n\\n\\nCross reference information Segment Product Component Platform Version Edition Systems and Asset Management Tivoli OMEGAMON XE for Messaging for z/OS IBM Tivoli OMEGAMON XE for WebSphere Message Broker Monitoring on z/OS z/OS 7.0, 7.0.1, 7.1.0, 7.3.0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
