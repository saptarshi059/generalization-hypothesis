{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3494083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b13e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fee66c239d74965bb96a8b553cc68bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b1f90b115f484cb921b049a5d0e878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4181a4e1f6d4082ab515ef8c74e1e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_unigrams(texts):\n",
    "    all_unigrams = []\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", 'senter', \"attribute_ruler\", \n",
    "                                        \"lemmatizer\", 'ner'], batch_size=10_000), total=len(texts)):\n",
    "        for token in doc:\n",
    "            if (not token.is_stop) and (not token.is_punct) and (not token.is_bracket) and \\\n",
    "            (not token.is_quote) and \\\n",
    "            (not token.is_currency) and \\\n",
    "            (not token.like_url) and \\\n",
    "            (not token.like_num) and \\\n",
    "            (not token.like_email) and \\\n",
    "            ('\\n' not in token.text) and (not token.text.strip() == ''):\n",
    "                all_unigrams.append(token.text)\n",
    "    \n",
    "    return all_unigrams\n",
    "\n",
    "def compute_overlap(ds1_texts, ds2_texts):\n",
    "    dataset1_unigrams = find_unigrams(ds1_texts)\n",
    "    dataset2_unigrams = find_unigrams(ds2_texts)\n",
    "    \n",
    "    set1 = set(dataset1_unigrams)\n",
    "    set2 = set(dataset2_unigrams)\n",
    "    common_terms = set1.intersection(set2)\n",
    "    \n",
    "    dataset1_unigrams_counter = Counter(dataset1_unigrams)\n",
    "    dataset2_unigrams_counter = Counter(dataset2_unigrams)\n",
    "    \n",
    "    combined_unigrams_common_counts = {}\n",
    "    for term in tqdm(common_terms):\n",
    "        # Taking combined frequency count across both corpora\n",
    "        combined_unigrams_common_counts[term] = dataset1_unigrams_counter[term] + \\\n",
    "                                                dataset2_unigrams_counter[term]\n",
    "\n",
    "    vocab = {}\n",
    "    top_100_words = list({k: v for k, v in sorted(combined_unigrams_common_counts.items(), \n",
    "                                                  key=lambda item: item[1], reverse=True)}.keys())[:100]\n",
    "\n",
    "    for term, idx in zip(top_100_words, list(range(100))):\n",
    "        vocab[term] = idx\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "dataset1 = pd.read_csv('squad_for_PPL_eval.csv')\n",
    "dataset2 = pd.read_csv('Saptarshi7-techqa-squad-style_for_PPL_eval.csv')\n",
    "\n",
    "dataset1_texts = dataset1['text'].to_list()\n",
    "dataset2_texts = dataset2['text'].to_list()\n",
    "\n",
    "vocab = compute_overlap(dataset1_texts, dataset2_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39dd5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "dataset1_vector = vectorizer.fit_transform([' '.join(x for x in dataset1_texts)])\n",
    "dataset2_vector = vectorizer.fit_transform([' '.join(x for x in dataset2_texts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f86e8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5947250625356564"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = stats.spearmanr(dataset1_vector.toarray()[0], dataset2_vector.toarray()[0])\n",
    "res.correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
