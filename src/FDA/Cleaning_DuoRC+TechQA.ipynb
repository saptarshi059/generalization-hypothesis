{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0d2823",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DuoRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f9c68",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "dataset = load_dataset('ibm/duorc', 'SelfRC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03974fe",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_split(split_name):\n",
    "    keep_indices = []\n",
    "    for idx, row in tqdm(enumerate(dataset[split_name])):\n",
    "        flag = 1\n",
    "        if row['no_answer'] == False:\n",
    "            for ans in row['answers']:\n",
    "                if ans not in row['plot']:\n",
    "                    flag = 0\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                keep_indices.append(idx)\n",
    "        else:\n",
    "            keep_indices.append(idx)\n",
    "    \n",
    "    dataset_subset = dataset[split_name].select(keep_indices)\n",
    "    dataset_df = pd.DataFrame(dataset_subset)\n",
    "    \n",
    "    answer_dicts = []\n",
    "    for row in tqdm(dataset_df.itertuples(index=False)):\n",
    "        answer_idxs = []\n",
    "        if row.no_answer == False:\n",
    "            for ans in row.answers:\n",
    "                answer_idxs.append(row.plot.find(ans))\n",
    "            answer_dicts.append({'text': row.answers, 'answer_start': answer_idxs})\n",
    "        else:\n",
    "            answer_dicts.append({'text': [], 'answer_start': []})\n",
    "    \n",
    "    dataset_df.drop(columns = ['answers'], inplace=True)\n",
    "    dataset_df['answers'] = answer_dicts\n",
    "    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023083e2",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_hf = Dataset.from_pandas(process_split('train'))\n",
    "validation_hf = Dataset.from_pandas(process_split('validation')) \n",
    "test_hf = Dataset.from_pandas(process_split('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e34a0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_dataset = DatasetDict()\n",
    "\n",
    "processed_dataset['train'] = train_hf\n",
    "processed_dataset['validation'] = validation_hf\n",
    "processed_dataset['test'] = test_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a97d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_dataset = processed_dataset.rename_columns({\"plot\":\"context\", \"plot_id\":\"id\"})\n",
    "processed_dataset = processed_dataset.remove_columns(['no_answer', 'question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c121f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_dataset.push_to_hub('Saptarshi7/duorc_processed', private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc376f",
   "metadata": {},
   "source": [
    "# TechQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "techqa = load_dataset('Saptarshi7/techqa-squad-style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing a question in the validation set which is just a null string.\n",
    "techqa['validation'] = techqa['validation'].filter(lambda x: x['question'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78874073",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78933ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_split(split_name):\n",
    "    # Removing those questions for which the tokenizer freaks out.\n",
    "    keep_indices = []\n",
    "    for idx, row in tqdm(enumerate(techqa[split_name])):\n",
    "        try:\n",
    "            tokenized_examples = tokenizer(row['question'], row['context'], truncation=\"only_second\", \n",
    "                                           max_length=512, stride=50, padding=\"max_length\")\n",
    "            keep_indices.append(idx)\n",
    "        except:\n",
    "            continue\n",
    "    dataset_subset = techqa[split_name].select(keep_indices)\n",
    "    \n",
    "    # Removing those questions for which the answers are not found in the context\n",
    "    keep_indices = []\n",
    "    for idx, row in tqdm(enumerate(dataset_subset)):\n",
    "        if row['is_impossible'] == False:\n",
    "            flag = 1\n",
    "            for ans in row['answers']['text']:\n",
    "                if ans not in row['context']:\n",
    "                    flag = 0\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                keep_indices.append(idx)\n",
    "        else:\n",
    "            keep_indices.append(idx)\n",
    "    \n",
    "    dataset_subset = dataset_subset.select(keep_indices)\n",
    "    techqa[split_name] = dataset_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f6ef7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process_split('train')\n",
    "process_split('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ce13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "techqa.push_to_hub('Saptarshi7/techqa_cleaned_for_bert', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c811854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acbcd1f9fe549418039079a1b0d4957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.7M/11.7M [00:01<00:00, 8.65MB/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10.7M/10.7M [00:00<00:00, 17.9MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890780830c05496caf636ac6644bf93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9f5d7c56aa4b5082b164b435860336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "s = load_dataset('Saptarshi7/techqa_cleaned_for_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a29097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
       "        num_rows: 599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
       "        num_rows: 297\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
